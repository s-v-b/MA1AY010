---
#title: "Exercices : semaine IV"
# subtitle: "M1 ISIFAR MA1AY010"

# date: "2025-09-08"

format:
  pdf:
    output-file: cc1-2025.pdf
    include-in-header:
      - file: before_header_td.tex
      - text: |
          \copypagestyle{style-td1}{mystyle}
          \makeevenhead{style-td1}{\sffamily\small Probabilités et Extrêmes}{}{\sffamily\small CC I}
          \makeoddhead{style-td1}{\sffamily\small Probabilités et Extrêmes}{}{\sffamily\small CC I}
          \pagestyle{style-td1}




engine: knitr
draft: true
---




```{r}
#| echo: false
#| warning: false
#| message: false

requireNamespace('quarto')
```







::: {.callout-note}

### CC I  :    

- 7 Octobre 2025

- Master I ISIFAR

- Durée : 1 heure 30 

- **Probabilités** 

:::


::: {.callout-caution}

- Aide-mémoire : une feuille A4 recto verso autorisée
- Aucun autre document autorisé
- Aucun moyen de communication électronique autorisé

:::


::: {#exr-   name=""} 

:::



<!-- Fonctions génératrices -->

<!-- Grimmet et Striztacker p. 175 -->

Soit  $X$ une variable binomiale à paramètres $n$ (fixé) et $V$ aléatoire  uniformément distribué sur $[0,1]$  ($X \sim  \text{Binom}(n, V)$). 

i. Calculer la fonction génératrice des probabilités de la loi de $X$ 
i. Quelle est l'espérance de $X$ ?
i. Que vaut $P\{X = k\}$ pour $k \in \{0,n\}$ ?


::: {.content-visible when-profile="solution"}

::: {.callout-note}

### Solution 

(@) 

    On utilisera la fait que si $Y_i \sim_{\text{i.i.d.}} \text{Berp}(p)$, alors $\sum_{i=1}^n Y_i \sim \text{Binom}(n, p)$. La fonction génératrice de $\text{Binom}(n, p)$ s'écrit donc 

    $$s \mapsto  \left(1-p + p s\right)^n$$

    et 

    $$\mathbb{E} \left[ s^{X} \mid \sigma(V) \right] = \left(1-p + p s\right)^n$$


    \begin{align*}
    G_X(s)  
      & =\mathbb{E} \left[ s^{X} \right] \\
      & = \mathbb{E} \left[ \mathbb{E} \left[ s^{X} \mid \sigma(V) \right]\right] \\
      & = \mathbb{E} \left[ \left(1-V + Vs\right)^n\right] \\
      & = \int_{[0,1]} \left(1-v + vs\right)^n  \mathrm{d}v \\
      & = \left[ \frac{1}{(n+1)(s-1)}\left(1-v + vs\right)^{n+1}\right]_0^1 \\
      & = \frac{s^{n+1} - 1}{(n+1)(s-1)} \\
      & =  \sum_{k=0}^n \frac{1}{n+1} s^{k} \, .
    \end{align*}

    On reconnnaît la fonction génératrice des probabilités de la loi uniforme sur $\{0, \ldots, n\}$

(@) L'espérance coïncide avec la dérivée de $G_X$ en $1$. Elle vaut $\frac{n}{2}$. 

(@) $P\{X=k\} = \frac{G_X^{(k)}(0)}{k!} = \frac{1}{n+1}$. 





:::

:::

::: {#exr-   name=""} 

:::



<!-- Variation sur les processus de branchements -->

On se donne un processus de branchement avec une distribution de reproduction Poissonienne de paramètre $\mu>0$. On note $Z_0=1, Z_1, \ldots$ les effectifs des différentes générations.  

i. Calculer la fonction génératrice de la loi de $Z_2$
i. Quelle est la probabilité que l'extinction ait lieu exactement à la génération $2$? 
i. Quelle est la probabilité que que l'extinction ait lieu exactement à la génération $n$? 

::: {.content-visible when-profile="solution"}

::: {.callout-note}

### Solution 

(@)  On  $G_\mu$ la fonction génératrice des probabilités de $\text{Poisson}(\mu)$, $G_n$ la fonction génératrice des probabilités de la loi de $Z_n$. 

    $$G_n = \underbrace{G_\mu \circ G_\mu \circ \ldots \circ G_\mu}_{n \text{ fois}}$$

    avec 

    $$G_\mu(s) =  \exp(\mu(s-1))$$

(@) 

    $$P\left\{ \text{Extinction à la génération } 2\right\} = P\left\{ Z_2=0 \vee Z_1 >0 \right\} = P\left\{ Z_2=0 \right\} - P\left\{ Z_1 =0 \right\}$$

    $$P\left\{ \text{Extinction à la génération } 2\right\} = G_2(0) - G_1(0)$$

    \begin{align*}
      P\left\{ \text{Extinction exactement à la génération } 2\right\}
      & = \exp(-\mu(1-\exp(-\mu))) - \exp(-\mu) \\
      & = \exp(-\mu) \left( \exp\left(\exp(-\mu)\right)-1\right)
    \end{align*}

(@) 

    
    \begin{align*}
    P\left\{ \text{Extinction exactement à la génération } n\right\} 
      & = P\left\{ Z_n=0 \vee Z_{n-1} >0 \right\} \\
      & = P\left\{ Z_n=0 \right\} - P\left\{ Z_{n-1} =0 \right\}
    \end{align*}


    $$P\left\{ \text{Extinction exactementà la génération } n\right\} = G_n(0) - G_{n-1}(0)$$

    \begin{align*}
      & P\left\{ \text{Extinction exactement à la génération } n\right\} \\
      & = \underbrace{G_\mu \circ G_\mu \circ \ldots \circ G_\mu}_{n \text{ fois}} (0) 
      - \underbrace{G_\mu \circ G_\mu \circ \ldots \circ G_\mu}_{n-1 \text{ fois}} (0) \\
      & = ...
    \end{align*}
:::



::: {.callout-caution  title="Solution Chat-GPT 3.5"}

On a un processus de Galton–Watson avec loi de reproduction $\mathrm{Poisson}(\mu)$. Notons la fonction génératrice de la loi des descendants
$$
f(s)=\mathbb{E}[s^{\xi}]=\exp\big(\mu(s-1)\big),
$$
où $\xi\sim\mathrm{Poisson}(\mu)$.

i) Fonction génératrice de $Z_2$

La fonction génératrice de $Z_n$ est la $n^{\text{ième}}$ composition de $f$ avec elle-même, donc pour $n=2$

$$
G_{Z_2}(s)=f\big(f(s)\big)
=\exp\Big(\mu\big(\exp(\mu(s-1))-1\big)\Big).
$$

ii) Probabilité que l'extinction ait lieu *exactement* à la génération (2)

L'évènement «extinction exactement à la génération 2» est ${Z_1>0  \text{ et } Z_2=0}$. On peut écrire
$$
\mathbb{P}(Z_2=0,Z_1>0)=\mathbb{P}(Z_2=0)-\mathbb{P}(Z_1=0).
$$

Or 

$$
\mathbb{P}(Z_1=0)=f(0)=e^{-\mu}) et (\mathbb{P}(Z_2=0)=G_{Z_2}(0)=f(f(0))=\exp!\big(\mu(e^{-\mu}-1)\big).
$$
Donc
$$
\boxed{\mathbb{P}(\text{extinction exactement en 2})=\exp!\big(\mu(e^{-\mu}-1)\big)-e^{-\mu}}
$$

On peut obtenir la même formule en conditionnant sur $Z_1=k$ et sommant 
$$\sum_{k\ge1}\mathbb{P}(Z_1=k) \times e^{-\mu k})$$

:::
:::

::: {#exr-   name=""} 

:::



<!-- Lois image. Formule de transfert -->

Soit $X \sim \mathrm{U}(-\pi/2,\pi/2)$ (loi uniforme sur $[-\pi/2,\pi/2]$), on définit $Y =\cos(X)$.

i. La loi de $Y$ est-elle absolument continue?
i. Si oui, déterminer une version de sa densité.

::: {.content-visible when-profile="solution"}

::: {.callout-note}

### Solution 

(@)

    Sur $[-\pi/2,\pi/2]$, $\cos$ croit de $0$ à $1$ puis décroit de $1$ à $0$. La fonction $\cos$ est paire. 

    Pour $x \in [0,1]$

    $$P \left\{ \cos(U) \leq x \right\} 
    = 2 \times P \left\{ \arccos(x) \leq U \leq \frac{\pi}{2}\right\} = 1 - \frac{2\arccos(x)}{\pi}$$

    La fonction de répartition est continue, dérivable et elle égale à l'intégrale de sa dérivée qui s'annule en $0$. La loi de $Y$ est donc absolument continue, à support  $[0,1]$.

    

(@)
    Une version continue de la densité est obtnue en dérivant la fonction de répartition:

    $$x \mapsto \frac{2}{\pi \sin(\arccos(x))} = \frac{2}{\pi \sqrt{1-x^2}}$$ 

    pour $x\in [0,1]$ 
    
:::

::: {.callout-caution title="Solution Chat-GPT 3.5"}

La loi  de $Y$ est absolument continue (sans atome) et on peut donner explicitement sa densité.

Remarquons d'abord que $\cos$ envoie l'intervalle $[-\pi/2,\pi/2]$ sur $[0,1]$. Pour $y\in(0,1)$ les équations $\cos x=y$ ont exactement deux solutions dans $[-\pi/2,\pi/2]$, à savoir $x_1=\arccos y$ et $x_2=-\arccos y$. La densité de (la loi de ) $X$ est $f_X(x)=\dfrac{1}{\pi}$ sur $[-\pi/2,\pi/2]$.

En utilisant la formule de changement de variable pour fonctions non monotones :
$$
f_Y(y)=\sum_{x:\cos x=y}\frac{f_X(x)}{|(\cos)'(x)|}
=\frac{1}{\pi}\left(\frac{1}{|,-\sin(\arccos y),|}+\frac{1}{|,-\sin(-\arccos y),|}\right).
$$
Or $\sin(\arccos y)|=\sqrt{1-y^2}$ et les deux termes sont égaux, d'où pour $y\in(0,1)$
$$
\boxed{,f_Y(y)=\frac{2}{\pi\sqrt{1-y^2}},.}
$$
Enfin $f_Y(y)=0$ pour $y\notin[0,1]$. (Les points $y=0$ et $y=1$ n'ont pas d'atome : la densité diverge en $0$ et $1$ mais ces points ont probabilité nulle.)


:::

::: {.callout-caution  title="Critique de Chat-GPT 3.5"}

La propriété *absolue continuité* n'est pas équivalente à  la propriété *être sans atome* (*diffuse*). 
La ou plutôt les lois de Cantor sont des exemples de lois sans atomes qui ne sont pas absolument continues.

La *formule de changement de variable pour fonctions non monotones* est vaseuse, on ne sait pas d'où elle sort, ni quel est son domaine d'application.

La discussion sur la divergence de la densité à la fin est elle aussi vaseuse. 

:::


:::

::: {#exr-   name=""} 

:::




Soit $X_1, \ldots, X_n, \ldots$ des variables aléatoires distribuées indépendamment selon une
loi de Pareto de paramètre $\alpha>0$, $P\{ X_1\geq t \} =  t^{-\alpha}, t \geq 1$. 
Soit $N$ indépendante des $X_i$, distribuée selon une loi de Poisson de paramètre $\mu>0.$ On définit $Z$ par $Z = \max_{i \leq N} X_i$.

*Remarque:* Si $N=0$, on convient de $\max_{i\leq N} X_i = 0$. 

i. Calculer la fonction de répartition de la loi de $Z$.
i. La loi de $Z$ possède-t-elle une espérance finie?  

::: {.content-visible when-profile="solution"}

::: {.callout-note}

### Solution 

(@)

    On note $F_{\alpha}(x) = 1 - t^{-\alpha}$  pour $t>1$, $0$ sinon.

    Pour $x> 1$, on a 
    \begin{align*}
      F_Z(x) 
        & = P\left\{Z \leq x\right\} \\
        & = \mathbb{E}\left[ \mathbb{I}_{Z \leq x}\right] \\
        & = \mathbb{E}\left[ \mathbb{E}\left[ \prod_{i=1}^N \mathbb{I}_{X_i \leq x}\mid  \sigma(N)\right]\right] \\
        & = \mathbb{E}\left[ \prod_{i=1}^N  \mathbb{E}\left[ \mathbb{I}_{X_i \leq x} \right] \right] \\
        & = \mathbb{E}\left[ (1-x^{-\alpha})^N  \right] \\
        & = \exp\left(\mu \left(1-x^{-\alpha} -1\right)\right) \\
        & = \exp\left(- \mu x^{-\alpha} \right)
    \end{align*}

    Par ailleurs, pour $x\in [0,1]$, $F_Z(x) = P \{ N=0\} = \exp(-\mu) = \lim_{x \downarrow 1} F_Z(x)$ 

(@)  
    $Z$ est positive, son espérance est bien définie et donnée par 

    $$\mathbb{E}Z = \int_0^\infty (1 -F_Z(t)) \mathrm{d}t$$

    \begin{align*}
       \int_0^\infty (1 -F_Z(x)) \mathrm{d}x 
          & =  (1- \exp(-\mu)) + \int_1^\infty \left( 1-  \exp\left(- \mu x^{-\alpha} \right)\right)\mathrm{d}x
    \end{align*}

    Si $\alpha > 1$, en utilisant $1-\exp(-x)\leq x$ oon vérifie que l'intégrale à droite est finie. L'espérance de $Z$ est finie

    Si $\alpha\leq 1$, l'intégrale est divergente, $\mathbb{E}Z$ est infinie ($Z$ n'est pas intégrable).

:::

::: {.callout-caution title="Solution Chat-GPT 3.5"}


Rappel : $N\sim\mathrm{Poisson}(\mu)$ indépendant des $X_i$, et pour tout $t\ge 1$
$P(X_1\ge t)=t^{-\alpha}$, donc $P(X_1\le t)=1-t^{-\alpha}$.

i) Fonction de répartition de $Z=\max_{1\le i\le N} X_i$ (avec convention $\max_{i\le0}X_i=0$)

Pour $t<0$ on a évidemment $F_Z(t)=0$.
Pour $0\le t<1$ tous les $X_i$ valent $\ge1$, donc ${Z\le t}={N=0}$ et
$$
F_Z(t)=P(Z\le t)=P(N=0)=e^{-\mu}.
$$
Pour $t\ge1$, conditionnons sur $N$ :
$$
P(Z\le t)=\sum_{k=0}^{\infty}P(N=k) \times P\big(\max_{1\le i\le k}X_i\le t\big)
=\sum_{k=0}^{\infty} e^{-\mu}\frac{\mu^k}{k!}\big(1-t^{-\alpha}\big)^k.
$$
La somme donne une exponentielle :
$$
F_Z(t)=e^{-\mu}\exp!\big(\mu(1-t^{-\alpha})\big)=\exp\big(-\mu t^{-\alpha}\big),\qquad t\ge1.
$$

Ainsi, de façon compacte (et continue en (t=1)) :
$$
\boxed{F_Z(t)=\begin{cases}
0 & t<0\\ 
e^{-\mu} & 0\le t<1 \\
\exp\big(-\mu t^{-\alpha}\big) & t\ge1.
\end{cases}}
$$

ii) Existence (ou non) de l'espérance de $Z$

Utilisons la formule $E[Z]=\int_{0}^{\infty}P(Z>t) dt$. Avec la CDF ci-dessus :

$$
\mathbb{E}[Z]=\int_0^1\big(1-e^{-\mu}\big) dt+\int_1^{\infty}\big(1-e^{-\mu t^{-\alpha}}\big) dt
=1-e^{-\mu}+\int_1^{\infty}\big(1-e^{-\mu t^{-\alpha}}\big) dt.
$$

Pour étudier la convergence de la seconde intégrale, posons $u=t^{-\alpha}$ (donc pour $t\in[1,\infty)$, $u\in(0,1])$. On obtient
$$
\int_1^{\infty}\big(1-e^{-\mu t^{-\alpha}}\big) dt
=\frac{1}{\alpha}\int_0^1\big(1-e^{-\mu u}\big)u^{-1/\alpha-1} du.
$$
Quand $u\to0^+$, $1-e^{-\mu u}\sim\mu u$, donc l'intégrande se comporte comme une constante fois $u^{-1/\alpha}$. L'intégrale $\int_0 u^{-1/\alpha} du$ converge près de $0$ si et seulement si $-1/\alpha>-1$, i.e. si et seulement si $\alpha>1$.

Conclusion :
$$
\boxed{\text{E}[Z]<\infty\ \Longleftrightarrow\ \alpha>1.}
$$

Si $\alpha>1$, une expression utile est
$$
\boxed{\mathbb{E}[Z]=1-e^{-\mu}+\frac{1}{\alpha}\int_0^1\big(1-e^{-\mu u}\big)u^{-1/\alpha-1},du.}
$$
(Autrement dit l'espérance est finie pour $\alpha>1$ et infinie pour $\alpha\le1$.)



:::


::: {#exr-   name=""} 

:::



<!-- Transformation,  -->

Le couple aléatoire $(X,Y)$ à valeur sur $]0,\infty)^2$ admet pour densité $f$. 

i. Si elle existe, quelle est la densité de la loi de $XY$ (une expression intégrale peut suffire) ? 
i. Préciser la densité lorsque $X, Y$ sont indépendantes et uniformément distribuées sur $[0,1]$


::: {.content-visible when-profile="solution"}

::: {.callout-note}

### Solution 

i. La transformation $]0, \infty)^2 \to ]0, \infty)^2$, $\begin{pmatrix} x \\ y \end{pmatrix}\mapsto \begin{pmatrix} x \\ xy \end{pmatrix}$ est bijective, continuement différentiable, de matrice Jacobienne $\begin{pmatrix} 1 & 0 \\ y & x \end{pmatrix}$ partout inversible et de déterminant Jacobien $x$. 

    La loi de $\begin{pmatrix} X \\ XY \end{pmatrix}$ admet une densité sur $]0,\infty)^2$ :

    $$\begin{pmatrix} u \\ v \end{pmatrix}  \mapsto \frac{f(u, v/u)}{u}$$

    La densité de la loi de $XY$ sur $]0,\infty)$ en $v$ est obtenue en intégrant:

    $$\int_{]0,\infty)} \frac{1}{u} f(u, v/u) \mathrm{d}u$$

ii. Dans cette configuration $f(x,y) = \mathbb{I}_{0<x< 1}\mathbb{I}_{0<y< 1}$. Pour $v \in (0,\infty)$ 

    \begin{align*}
    \int_{]0,\infty)} \frac{1}{u} f(u, v/u) \mathrm{d}u
      & = \int_{]0,\infty)} \mathbb{I}_{0<u< 1}\mathbb{I}_{0<v/u< 1}\frac{1}{u}  \mathrm{d}u \\
      & = \int_v^ 1 \frac{1}{u}\mathrm{d}u \\
      & = \ln \frac{1}{v} \, .
    \end{align*}

    La fonction de répartition associée est $v \mapsto v - v \ln v.$

:::

:::