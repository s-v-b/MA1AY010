---
#title: "Exercices : semaine IV"
# subtitle: "M1 ISIFAR MA1AY010"

# date: "2025-09-08"

format:
  pdf:
    output-file: td4.pdf
    include-in-header:
      - file: before_header_td.tex
      - text: |
          \copypagestyle{style-td1}{mystyle}
          \makeevenhead{style-td1}{\sffamily\small Probabilités et Extrêmes}{}{\sffamily\small TD IV}
          \makeoddhead{style-td1}{\sffamily\small Probabilités et Extrêmes}{}{\sffamily\small TD IV}
          \pagestyle{style-td1}
    
  html:
    output-file: td4.html

engine: knitr
draft: false
---




```{r}
#| echo: false
#| warning: false
#| message: false

requireNamespace('quarto')
```




```{r}
#| echo: false
#| eval: true

cnt_exo <- 1
```



::: {.callout-note}

### TD IV : Espérance conditionnelle/Catactérisations   

- 29 Septembre 2025-2 octobre 2025

- Master I ISIFAR

- **Probabilités** 

:::




::: {.callout-note}

### Conventions 

Dans les 3 exercices qui suivent, $X_1, \ldots, X_n, ...$ constituent une famille indépendante
de variables aléatoires identiquement distribuées à valeur dans $\{-1,1\}$.

L'univers des possibles est $\Omega = \{-1,1\}^{\mathbb{N}}$. Les $X_i$ sont les projections
canoniques.

On note $\mathcal{F}_n$ la tribu engendrée par les $n$ premières coordonnées :

$$
\mathcal{F}_n =  \sigma(X_1, \ldots, X_n) \,
$$

L'univers est muni de la tribu des cylindres $\mathcal{F} = \sigma\left(\bigcup_n \mathcal{F}_n\right)$.

On note $\Delta$ une constante à valeur dans $(0,1)$ (la *dérive* de la marche aléatoire).

On note $\mathbb{P}$ la loi produit infini, telle que pour tout $x \in \{-1, 1\}^n$

$$
\mathbb{P}\left\{\bigwedge_{i=1}^n X_i = x_i\right\} = \prod_{i=1}^n \frac{1}{2}\left(1 + x_i \Delta\right)
$$


On étudie la marche alétoire sur $\mathbb{Z}$ de dérive $\Delta$.

On note $S_n = \sum_{i=1}^n X_i$.

L'indice $n$ représente le temps, $S_n$ la position à l'instant $n$.

:::

::: {.content-visible when-profile="solution"}

::: {.callout-note}

### Solution

:::

:::




:::  {#exr-brw-1  name="Marches aléatoires biaisées i"}  

:::


a. Quelle est la loi de $S_n$ ?
a. $S_n$ est-elle $\mathcal{F}_n, \mathcal{F}_{n-1}, \mathcal{F}_{n+1}$ mesurable ?
a. Quelle est l'espérance de $S_n$ ?
a. Quelle est la variance de $S_n$ ?

::: {.callout-note}

### On admettra l'inégalité de Hoeffding:

Si $Y_1, \ldots, Y_n$ sont des variables aléatoires indépendantes telles que
$a_i \leq Y_i \leq b_i$ (les $Y_i$ sont bornées), alors 

$$
P \left\{  Z - \mathbb{E} Z \geq t  \right\} \leq \mathrm{e}^{- 2 \frac{t^2}{\sum_{i=1}^n (b_i-a_i)^2}}
$$

avec $Z = \sum_{i=1}^n Y_i$.

:::






:::  {#exr-brw-2  name="Marches aléatoires biaisées ii"}

:::

Pour $0 \leq \tau \leq  n \Delta$,

a. Majorer $\mathbb{P}\{ S_n \leq \tau \}$ à l'aide de l'inégalité de Chebyshev
a. Majorer $\mathbb{P}\{ S_n \leq \tau \}$ à l'aide de l'inégalité de Hoeffding
a. L'ensemble 
$$
E = \left\{ \omega :  \forall n,  S_n(\omega) < \tau \right\}
$$
appartient-il à la tribu $\mathcal{F}_m$ pour un $m$ donné ? est-il un événement de $\mathcal{F}$ ?
a.  Si $E$ est  un événement, quelle est sa probabilité ?

::: {.content-visible when-profile="solution"}

::: {.callout-note}

### Solution

:::

:::


::: {.callout-note}

### Convention


On suppose $\tau \in \mathbb{N} ∖  \{0\}$.

On note $T = \inf \left\{ n : S_n \geq \tau \right\}$. Si $\forall n, \quad S_n(\omega)< \tau$,
alors $T(\omega) = \infty$.

On note $S_T$, la fonction définie par 
$$
S_T(\omega) = \sum_{n=1}^\infty \mathbb{I}_{T(\omega)=n} S_n(\omega)\qquad \text{si } T(\omega) < \infty
$$
et $S_T(\omega) =0$ si $T(\omega) =\infty$.

:::




:::  {#exr-brw-3  name="Marches aléatoires biaisées iii"}
  
:::

a. Pourquoi peut-on considérer que $T$ est une variable aléatoire (à valeur dans $\mathbb{N} \cup \{\infty\}$) ?
a. Quelle est la probabilité que $T = \infty$ ?
a. L'événement $\{ T \leq n \}$ est-il $\mathcal{F}_{n-1}, \mathcal{F}_n, \mathcal{F}_{n+1}$ mesurable ?
a. Pourquoi peut-on considérer que $S_T$ est une variable aléatoire ?
a. Quelle est l'espérance de $S_T$ ?
a. Montrer que $\mathbb{E} S_T = \Delta \mathbb{E} T$ En déduire $\mathbb{E} T$.

::: {.content-visible when-profile="solution"}

::: {.callout-note}

### Solution

:::

:::

 

:::  {#exr-negbin  name="Binomiale négative"}

Les variables $X_1, :::
X_2, \ldots, X_n, \ldots$ sont des variables de Bernoulli de probabilité
de succès $p \in (0,1)$, indépendantes. On définit $T_1 = \min \{i : X_i=1\}$ (temps du premier succès), $T_2 = \min \{i : i > T_1, X_i=1\}$ (temps du premier succès après $T_1$),
et récursivement $T_{n+1} = \min \{ i : i > T_n, X_i =1\}$ (temps du $n+1$eme succès).

On admet l'existence d'un espace de probabilité $(\Omega, \mathcal{F}, P)$ où $\Omega = \{0,1\}^{\mathbb{N}}$, $\mathcal{F}$ est
une tribu pour laquelle les $X_i$ sont mesurables, et $P$ tel  que $X_1, \ldots, X_n, \ldots$ est une famille indépendante.


a. $T_1$ et plus généralement $T_n$ sont-elles des variables aléatoires?
a. Calculer $P \{ T_1 > k  \}$ pour $k \in \mathbb{N}$.
a. Calculer $P \{ T_1 = k  \}$ pour $k \in \mathbb{N}$
a. Calculer $\mathbb{E}T_1$.
a. Calculer $P \{ T_1 = k  \wedge T_2 = k+j\}$ pour $k, j \in \mathbb{N}$
a. Calculer $P \{ T_2 = k  \}$ pour $k \in \mathbb{N}$
a. Calculer $\mathbb{E}T_2$
a. Calculer $\mathbb{E} T_n$

::: {.content-visible when-profile="solution"}

::: {.callout-note}

### Solution

a. Les événements de la forme $\{ T_n \leq m\}$ sont dans $\sigma(X_1, \ldots X_m)$, car la seule connaissance de $X_1, \ldots, X_m$ suffit pour déterminer si le $i^{\text(eme)}$ succès survient avant le temps $m$. Donc les événements de la forme  $\{ T_n \leq m\}$ sont tous dans la tribu des cylindres $\sigma\left(\cup_m \sigma(X_1, \ldots X_m) \right)$. Tout événement de 
la forme $\{ T_n \in A\}$ avec $A \subset \mathbb{N} \cup \{ \infty\}$, appartient à la tribu 
engendrée par les événements $\{ T_n \leq m\}$, donc à la tribu  $\sigma\left(\cup_m \sigma(X_1, \ldots X_m) \right)$. 
a. $P\{ T_1 > k  \} = (1-p)^k$ ($T_1$ suit une loi géométrique)
a. $P\{ T_1 = k  \} = P\{ T_1 > k-1  \} - P\{ T_1 > k  \} = (1-p)^{k-1} p$ pour $k\geq 1$
a. $\mathbb{E} T_1 =  \sum_{k=0}^\infty P\{ T_1 > k  \} = \frac{1}{p}$
a. $P \{ T_1 = k  \wedge T_2 = k+j\} = (1-p)^{k-1} p (1-p)^{j-1} p$ pour $k, j \in \mathbb{N}\setminus \{0\}$, $T_1 \perp\!\!\!\perp T_2-T_1$ et $T_2-T_1 \sim T_1$  
a. $P \{ T_2 = k  \} = \sum_{j=1}^{k-1} (1-p)^{j-1} p (1-p)^{k-j-1} p = p \binom{k-1}{1} (1-p)^{k-2} p$  pour $k\geq 2$ 
a. $\mathbb{E} T_2 = 2 \mathbb{E} T_1 = \frac{2}{p}$
a. $\mathbb{E} T_n = n \mathbb{E} T_1 = \frac{n}{p}$

:::

:::




:::  {#exr-binzballs  name="Allocations aléatoires"}

:::


On dispose de $n$ urnes numérotées de $1$ à $n$ et de $n$ boules. Les boules sont réparties de manière uniforme dans les urnes (chaque boule se comporte de manière indépendante des autres et a probabilité $1/n$ de tomber dans chaque urne). On note $U_i$ la variable aléatoire désignant le nombre de boules qui tombent dans l'urne $i$. Dans la suite  $\alpha >1$ est  un réel.


a. Déterminer la loi de $U_i$.
a. Montrer que l'on a :
$$\mathbb{P}( \max_{1 \leq i \leq n} U_i > \alpha \ln n) \leq n \mathbb{P}( U_1 > \alpha \ln n).$$
a. Calculer $\mathbb{E}(\exp(U_1))$.
a. Montrer que pour tout $\beta > -n$, on a $(1+\beta/n)^n \leq \exp(\beta)$. 
a. Montrer que $\mathbb{P}(U_1 > \alpha \ln n) \leq \frac{\exp(\exp(\alpha)-1)}{n^\alpha}$.
a. En déduire que si $\alpha >1$, on a $$\mathbb{P}( \max_{1 \leq i \leq n} U_i > \alpha \ln n) \rightarrow_{n \rightarrow \infty} 0.$$

::: {.content-visible when-profile="solution"}

::: {.callout-note}

### Solution

a. $U_i  \sim \text{Binom}(n, 1/n)$
a. Les $U_i$ ne sont pas indépedantes (on a toujours $\sum_{i=1}^n U_i =n$), mais elles sont identiquement distribuées. 

    \begin{align*}
    \mathbb{P}( \max_{1 \leq i \leq n} U_i > \alpha \ln n) 
      & = \mathbb{P}( \cup_{1 \leq i \leq n} \{ U_i > \alpha \ln n\} ) \\
      & \leq \sum_{i=1}^n  \mathbb{P}( \{ U_i > \alpha \ln n\} ) \\
      & = n \mathbb{P}( \{ U_1 > \alpha \ln n\} )
    \end{align*}

a. $\mathbb{E}(\exp(U_1)) = \left(1 + \frac{1}{n} \left(\mathrm{e}-1\right) \right)^n \leq \exp\left( \mathrm{e}-1 \right)$
a. En utilisant l'inégalité de Markov, 

    \begin{align*}
      \mathbb{P}( \max_{1 \leq i \leq n} U_i > \alpha \ln n) 
        & \leq n \mathbb{P}( \{ U_1 > \alpha \ln n\} ) \\
        & \leq n \frac{\mathbb{E}(\exp(U_1))}{n^{\alpha}} \\
        & \leq \frac{\exp\left( \mathrm{e}-1 \right)}{n^{\alpha-1}}
    \end{align*}

:::

:::



:::  {#exr-roc  name="Restitution Organisée de Connaissances"}

:::

- Soient $A, B, C$ trois événements dans un espace probabilisé. A-t-on  toujours: $A \perp\!\!\!\perp B \text{ et } B \perp\!\!\!\perp C \Rightarrow A \perp\!\!\!\perp C$?
- Soient $P$ et $Q$ deux lois de probabilités sur $(\Omega, \mathcal{F})$, on définit l'ensemble $\mathcal{M} = \big\{ A : A \in \mathcal{F}, P(A)=Q(A)\}$.  Répondre par *vrai/faux/je ne sais pas* aux questions suivantes:


    a. $\mathcal{M}$ est-il toujours une classe monotone ?
    a. $\mathcal{M}$ est-il toujours une $\sigma$-algèbre ?
    a. $\mathcal{M}$ est-il toujours une $\pi$-classe ?

- Soient $G$ et $F$  sont deux fonctions génératrices de probabilité.   Répondre par *vrai/faux* aux questions suivantes:

    a. Est-il vrai que $G \times F$ est toujours une fonction génératrice ?
    a. Est-il vrai que $G + F$ est toujours une fonction génératrice de probabilité ?
    a. Est-il vrai que $\lambda G + (1-\lambda) F$ avec $\lambda \in [0,1]$ est toujours une fonction génératrice de probabilité ?


- Si $\widehat{F}$ est la fonction caractéristique de la loi de $X$, et si $\epsilon \perp\!\!\!\perp X$,
  avec $P\{\epsilon=1\}= P\{\epsilon=-1\}=1/2$, quelle est la fonction caractéristique de la loi de $\epsilon X$?

::: {.content-visible when-profile="solution"}

::: {.callout-note}

### Solution

:::

:::




:::  {#exr-size-biased  name="Distributions biaisées par la taille"}

:::

Si $X$ est une variable aléatoire positive intégrable, la version *biaisée par la taille* de  $X$ est la variable aléatoire $X^*$ dont la loi $Q$ est absolument continue par rapport à celle de $X$ (notée $P$) et dont la densité (par rapport à celle de $X$)
est proportionnelle à $X$: 
$$
\frac{\mathrm{d}Q}{\mathrm{d}P}(x) = \frac{x}{\mathbb{E}X} \, .
$$

a. Caractériser $X^*$ lorsque $X$ est une Bernoulli.
a. Caractériser $X^*$ lorsque $X$ est binomiale.
a. Caractériser $X^*$ lorsque $X$ est Poisson.
a. Caractériser $X^*$ lorsque $X$ est Gamma.
a. Si $X$ est à valeurs entières, exprimer la fonction génératrice de $X^*$ en fonction de celle de $X$.
a. Exprimer la transformée de Laplace de $X^*$ en fonction de celle de $X$.
a. Si $U$ est une transformée de Laplace, dérivable à droite en $0$, $U'/U'(0)$ est-elle la
transformée de Laplace d'une loi sur $[0, \infty)$?

::: {.content-visible when-profile="solution"}

::: {.callout-note}

### Solution

:::

:::





:::  {#exr-gaussian-friends  name="Amies des gaussiennes"}

:::

::: {.callout-note}

### Rappel

La loi normale centrée réduite $\mathcal{N}(0,1)$ admet pour densité $x\mapsto \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{x^2}{2}\right)$,

:::


- Si $X \sim \mathcal{N}(0,1)$, donner une densité de la loi de $Y=\exp(X)$ (Loi log-normale). Calculer l'espérance et la variance de $Y$.
- Même question si $X \sim \mathcal{N}(\mu, \sigma^2)$.
- Si $X, Y \sim \mathcal{N}(0,1)$, avec $X \perp\!\!\!\perp Y$, donner une densité de la loi de $Z=Y/X$ (Loi de Student à 1 degré de liberté)
- Si $X, Y \sim \mathcal{N}(0,1)$, avec $X \perp\!\!\!\perp Y$, donner une densité de la loi de $W = Y/ \sqrt{X^2}$. 
- Si $X \sim \mathcal{N}(0,1)$ et $\epsilon$ vaut $\pm 1$ avec probabilité $1/2$  (variable de Rademacher) avec   $X \perp\!\!\!\perp \epsilon$, donner une densité de la loi de $Y = \epsilon X$. $Y$ et $X$ sont-elles indépendantes ?


::: {#exr-   name=""} 

:::

:::  {#exr-reflexion  name="Principe de réflexion"}

:::

Principe de réflexion

Dans cet exercice, $X_1, X_2, \ldots$ sont des variables de Rademacher indépendantes ($P\{X_i = \pm 1\} = \frac{1}{2}$), $S_n =\sum_{i=1}^n X_i, S_0=0$ et  $M_n = \max_{k \leq n} S_n$.

Montrer que, pour $a> 0$,  

$$P\left\{ M_n > a \right\}\leq 2 P\left\{ S_n > a \right\}$$



::: {.callout-note}

### Statistique des rangs/Statistiques d'ordre


Les statistiques d'ordre $X_{1:n}\leq X_{2:n}\leq X_{n:n}$ d'un $n$-échantillon  $X_1,\ldots,X_n$
d'observations indépendantes identiquement distribuées sont formées par le réarrangement croissant (convention) de l'échantillon. 

Quand $n$  est clair d'après le contexte on peut les noter $X_{(1)} \leq \ldots \leq X_{(n)}$. 

:::

 

:::  {#exr-order-1  name="Statistiques d'ordre"}

a. Vérifier que la l:::
oi jointe des statistiques d'ordre est absolument continue par rapport 
à la loi de l'échantillon.
a. On suppose que $X$ est une variable aléatoire réelle, absolument continue, de densité continue. Montrer que l'échantillon est presque sûrement formé de valeurs deux à deux distinctes. 
a. Donner la densité de la loi jointe des statistiques d'ordre.
a. Si la loi des $X_i$  définie par sa fonction de répartition $F$, admet une densité $f$, quelle est la densité de la loi de $X_{k:n}$  pour $1\leq k\leq n$ ?  
a.  Montrer que conditionnellement à $X_{k:n}=x$, la suite 

    $$(X_{i:n}-X_{k:n})_{i=k+1,\ldots, n}$$  

    est distribuée comme les statistiques d'ordre d'un $n-k$ échantillon de la loi d'excès au dessus de $x$  (fonction de survie $\overline{F}(x+\cdot)/\overline{F}(x))$ avec la convention $\overline{F}=1-F$). 

(Représentation de Rényi)

:::  {#exr-order-2  name="Statistiques d'ordre d'un échantillon exponentiel"}

Cet exercice reprend:::
 les conventions de l'exercice précédent. On s'intéresse maintenant aux statistiques d'ordre d'un échantillon exponentiel.

a. Si $X_1,\ldots,X_n$ est un échantillon i.i.d. de la loi exponentielle d'espérance $1$ (densité $\mathbb{I}_{x>0} \mathrm{e}^{-x}$), et $X_{n:n}\geq X_{n-1:n}\geq  \ldots \geq X_{1:n}$ les statistiques d'ordre associées, montrer que:

  i. avec la convention $X_{0:n}=0$, les écarts $(X_{i:n}-X_{i-1:n})_{1\leq i\leq n}$ (*spacings*) forment une
  collection de variables aléatoires indépendantes ;
  i. $X_{i:n}-X_{i-1:n}$ est distribuée selon une loi exponentielle
  d'espérance $\tfrac{1}{i}$ .

a. Maintenant  $(k_n)_n$ est une suite croissante d'entiers qui tend vers l'infini,  telle que $k_n/n$ tend vers une limite finie (éventuellement nulle).  Montrer que

    $$\frac{X_{k_n:n} -\mathbb{E} X_{k_n:n} }{\sqrt{\operatorname{var}(X_{k_n:n} )}}$$ 
    
    converge en loi vers une Gaussienne centrée réduite. 



::: {.content-visible when-profile="solution"}

::: {.callout-note}

### Solution

:::

:::

