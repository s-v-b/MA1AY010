---
#title: "Exercices : semaine IV"
# subtitle: "M1 ISIFAR MA1AY010"

# date: "2025-09-08"

format:
  pdf:
    output-file: td4.pdf
    class: exam
    include-in-header:
      - text: "\\lhead{{\\sf  Probabilités \\\\ TD IV}}"
  html:
    output-file: td4.html

engine: knitr
draft: false
---




```{r}
#| echo: false
#| warning: false
#| message: false

requireNamespace('quarto')
```




```{r}
#| echo: false
#| eval: true

cnt_exo <- 1
```



::: {.callout-note}

### TD IV : Espérance conditionnelle/Catactérisations   

- 29 Septembre 2025-2 octobre 2025

- Master I ISIFAR

- **Probabilités** 

:::




::: {.callout-note}

### Conventions 

Dans les 3 exercices qui suivent, $X_1, \ldots, X_n, ...$ constituent une famille indépendante
de variables aléatoires identiquement distribuées à valeur dans $\{-1,1\}$.

L'univers des possibles est $\Omega = \{-1,1\}^{\mathbb{N}}$. Les $X_i$ sont les projections
canoniques.

On note $\mathcal{F}_n$ la tribu engendrée par les $n$ premières coordonnées :

$$
\mathcal{F}_n =  \sigma(X_1, \ldots, X_n) \,
$$

L'univers est muni de la tribu des cylindres $\mathcal{F} = \sigma\left(\bigcup_n \mathcal{F}_n\right)$.

On note $\Delta$ une constante à valeur dans $(0,1)$ (la *dérive* de la marche aléatoire).

On note $\mathbb{P}$ la loi produit infini, telle que pour tout $x \in \{-1, 1\}^n$

$$
\mathbb{P}\left\{\bigwedge_{i=1}^n X_i = x_i\right\} = \prod_{i=1}^n \frac{1}{2}\left(1 + x_i \Delta\right)
$$


On étudie la marche alétoire sur $\mathbb{Z}$ de dérive $\Delta$.

On note $S_n = \sum_{i=1}^n X_i$.

L'indice $n$ représente le temps, $S_n$ la position à l'instant $n$.

:::


### Exercice `r cnt_exo` (marches aléatoires biaisées i)

{{< include _increment.qmd >}}  



a. Quelle est la loi de $S_n$ ?
a. $S_n$ est-elle $\mathcal{F}_n, \mathcal{F}_{n-1}, \mathcal{F}_{n+1}$ mesurable ?
a. Quelle est l'espérance de $S_n$ ?
a. Quelle est la variance de $S_n$ ?

::: {.callout-note}

### On admettra l'inégalité de Hoeffding:

Si $Y_1, \ldots, Y_n$ sont des variables aléatoires indépendantes telles que
$a_i \leq Y_i \leq b_i$ (les $Y_i$ sont bornées), alors 
$$
P \left\{  Z - \mathbb{E} Z \geq t  \right\} \leq \mathrm{e}^{- 2 \frac{t^2}{\sum_{i=1}^n (b_i-a_i)^2}}
$$
avec $Z = \sum_{i=1}^n Y_i$.

:::





### Exercice `r cnt_exo`  (marches aléatoires biaisées ii)

{{< include _increment.qmd >}}


Pour $0 \leq \tau \leq  n \Delta$,

a. Majorer $\mathbb{P}\{ S_n \leq \tau \}$ à l'aide de l'inégalité de Chebyshev
a. Majorer $\mathbb{P}\{ S_n \leq \tau \}$ à l'aide de l'inégalité de Hoeffding
a. L'ensemble 
$$
E = \left\{ \omega :  \forall n,  S_n(\omega) < \tau \right\}
$$
appartient-il à la tribu $\mathcal{F}_m$ pour un $m$ donné ? est-il un événement de $\mathcal{F}$ ?
a.  Si $E$ est  un événement, quelle est sa probabilité ?


::: {.callout-note}

### Convention


On suppose $\tau \in \mathbb{N} ∖  \{0\}$.

On note $T = \inf \left\{ n : S_n \geq \tau \right\}$. Si $\forall n, \quad S_n(\omega)< \tau$,
alors $T(\omega) = \infty$.

On note $S_T$, la fonction définie par 
$$
S_T(\omega) = \sum_{n=1}^\infty \mathbb{I}_{T(\omega)=n} S_n(\omega)\qquad \text{si } T(\omega) < \infty
$$
et $S_T(\omega) =0$ si $T(\omega) =\infty$.

:::


### Exercice `r cnt_exo` (marches aléatoires biaisées iii)

{{< include _increment.qmd >}}
  

a. Pourquoi peut-on considérer que $T$ est une variable aléatoire (à valeur dans $\mathbb{N} \cup \{\infty\}$) ?
a. Quelle est la probabilité que $T = \infty$ ?
a. L'événement $\{ T \leq n \}$ est-il $\mathcal{F}_{n-1}, \mathcal{F}_n, \mathcal{F}_{n+1}$ mesurable ?
a. Pourquoi peut-on considérer que $S_T$ est une variable aléatoire ?
a. Quelle est l'espérance de $S_T$ ?
a. Montrer que $\mathbb{E} S_T = \Delta \mathbb{E} T$ En déduire $\mathbb{E} T$.


### Exercice `r cnt_exo` 

{{< include _increment.qmd >}}

Les variables $X_1, X_2, \ldots, X_n, \ldots$ sont des variables de Bernoulli de probabilité
de succès $p \in (0,1)$, indépendantes. On définit $T_1 = \min \{i : X_i=1\}$ (temps du premier succès), $T_1 = \min \{i : i > T_1, X_i=1\}$ (temps du premier succès après $T_1$),
et récursivement $T_{n+1} = \min \{ i : i > T_n, X_i =1\}$ (temps du $n+1$eme succès).

On admet l'existence d'un espace de probabilité $(\Omega, \mathcal{F}, P)$ où $\Omega = \{0,1\}^{\mathbb{N}}$, $\mathcal{F}$ est
une tribu pour laquelle les $X_i$ sont mesurables, et $P$ tel  que $X_1, \ldots, X_n, \ldots$ est une famille indépendante.


a. $T_1$ et plus généralement $T_n$ sont-elles des variables aléatoires?
a. Calculer $P \{ T_1 > k  \}$ pour $k \in \mathbb{N}$.
a. Calculer $P \{ T_1 = k  \}$ pour $k \in \mathbb{N}$
a. Calculer $\mathbb{E}T_1$.
a. Calculer $P \{ T_1 = k  \wedge T_2 = k+j\}$ pour $k, j \in \mathbb{N}$
a. Calculer $P \{ T_2 = k  \}$ pour $k \in \mathbb{N}$
a. Calculer $\mathbb{E}T_2$
a. Calculer $\mathbb{E} T_n$


### Exercice `r cnt_exo` 

{{< include _increment.qmd >}}



On dispose de $n$ urnes numérotées de $1$ à $n$ et de $n$ boules. Les boules sont réparties de manière uniforme dans les urnes (chaque boule se comporte de manière indépendante des autres et a probabilité $1/n$ de tomber dans chaque urne). On note $U_i$ la variable aléatoire désignant le nombre de boules qui tombent dans l'urne $i$. Soit $\alpha >1$ un réel.


a. Déterminer la loi de $U_i$.
a. Montrer que l'on a :
$$\mathbb{P}( \max_{1 \leq i \leq n} U_i > \alpha \log n) \leq n \mathbb{P}( U_1 > \alpha \log n).$$
a. Calculer $\mathbb{E}(\exp(U_1))$.
a. Montrer que pour tout $\beta > -n$, on a $(1+\beta/n)^n \leq \exp(\beta)$. 
a. Montrer que $\mathbb{P}(U_1 > \alpha \log n) \leq \frac{\exp(\exp(\alpha)-1)}{n^\alpha}$.
a. En déduire que si $\alpha >1$, on a $$\mathbb{P}( \max_{1 \leq i \leq n} U_i > \alpha \log n) \rightarrow_{n \rightarrow \infty} 0.$$



### Exercice `r cnt_exo`  (Restitution Organisée de Connaissances)

{{< include _increment.qmd >}}


- Soient $A, B, C$ trois événements dans un espace probabilisé. A-t-on  toujours: $A \perp\!\!\!\perp B \text{ et } B \perp\!\!\!\perp C \Rightarrow A \perp\!\!\!\perp C$?
- Soient $P$ et $Q$ deux lois de probabilités sur $(\Omega, \mathcal{F})$, on définit l'ensemble $\mathcal{M} = \big\{ A : A \in \mathcal{F}, P(A)=Q(A)\}$.  Répondre par *vrai/faux/je ne sais pas* aux questions suivantes:


    a. $\mathcal{M}$ est-il toujours une classe monotone ?
    a. $\mathcal{M}$ est-il toujours une $\sigma$-algèbre ?
    a. $\mathcal{M}$ est-il toujours une $\pi$-classe ?

- Soient $G$ et $F$  sont deux fonctions génératrices de probabilité.   Répondre par *vrai/faux* aux questions suivantes:

    a. Est-il vrai que $G \times F$ est toujours une fonction génératrice ?
    a. Est-il vrai que $G + F$ est toujours une fonction génératrice de probabilité ?
    a. Est-il vrai que $\lambda G + (1-\lambda) F$ avec $\lambda \in [0,1]$ est toujours une fonction génératrice de probabilité ?


- Si $\widehat{F}$ est la fonction caractéristique de la loi de $X$, et si $\epsilon \perp\!\!\!\perp X$,
  avec $P\{\epsilon=1\}= P\{\epsilon=-1\}=1/2$, quelle est la fonction caractéristique de la loi de $\epsilon X$?


### Exercice `r cnt_exo`


{{< include _increment.qmd >}}


Si $X$ est une variable aléatoire positive intégrable, la version *biaisée par la taille* de  $X$ est la variable aléatoire $X^*$ dont la loi $Q$ est absolument continue par rapport à celle de $X$ (notée $P$) et dont la densité (par rapport à celle de $X$)
est proportionnelle à $X$: 
$$
\frac{\mathrm{d}Q}{\mathrm{d}P}(x) = \frac{x}{\mathbb{E}X} \, .
$$

a. Caractériser $X^*$ lorsque $X$ est une Bernoulli.
a. Caractériser $X^*$ lorsque $X$ est binomiale.
a. Caractériser $X^*$ lorsque $X$ est Poisson.
a. Caractériser $X^*$ lorsque $X$ est Gamma.
a. Si $X$ est à valeurs entières, exprimer la fonction génératrice de $X^*$ en fonction de celle de $X$.
a. Exprimer la transformée de Laplace de $X^*$ en fonction de celle de $X$.
a. Si $U$ est une transformée de Laplace, dérivable à droite en $0$, $U'/U'(0)$ est-elle la
transformée de Laplace d'une loi sur $[0, \infty)$?


### Exercice `r cnt_exo`


{{< include _increment.qmd >}}


::: {.callout-note}

### Rappel

La loi normale centrée réduite $\mathcal{N}(0,1)$ admet pour densité $x\mapsto \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{x^2}{2}\right)$,

:::


- Si $X \sim \mathcal{N}(0,1)$, donner une densité de la loi de $Y=\exp(X)$ (Loi log-normale). Calculer l'espérance et la variance de $Y$.
- Même question si $X \sim \mathcal{N}(\mu, \sigma^2)$.
- Si $X, Y \sim \mathcal{N}(0,1)$, avec $X \perp\!\!\!\perp Y$, donner une densité de la loi de $Z=Y/X$ (Loi de Student à 1 degré de liberté)
- Si $X, Y \sim \mathcal{N}(0,1)$, avec $X \perp\!\!\!\perp Y$, donner une densité de la loi de $W = Y/ \sqrt{X^2}$. 
- Si $X \sim \mathcal{N}(0,1)$ et $\epsilon$ vaut $\pm 1$ avec probabilité $1/2$  (variable de Rademacher) avec   $X \perp\!\!\!\perp \epsilon$, donner une densité de la loi de $Y = \epsilon X$. $Y$ et $X$ sont-elles indépendantes ?


### Exercice `r cnt_exo`

{{< include _increment.qmd >}}


Principe de réflexion

Dans cet exercice, $X_1, X_2, \ldots$ sont des variables de Rademacher indépendantes ($P\{X_i = \pm 1\} = \frac{1}{2}$), $S_n =\sum_{i=1}^n X_i, S_0=0$ et  $M_n = \max_{k \leq n} S_n$.

Montrer que, pour $a> 0$,  

$$P\left\{ M_n > a \right\}\leq 2 P\left\{ S_n > a \right\}$$



::: {.callout-note}

### Statistique des rangs/Statistiques d'ordre


Les statistiques d'ordre $X_{1:n}\leq X_{2:n}\leq X_{n:n}$ d'un $n$-échantillon  $X_1,\ldots,X_n$
d'observations indépendantes identiquement distribuées sont formées par le réarrangement croissant (convention) de l'échantillon. 

Quand $n$  est clair d'après le contexte on peut les noter $X_{(1)} \leq \ldots \leq X_{(n)}$. 

:::

### Exercice `r cnt_exo`

{{< include _increment.qmd >}}

a. Vérifier que la loi jointe des statistiques d'ordre est absolument continue par rapport 
à la loi de l'échantillon.
a. On suppose que $X$ est une variable aléatoire réelle, absolument continue, de densité continue. Montrer que l'échantillon est presque sûrement formé de valeurs deux à deux distinctes. 
a. Donner la densité de la loi jointe des statistiques d'ordre.
a. Si la loi des $X_i$  définie par sa fonction de répartition $F$, admet une densité $f$, quelle est la densité de la loi de $X_{k:n}$  pour $1\leq k\leq n$ ?  
a.  Montrer que conditionnellement à $X_{k:n}=x$, la suite 

    $$(X_{i:n}-X_{k:n})_{i=k+1,\ldots, n}$$  

    est distribuée comme les statistiques d'ordre d'un $n-k$ échantillon de la loi d'excès au dessus de $x$  (fonction de survie $\overline{F}(x+\cdot)/\overline{F}(x))$ avec la convention $\overline{F}=1-F$). 
a. Si $X_1,\ldots,X_n$ est un échantillon i.i.d. de la loi exponentielle d'espérance $1$ (densité $\mathbb{I}_{x>0} \mathrm{e}^{-x}$), et $X_{n:n}\geq X_{n-1:n}\geq  \ldots \geq X_{1:n} $ les statistiques d'ordre associées, montrer que:

  i. avec la convention $X_{0:n}=0$, les écarts $(X_{i:n}-X_{i-1:n})_{1\leq i\leq n}$ (\emph{spacings}) forment une
  collection de variables aléatoires indépendantes ;
  i. $X_{i:n}-X_{i-1:n}$ est distribuée selon une loi exponentielle
  d'espérance $\tfrac{1}{i}$ .

a. Maintenant, $X_1,\ldots,X_n$ est un échantillon i.i.d. de la loi exponentielle d'espérance $1$ (densité $\mathbb{I}_{x>0} \mathrm{e}^{-x}$), et $X_{n:n}\geq X_{n-1:n}\geq  \ldots \geq X_{1:n}$ les statistiques d'ordre associées, et $(k_n)_n$ est une suite croissante d'entiers qui tend vers l'infini,  telle que $k_n/n$ tende vers une limite finie (éventuellement nulle).  Montrer que

    $$\frac{X_{k_n:n} -\mathbb{E} X_{k_n:n} }{\sqrt{\operatorname{var}(X_{k_n:n} )}}$$ 
    
    converge en loi vers une Gaussienne centrée réduite. 





