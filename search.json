[
  {
    "objectID": "corriges/td1.html",
    "href": "corriges/td1.html",
    "title": "Variables aléatoires réelles",
    "section": "",
    "text": "NoteTD I : Révisions de Licence\n\n\n\n\n8 Septembre 2025-12 Septembre 2025\nMaster I Isifar\nProbabilités"
  },
  {
    "objectID": "corriges/td1.html#fonctions-de-répartition",
    "href": "corriges/td1.html#fonctions-de-répartition",
    "title": "Variables aléatoires réelles",
    "section": "Fonctions de répartition",
    "text": "Fonctions de répartition\n\nExercice 1 (Transformation affine)  \n\nSoit \\(X\\) une variable aléatoire réelle, \\(F_X\\) sa fonction de répartition, \\(a, b\\) deux réels fixés, et \\(Y := aX+b\\)\n\nOn suppose dans cette question que \\(a=1\\). Comment déduire \\(F_Y\\) de \\(F_X\\)?\nSi (la loi de) \\(X\\) admet une densité, en est-il de même de \\(Y\\)? Si oui, exprimer dans ce cas \\(f_Y\\) à l’aide de \\(f_X\\).\nOn suppose dans cette question que \\(b=0\\) et \\(a&gt;0\\). Comment déduire \\(F_Y\\) de \\(F_X\\)?\nSi (la loi de ) \\(X\\) admet une densité, à quelle condition sur \\(a\\) en est-il de même de (la loi de) \\(Y\\)? Exprimer dans ce cas \\(f_Y\\) à l’aide de \\(f_X\\).\nRépondre aux mêmes questions lorsque \\(b=0\\) et \\(a=-1\\)?\nRépondre enfin aux mêmes questions lorsque \\(a\\) et \\(b\\) sont quelconques.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\nSoit \\(F_Y\\) la fonction de répartition de \\(Y\\).\n\nOn a\n\\[F_Y(x) = \\mathbb{P}(Y \\le x) = \\mathbb{P}(X+b \\le x) = \\mathbb{P}(X \\le x-b) = F_X(x-b)\\]\nAinsi, le graphe de \\(F_Y\\) se déduit de celui de \\(F_X\\) en le translatant de \\(b\\) vers la droite. Si \\(X\\) admet une densité \\(f_X\\),\n\\[F_Y(x) = \\int_{-\\infty}^{x-b} f_X(u) du = \\int_{-\\infty}^b f_Y(v) dv,\\]\noù \\(f_Y(v) = f_X(v-b), u \\in \\mathbb{R}\\).\nAinsi, si \\(X\\) admet une densité \\(f_X\\), \\(Y\\) admet la densité \\(f_Y\\) définie ci-dessus.\nOn a dans ce cas \\[F_Y(x) = \\mathbb{P}(Y \\le x) = \\mathbb{P}(aX \\le x) = \\mathbb{P}(X \\le \\frac{x}{a}) = F_X(\\frac{x}{a}).\\] Ainsi le graphe de \\(F_Y\\) se déduit de celui de \\(F_X\\) en dilatant ce dernier par \\(a\\).\nSi \\(X\\) admet une densité \\(f_X\\), on a \\[F_Y(x) = \\int_{-\\infty}^{\\frac{x}{a}} f_X(u) du = \\int_{-\\infty}^x \\frac{1}{a} f_X(\\frac{v}{a}) dv = \\int_{-\\infty}^x f_Y(v) dv,\\] où \\(f_Y(v) = \\frac{1}{a} f_X(\\frac{v}{a}), v \\in \\mathbb{R}\\). Ainsi, si \\(X\\) admet une densité \\(f_X\\), alors \\(Y\\) admet la densité \\(f_Y\\) définie ci-dessus.\nDans ce cas \\[F_Y(x) = \\mathbb{P}(-X \\le x) = \\mathbb{P}(X \\ge -x) = 1-F_X((-x)^-).\\] Si \\(X\\) admet une densité \\(f_X\\), \\[F_Y(x) = \\int_{-x}^{\\infty} f_X(u) du = \\int_{-\\infty}^x f_X(-v) dv = \\int_{-\\infty}^x f_Y(v)dv,\\] où \\(f_Y(v) = f_X(-v), x \\in \\mathbb{R}\\). Ainsi, si \\(X\\) admet une densité \\(f_X\\), alors \\(Y\\) admet la densité \\(f_Y\\) définie ci-dessus.\nTraitons d’abord le cas \\(a=0\\). Dans ce cas, quelque soit \\(X\\), on obtient que \\(Y\\) est déterministe, \\(\\mathbb{P}(Y=b)=1\\) (en particulier \\(Y\\) n’a pas de densité même si \\(X\\) en possède une.\nLorsque \\(a &gt; 0\\), on obtient par un raisonnement similaire à 1,2, \\[F_Y(x) = F_X\\left(\\frac{x-b}{a}\\right),\\] et si \\(X\\) possède la densité \\(f_X\\), alors \\(Y\\) possède la densité \\(f_Y\\) telle que \\(f_Y(v) = \\frac{1}{a} f_X\\left( \\frac{v-b}{a} \\right), v \\in \\mathbb{R}\\).\nEnfin lorsque \\(a&lt;0\\), \\[F_Y(x) = \\mathbb{P}(aX+b \\le x) = \\mathbb{P}(aX \\le x-b) = \\mathbb{P}\\left(X \\ge \\frac{x-b}{a}\\right) = 1-F_X\\left(\\left(\\frac{x-b}{a}\\right)^-\\right),\\] et si \\(X\\) possède la densité \\(f_X\\), \\[F_Y(x) = \\int_{\\frac{x-b}{a}}^{+\\infty} f_X(u) du = \\frac{1}{|a|} \\int_{-\\infty}^{x} f_X\\left(\\frac{v-b}{a}\\right) dv,\\] de sorte que \\(Y\\) possède la densité \\(f_Y\\) où \\(f_Y(v) = \\frac{1}{|a|} f_X\\left(\\frac{v-b}{a}\\right), v \\in \\mathbb{R}\\).\n\n\n\n\nExercice 2 (Minimum, maximum de variables indépendantes)  \n\nSoient \\(X_i, i \\ge 1\\), des variables indépendantes. Pour \\(k \\ge 2\\), on note \\(Y_k = \\min (X_1,...,X_k)\\), \\(Z_k = \\max(X_1,...,X_k).\\)\n\nDans cette question on s’intéresse à \\(k=2\\).\nComment déduire \\(F_{Y_2}\\) de \\(F_{X_1}, F_{X_2}\\)? Même question pour \\(F_{Z_2}\\).\nGénéraliser à \\(k\\) quelconque.\nQuelle est la loi de \\(Z_k\\) lorsque les \\(\\{X_i, i \\ge 1\\}\\) sont i.i.d., \\(\\sim \\mathrm{Unif}[0,1]\\)?\nQuelle est la loi de \\(Y_k\\) lorsque les \\(\\{X_i, i \\ge 1\\}\\) sont des variables exponentielles indépendantes, avec \\(X_i \\sim \\text{Exp}(\\lambda_i)\\) (où pour tout \\(i\\), \\(\\lambda_i &gt;0\\))?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nOn a pour tout \\(x \\in \\mathbb{R}\\), \\[1-F_{Y_2}(x) = \\mathbb{P}(Y_2 &gt; x) = \\mathbb{P}(\\min(X_1,X_2) &gt;x)) = \\mathbb{P}(X_1 &gt;x, X_2 &gt;x) = \\mathbb{P}(X_1&gt;x) \\mathbb{P}(X_2&gt;x),\\] où pour la dernière égalité on s’est servi de l’indépendance de \\(X_1\\) et \\(X_2\\). On déduit que pour tout \\(x \\in \\mathbb{R}\\), \\[1-F_{Y_2}(x) = (1-F_{X_1}(x)) (1-F_{X_2}(x)).\\]\n\nDe manière similaire, on obtient pour tout \\(x \\in \\mathbb{R}\\) \\[F_{Z_2}(x) = F_{Y_1}(x) F_{Y_2}(x).\\]\n\nPar un raisonnement similaire, pour tout \\(x \\in \\mathbb{R}\\),\n\\[ 1-f_{Y_k}(x) = \\prod_{i=1}^k (1-F_{X_i}(x))\\]\nPar ailleurs, pour tout \\(x \\in \\mathbb{R}\\), \\[F_{Z_k}(x) = \\mathbb{P}(\\max(X_1,...,X_k) \\le x) = \\mathbb{P}(X_i \\le x, 1 \\le i \\le k) = \\prod_{i=1}^k \\mathbb{P}(X_i \\le x)\\] où pour la dernière égalité on s’est servi de l’indépendance des \\((X_i, 1 \\le i \\le k)\\). On déduit, pour tout \\(x \\in \\mathbb{R}\\), \\[F_{Z_k}(x) = \\prod_{i=1}^k F_{X_i}(x)\\]\nDans ce cas \\(F_{X_i}(x) = x \\mathbb{I}_{[0,1]}(x) + \\mathbb{I}_{]1,+\\infty[}(x)\\), et donc\n\\[F_{Z_k}(x) = \\begin{cases} 0 & \\text{si } x \\le 0\\\\ x^k & \\text{si } 0 \\le x \\le 1 \\\\  1 & \\mbox{si } x \\ge 1 \\end{cases},\\]\non déduit donc que \\(Z_k\\) est une variable de densité \\[f_{Z_k}(x) = k x^{k-1} \\mathbf{1}_{[0,1]}(x), x \\in \\mathbb{R}.\\]\nDans ce cas \\(1-F_{X_i}(x) = \\exp(-\\lambda_i x) \\mathbf{1}_{\\{x \\ge 0\\}} + \\mathbf{1}_{\\{x&lt;0\\}}\\), on a donc\n\\[1-F_{Y_k}(x) = \\begin{cases} 1 & \\mbox{si } x \\le 0 \\\\ \\exp\\left(-x \\sum_{i=1}^k \\lambda_i\\right)  & \\text{sinon}\\end{cases}\\]\nde sorte que \\(Y_k \\sim \\exp(\\Lambda)\\), avec \\(\\Lambda = \\sum_{i=1}^k \\lambda_i\\).\n\n\n\n\nExercice 3  \n\nOn suppose que \\(X \\sim \\mathcal{N}(0,1)\\). Que valent\n\\[\\mathbb{P}(X \\le 1), \\quad \\mathbb{P}(-1.23 \\le X \\le 0.43), \\quad \\mathbb{P}(X&gt;0.32)?\\]\n\n\n\n\n\n\nNoteSolution\n\n\n\nEn utillisant le logiciel R, la fonction de répartition de la loi \\(\\mathcal{N}(0,1)\\) est désignée par pnorm(), la fonction réciproque (fonction quantile) est désignée par qnorm\n\n\n\n\n\n\n\n\n\n\n\\(\\approx\\)\n\n\n\n\n\\(\\mathbb{P}(X \\le 1)\\)\npnorm(1)\n0.84\n\n\n\\(\\mathbb{P}(-1.23 \\le X \\le 0.43)\\)\npnorm(.43) - pnorm(-1.23)\n0.56\n\n\n\\(\\mathbb{P}(X &gt; .32 1)\\)\n1 - pnorm(.32)\n0.37\n\n\n\n\n\n\nExercice 4  \n\nOn suppose que l’écart à la taille moyenne \\(T=15.5\\) des individus d’une population suit une loi normale centrée réduite.\nDans quel intervalle centré en \\(T\\) se situent les tailles de \\(99\\%\\) des individus de la population?\n\n\n\n\n\n\nNoteSolution\n\n\n\nD’après la table, pour \\(Z \\sim \\mathcal{N}(0,1)\\), on a \\(\\mathbb{P}(Z \\le 2.57) \\approx 0.9949),\\) et \\(\\mathbb{P}(Z \\le 2.58) \\approx 0.9951\\), de sorte que \\(\\mathbb{P}(|Z| \\ge 2.57) \\approx 0.0102\\), et \\(\\mathbb{P}(|Z| \\ge 2.58) \\approx 0.0098\\).\nOn déduit que les tailles de \\(99\\%\\) de la population se situent entre \\(15.5 - 2,58 = 12.92\\) et \\(15.5+2.58 = 18.08\\).\n\n\n\nExercice 5  \n\nEtant donnée X une variable aléatoire gaussienne de paramètres \\(\\mu\\) et \\(\\sigma^2\\), donner la probabilité que \\(|X - \\mu|\\) dépasse \\(k\\sigma\\) pour \\(k = 1, 2, 3\\).\nSuggestion: On commencera par montrer que \\(\\sigma^{-1}(X-\\mu)\\) suit une loi normale centrée réduite.\nReprendre les questions de l’exercice précédent lorsque \\(\\mu=2, \\sigma=2\\).\nReprendre les questions de l’exercice précédent lorsque \\(\\mu=0, \\sigma=1/2\\).\n\n\n\n\n\n\nNoteSolution\n\n\n\nPuisque \\((X-\\mu)/\\sigma \\sim \\mathcal{N}(0,1)\\) on a\n\\[\\mathbb{P}(|X- \\mu| \\ge k \\sigma)= \\mathbb{P}(|Z| \\ge k),\\] et donc d’après la table, pour \\(k=1\\) ceci vaut\n\\[\\mathbb{P}(|Z| \\ge 1) =  2 (1-\\mathbb{P}(Z \\le 1)) \\approx 2 \\times (1-0.8413) = 2 * 0.1587 = 0.3174\\] pour \\(k=2\\),\n\\[\\mathbb{P}(|Z| \\ge 2) = 2(1-\\mathbb{P}(Z \\le 2) \\approx 2 \\times (1- 0.9772) = 0.0456.\\] enfin pour \\(k=3\\),\n\\[\\mathbb{P}(|Z| \\ge 3) = 2 (1- \\mathbb{P}(Z \\le 3) \\approx 2 \\times (1-0.9987) = 0.0026.\\]\nPour \\(\\mu=2, \\sigma=2\\), on a\n\\[\\mathbb{P}(X \\le 1) = \\mathbb{P}(Z \\le -1/2) \\approx 0.3085,\\]\n\\[ \\mathbb{P}(-1.23 \\le X \\le 0.43) = \\mathbb{P}(-\\frac{3.23}{2} \\le Z \\le -\\frac{1.57}{2}) \\approx 0.9463-0.7823 = 0.1640,\\]\n\\[\\mathbb{P}(X &gt; 0.32) = \\mathbb{P}(Z&gt;-0.84) \\approx 0.7995.\\]\nPour \\(\\mu=0, \\sigma=1/2\\), on trouve \\[\\mathbb{P}(X \\le 1) = \\mathbb{P}(Z \\le 2) \\approx 0.9772\\]\n\\[\\mathbb{P}(-1.23 \\le X \\le 0.43 ) = \\mathbb{P}(-2.46 \\le Z \\le 0.86) \\approx 0.8051 - (1-0.9931) = 0.7982,\\]\n\\[\\mathbb{P}(X&gt;0.32) = \\mathbb{P}(Z &gt; 0.64) \\approx 1-0.7389 = 0.2611\\]"
  },
  {
    "objectID": "corriges/td1.html#densités",
    "href": "corriges/td1.html#densités",
    "title": "Variables aléatoires réelles",
    "section": "Densités",
    "text": "Densités\n\nExercice 6  \n\nDans les cas suivants, trouver la valeur de \\(C\\) pour que \\(f\\) soit une densité de probabilité.\n\n\\(f(x) = C \\frac{1}{\\sqrt{x (1-x)}}, 0 &lt;x &lt;1,\\)\n\\(f(x) = C \\exp(-x-\\exp(-x)), x \\in \\mathbb{R},\\)\n\\(f(x) = C \\frac{1}{1+x^2}\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nOn a \\[x(1-x) = x-x^2 = \\frac{1}{4} - \\left(x-\\frac{1}{2}\\right)^2\\] et on sait que la primitive de \\(x \\to \\frac{1}{\\sqrt{a^2-x^2}}\\) est \\(x \\to \\arcsin\\left(\\frac{x}{a}\\right)\\). On a donc en effectuant le changement de variable \\(y = x-1/2\\), \\[\\int_{0}^1 \\frac{1}{\\sqrt{x(1-x)}} = \\int_{-1/2}^{1/2} \\frac{dy}{\\sqrt{\\frac{1}{4}-y^2}} = \\left[\\arcsin(2y)\\right]_{-1/2}^{1/2} = \\pi,\\] et il faut donc poser \\(C = \\frac{1}{\\pi}\\) pour que \\(f\\) soit une densité de probabilité sur \\(]0,1[\\).\n\nSoit \\(h(x) = \\exp(-\\exp(-x)), x \\in \\mathbb{R}\\), on a pour \\(x \\in \\mathbb{R}\\), \\[h'(x) =\\exp(-x) \\exp(-\\exp(-x)) = \\exp(-x-\\exp(-x)),\\] de sorte que \\[ \\int_{-\\infty}^{\\infty} \\exp(-x-\\exp(-x)) = \\left[\\exp(-\\exp(-x))\\right]_{-\\infty}^{\\infty} = 1,\\] et il faut poser \\(C=1\\) pour que \\(f\\) soit une densité de probabilité sur \\(\\mathbb{R}\\).\n\nOn a \\[\\int_{\\infty}^{\\infty} \\frac{dx}{1+x^2} = \\left[ \\arctan(x) \\right]_{-\\infty}^{\\infty} = \\pi,\\] et il faut donc poser \\(C=\\frac{1}{\\pi}\\) pour que \\(f\\) soit une densité de probabilité sur \\(\\mathbb{R}\\).\n\n\n\n\nExercice 7  \n\n(Mélange)\nOn suppose que \\(X\\) et \\(Y\\) sont deux variables de densités respectives \\(f_X, f_Y\\), et que \\(\\alpha \\in [0,1]\\). Montrer que \\(g : = \\alpha f_X + (1-\\alpha) f_Y\\) est également une densité de probabilité.\nTrouver une variable aléatoire dont \\(g\\) est la densité.\n\n\n\n\n\n\nNoteSolution\n\n\n\nLa fonction \\(g\\) reste bien évidemment borélienne, et puisque \\(\\alpha \\in [0,1]\\), positive, de plus\n\\[\\int_{\\mathbb{R}} g(x) dx = \\alpha \\int_{\\mathbb{R}} f_X(x) dx + (1-\\alpha) \\int_{\\mathbb{R}} f_Y(x) dx = \\alpha + (1-\\alpha) = 1,\\]\non conclut que \\(g\\) est bien une densité de probabilité.\nSoit \\(\\xi \\sim \\mathrm{Ber}(\\alpha)\\), indépendante de \\((X,Y)\\). Alors \\(Z= \\xi X + (1-\\xi)Y\\) possède la densité \\(g\\). En effet, en utilisant l’indépendance de \\(\\xi\\) et \\((X,Y)\\) à la deuxième ligne ci-dessous,\n\\[\\begin{align*}\n\\mathbb{P}(Z \\le x) & =  \\mathbb{P}(\\xi=1,  X \\le x) + \\mathbb{P}(\\xi=0, Y \\le x) \\\\ & =  \\mathbb{P}(\\xi=1) \\mathbb{P}(X \\le x) + \\mathbb{P}(\\xi=0) \\mathbb{P}(Y \\le x) \\\\ & =  \\alpha F_X(x) + (1-\\alpha) F_Y(x) \\\\ & =  \\int_{-\\infty}^x (\\alpha f_X(x) + (1-\\alpha)f_Y(x)) dx\n\\end{align*}\\]\n\n\n\nExercice 8  \n\nSoit \\(X\\) de densité \\(f\\), et \\((\\alpha, \\beta) \\in \\overline{\\mathbb{R}}^2\\) sont supposés tels que \\[\\mathbb{P}(\\alpha &lt; X &lt; \\beta) =1\\] On suppose que \\(g\\) est un \\(C^{1}\\)-difféomorphisme croissant de \\((\\alpha, \\beta)\\) sur \\((g(\\alpha),g(\\beta))\\).\n\nMontrer que \\(g(X)\\) a pour densité \\(\\frac{f(g^{-1}(x))}{g'(g^{-1}(x))} \\mathbf{1}_{\\{x \\in (g(\\alpha), g(\\beta))\\}}\\).\nQuelle est la densité de la variable \\(aX+b\\), où \\(a&gt;0\\) et \\(b\\in \\mathbb{R}\\) sont fixés?\n\nSoit \\(Y \\sim \\mathcal{N}(0,1)\\). Quelle est la densité de \\(Z = \\exp(Y)\\)?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nPosons \\(T= g(X)\\). Pour \\(h : \\mathbb{R} \\to \\mathbb{R}_+\\) mesurable, on a \\[\\mathbb{E}[h(T)]  = \\mathbb{E}[h(g(X))] = \\int_{\\alpha}^{\\beta} h(g(x)) f(x) dx\\]\nEn effectuant le changement de variables \\(t = g(x)\\) il vient \\[ \\mathbb{E}[h(T)]  = \\int_{g(\\alpha)}^{g(\\beta)} \\frac{h(t) f(g^{-1}(t))}{g'(g^{-1}(t))} dt. \\] Comme ceci est valable pour tout fonction \\(h\\) mesurable positive, on conclut que \\(T\\) possède la densité souhaitée sur \\((g(\\alpha),g(\\beta))\\)\nIci \\(g : x \\to ax+b\\), \\(g^{-1} : x \\to \\frac{x-b}{a}\\), \\(g'(x) = a\\) pour tout \\(x\\) et dans ce cas la densité recherchée s’exprime donc \\[\\frac{1}{a} f\\left( \\frac{x-b}{a} \\right) \\mathbf{1}_{(a\\alpha+b, a\\beta+b)}(x), \\ x \\in \\mathbb{R}.\\] Quitte à prendre \\(\\alpha = -\\infty, \\beta = +\\infty\\), on retrouve le résultat de l’exercice 2.\nIci \\(\\alpha = - \\infty, \\beta=+\\infty\\), \\(f(y) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-y^2/2), y \\in \\mathbb{R}\\), et \\(g: x \\to \\exp(x)\\), \\(g^{-1} : y \\to \\ln(y)\\) et \\(g'(g^{-1}(x)) = x\\). On obtient donc que la densité de \\(Z\\) est \\[\\frac{1}{x \\sqrt{2\\pi}} \\exp\\left(-\\frac{\\ln(x)^2}{2}\\right) \\mathbf{1}_{\\{x &gt;0\\}}.\\]"
  },
  {
    "objectID": "corriges/td1.html#lois-usuelles-calculs-de-loi",
    "href": "corriges/td1.html#lois-usuelles-calculs-de-loi",
    "title": "Variables aléatoires réelles",
    "section": "Lois usuelles, calculs de loi",
    "text": "Lois usuelles, calculs de loi\n\nExercice 9  \n\n(Fonctions de répartition et fonctions caractéristiques de lois usuelles)\n\n\n\n\n\n\nAttention : dans le cas d’une variable continue, quand on calcule \\(\\Phi_X\\) on %doit intégrer sur \\(\\mathbb{R}\\) une fonction complexe. Trois méthodes sont envisageables.\nParfois on peut intégrer séparément partie réelle et partie imaginaire.\nParfois il est utile de se servir de la formule de Cauchy. En particulier, cette formule assure que si \\(f\\) est une fonction holomorphe, si \\(\\mathcal{C}\\) est un contour fermé “raisonnable” (en particulier tout cercle ou polygone régulier), et enfin si \\(\\overset{\\circ}{\\mathcal{C}}\\) désigne l’ensemble des points se trouvant à l’intérieur de ce contour, alors\n\\[\\forall a \\in \\overset{\\circ}{\\mathcal{C}}, \\quad f(a) = \\frac{1}{2\\pi i} \\oint_{\\mathcal{C}}  \\frac{f(z)}{z-a} \\mathrm{d}z.\\]\nAttention : cette formule montre bien que l’on ne peut pas traiter l’intégrale d’une fonction complexe en faisant “comme si” \\(i\\) était réél (!) La méthode des résidus est en outre une conséquence directe de la formule de Cauchy.\nEnfin, on peut utiliser le prolongement analytique (voir l’exemple de la fonction \\(\\Gamma\\)).\n\n\n\n\nExprimer le plus simplement \\(F_X\\) dans les cas suivants (on pourra se contenter de tracer l’allure du graphe de la fonction de répartition lorsque celle-ci ne possède pas d’expression simple).\n\n\\(n \\in \\mathbb{N}^*, p \\in [0,1], X \\sim \\text{Bin}(n,p)\\),\n\\(\\lambda&gt;0, X \\sim \\text{Poisson}(\\lambda)\\),\n\\(a&gt;0, X\\sim \\text{Unif}[-a,a]\\),\n\\(\\lambda&gt;0, X \\sim \\mathbf{exp}(\\lambda)\\),\n\\(\\lambda&gt;0, s \\in \\mathbb{N}^*\\), \\(X \\sim \\Gamma(\\lambda, s)\\) (on rappelle que la densité \\(f_X\\) de \\(X \\sim \\Gamma(\\lambda,s)\\) est telle que \\(f_X(x) =\\frac{1}{\\Gamma(s)} \\lambda^s x^{s-1}\\exp(-\\lambda x) \\mathbf{1}_{[0,\\infty[}(x), x \\in \\mathbb{R}\\)),\n\\(X \\sim \\mathcal{N}(0,1)\\),\n\\(\\mu \\in \\mathbb{R}, \\sigma&gt;0, X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\).\n\n\\(a&gt;0, X \\sim \\mathrm{Cauchy}(a)\\) (on rappelle que la densité \\(f_X\\) de la loi de Cauchy de paramètre \\(a\\) est telle que \\(f_X(x) = \\frac{a}{\\pi(x^2+a^2)}, x\\in \\mathbb{R}\\)).\n\n(*) \\(X \\sim \\mathrm{stable}(1/2)\\) (cette loi a pour densité \\(\\sqrt{2\\pi x^{-3}} \\exp(-1/2x)\\mathbf{1}_{[0,\\infty[}(x).\\))\n\nLesquelles parmi ces variables possèdent une densité?\n\nExprimer le plus simplement \\(\\Phi_X\\) pour les \\(7\\) premières variables de la première question ci-dessus. En déduire, ou trouver par un calcul direct, \\(E[X],\\) et \\(\\mathrm{Var}[X]\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nBinômiale Il s’agit d’une variable discrète à valeurs dans \\([|0,n|]\\), (elle ne possède donc pas de densité), et telle que \\[\\mathbb{P}(X=k) = {n \\choose k} p^k (1-p)^{n-k}, \\ 0\\le k \\le n\\] On a\n\\[F_X(x) = \\sum_{k=0}^n \\mathbf{1}_{\\{x \\ge k\\}} {n \\choose k} p^k(1-p)^{n-k}, x \\in \\mathbb{R}.\\] \\(X\\) a même loi que \\(\\sum_{j=1}^n \\xi_j\\) où les \\((\\xi_j, 0 \\le j \\le n)\\) i.i.d suivant la loi Ber\\((p)\\), de sorte que \\[\\Phi_X(t) = \\mathbb{E}[\\exp(itX)] = \\Phi_{\\xi_1}(t)^n = (p\\exp(it)+(1-p))^n, \\ t \\in \\mathbb{R}\\] On a enfin, toujours gr^ace à l’écriture de \\(X\\) comme somme de \\(n\\) variables de Bernoulli indépendantes et de même paramètre \\(p\\), \\[\\mathbb{E}[X] = np , \\qquad \\mathrm{Var}[X] = np(1-p)\\]\nPoisson Il s’agit d’une variable discrète à valeurs dans \\(\\mathbb{N}\\) (elle ne possède donc pas de densité) et telle que \\[ \\mathbb{P}(X=k) = \\lambda^k \\frac{\\exp(-\\lambda)}{k!}, k \\in \\mathbb{N}.\\] On a \\[ F_X(x) = \\sum_{k \\in \\mathbb{N}} \\mathbf{1}_{\\{x \\ge k\\}} \\exp(-\\lambda)\\frac{\\lambda^k}{k!}, x \\in \\mathbb{R}\\] et \\[ \\Phi_X(t) = \\sum_{ k \\in \\mathbb{N}} \\exp(itk) \\exp(-\\lambda) \\frac{\\lambda^k}{k!} = \\exp(\\lambda \\exp(it) -1), t \\in \\mathbb{R}\\] On a par un calcul direct \\[ \\mathbb{E}[X] = \\mathrm{Var}[X] = \\lambda.\\]\nUniforme continue symétrique\nIl s’agit de la loi de densité \\(\\frac{1}{2a} \\mathbf{1}_{[-a,a]}\\). On a pour \\(x \\in \\mathbb{R}\\),\n\\[F_X(x) = \\begin{cases}  0 & \\mbox{si } x \\le -a \\\\\n  \\frac{1}{2a} (x+a) & \\mbox{si } x \\in [-a,a] \\\\\n  1 & \\mbox{si } x \\ge a \\end{cases}\\]\net pour \\(t \\in \\mathbb{R}\\),\n\\[\\Phi_X(t) = \\begin{cases} 1 & \\mbox{si } t =0 \\\\ \\frac{\\sin(ta)}{ta} & \\mbox{si } t \\ne 0.\\end{cases}\\]\nOn a par un calcul direct\n\\[\\mathbb{E}[X] = 0, \\mathrm{Var}[X] = \\frac{a^2}{3}\\]\nExponentielle\nIl s’agit de la loi de densité \\(x \\to \\lambda \\exp(-\\lambda x) \\mathbf{1}_{\\{x \\ge 0\\}}\\). On a pour \\(x \\in \\mathbb{R}\\),\n\\[F_X(x) =  \\begin{cases}  0 & \\mbox{si } x \\le 0 \\\\\n  1-\\exp(-\\lambda x) & \\mbox{si } x \\ge 0 \\end{cases}\\]\net\n\\[\\Phi_X(t) = \\frac{\\lambda}{\\lambda-it}, t\\ in \\mathbb{R}\\]\nOn a enfin par un calcul direct (i.p.p)\n\\[\\mathbb{E}[X] = \\frac{1}{\\lambda}, \\ \\ \\mathrm{Var}[X] = \\frac{1}{\\lambda^2}\\]\nGamma\nLa densité est rappelée en énoncé. La fonction de répartition n’admet pas en général de forme plus simple que \\(\\int_{-\\infty}^x f_X(u)du\\). On a de plus\n\\[\\Phi_X(t) = \\left(\\frac{\\lambda}{\\lambda-it} \\right)^s, t \\in \\mathbb{R}\\]\nQuitte à calculer la dérivée première et seconde en \\(0\\) de \\(\\Phi_X\\) on trouve\n\\[\\mathbb{E}[X] = \\frac{s}{\\lambda}, \\qquad \\mathrm{Var}[X] = \\frac{s}{\\lambda^2}.\\]\nGaussienne centrée réduite\nIl s’agit de la loi de densité \\(f_X : x \\to \\frac{1}{\\sqrt{2\\pi}} \\exp(-x^2/2)\\), la fonction \\(F_X\\) n’a pas de forme plus simple que \\(\\int_{-\\infty}^x f_X(u)du\\), et \\[ \\Phi_X(t) = \\exp(-t^2/2), \\ t\\in \\mathbb{R}\\] On a (soit par calcul direct, soit en dérivant \\(\\Phi_X\\)) \\[\\mathbb{E}[X] = 0, \\quad \\mathrm{Var}[X] =1\\]\nGaussienne\nSi \\(Z \\sim \\mathcal{N}(0,1)\\), on a \\(X = \\mu + \\sigma Z \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), et \\(X\\) a pour densité \\[f_X : x \\to \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-\\mu)^2}{2}\\right).\\] la fonction \\(F_X\\) n’a pas de forme plus simple que \\(\\int_{-\\infty}^x f_X(u)du\\) et \\[\\Phi_X(t) = \\exp\\left(it \\mu - \\frac{\\sigma^2 t^2}{2}\\right), t\\in \\mathbb{R}.\\] Puisque \\(X = \\mu + \\sigma Z\\) on trouve que \\[\\mathbb{E}[X] = \\mu, \\quad \\mathrm{Var}[X]= \\sigma^2.\\]\nCauchy\nLa densité est rappelée dans l’énoncé, on a pour \\(x \\in \\mathbb{R}\\), \\[F_X(x) = \\frac{1}{\\pi} \\left(\\frac{\\pi}{2}+ \\arctan\\left(\\frac{x}{a}\\right)\\right),\\] et \\[ \\Phi_X(t) = \\exp(-a|t|), t \\in \\mathbb{R}.\\]\nStable \\((1/2)\\)\nLa loi stable\\((1/2)\\) a la densité rappelée en énoncé (la fonction \\(F_X\\) n’a pas de forme plus simple que \\(\\int_{-\\infty}^x f_X(u)du\\)), sa fonction caractéristique est donnée par \\[\\Phi_X(t) = \\exp(-\\sqrt{|t|}(1+\\mathrm{sgn}(t)i), \\ t\\in \\mathbb{R}.\\]\net on note que \\(x \\to x f_X(x)\\) n’est pas intégrable (donc \\(\\mathbb{E}[X]= +\\infty\\), \\(\\mathrm{Var}[X]=+\\infty\\)).\n\n\n\n\nExercice 10  \n\nPour des valeurs de \\(t\\) que l’on précisera, calculer la transformée de Laplace \\(L(t) := \\mathbb{E}[\\exp(-t X)]\\) et la fonction génératrice des moments \\(G(u) := \\mathbb{E}[u^X]\\) de la variable \\(X\\) dans les cas suivants.\n\n\\(X \\sim \\mathrm{Ber}(p)\\), où \\(p \\in [0,1]\\),\n\\(X \\sim \\mathrm{Bin}(n,p)\\), où \\(n \\in \\mathbb{N}^*, p \\in [0,1]\\),\n\\(X \\sim \\mathrm{Geom}(p)\\), où \\(p \\in [0,1]\\),\n\\(X \\sim \\mathrm{Poisson}(\\lambda)\\), où \\(\\lambda&gt;0\\),\n\n\\(X=Y_1+...+Y_n\\), où les \\(Y_i, 1 \\le i \\le n\\) sont des variables indépendantes, et \\(Y_i \\sim \\mathrm{Poisson}(\\lambda_i)\\), avec \\(\\lambda_i &gt;0\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\\(G(u) = 1-p + pu, u \\in \\mathbb{R}\\), \\(L(t) = (1-p) + p \\exp(-t) = G(\\exp(-t)), t \\in \\mathbb{R}\\).\n\\(G(u) = (1-p + pu)^n, u \\in \\mathbb{R}\\), \\(L(t)= G(\\exp(-t)), t \\in \\mathbb{R}\\)\n\\(G(u) = \\frac{up}{1-u(1-p)}, |u|&lt;\\frac{1}{1-p}\\), et \\(L(t) = G(\\exp(-t)),  t &gt; \\ln(1-p)\\).\n\\(G(u) = \\exp(\\lambda(u-1)), u \\in \\mathbb{R}\\), et \\(L(t) = G(\\exp(-t)), t \\in \\mathbb{R}\\)\n\\(G(u) = \\exp(\\Lambda(u-1)), u \\in \\mathbb{R}\\) où \\(\\Lambda = \\sum_{i=1}^n \\lambda_i\\), \\(L(t) = G(\\exp(-t)), t \\in \\mathbb{R}\\)\n\n\n\n\nExercice 11  \n\nSoit \\(X\\) une v.a.r. de densité \\(f\\). Quelle est la densité de \\(X^2\\)? Qu’obtient-on dans le cas où \\(X \\sim \\mathcal{N}(0,1)\\)?\n\n\n\n\n\n\nNoteSolution\n\n\n\nSoit \\(\\phi : \\mathbb{R} \\to \\mathbb{R}\\) borélienne positive. et \\(Y=X^2\\). On a, en effectuant le changement de variables \\(u=x^2\\) à la quatrième ligne ci-dessous,\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(Y^2)] & =  \\mathbb{E}[\\phi(X^2)] \\\\ & =  \\int_{\\mathbb{R}} \\phi(x^2) f(x) dx \\\\ & =  \\int_{\\mathbb{R}_-}\\phi(x^2) f(x) dx + \\int_{\\mathbb{R}_+} \\phi(x^2) f(x) dx \\\\ & =  \\int_{0}^{\\infty} \\phi(u) f(-\\sqrt{u}) \\frac{du}{2\\sqrt{u}} + \\int_0^{\\infty} \\phi(u) f(\\sqrt{u})\\frac{du}{2\\sqrt{u}} \\\\  & =  \\int_0^{\\infty} \\phi(u) \\frac{f(-\\sqrt{u}) + f(\\sqrt{u})}{2\\sqrt{u}} du\n\\end{align*}\\]\nComme l’égalité ci-dessus est valable pour toute \\(\\phi : \\mathbb{R} \\to \\mathbb{R}\\) borélienne positive, on conclut que \\(Y\\) possède la densité\n\\[u\\to f_Y(u)  = \\frac{f(-\\sqrt{u}) + f(\\sqrt{u})}{2\\sqrt{u}} \\mathbf{1}_{\\{u&gt;0\\}}.\\]\nDans le cas où \\(X \\sim \\mathcal{N}(0,1)\\), on obtient\n\\[f_Y(u) = \\frac{1}{\\sqrt{2\\pi}} \\frac{\\exp\\left(-\\frac{u}{2}\\right)}{\\sqrt{u}} \\mathbf{1}_{\\{u &gt; 0\\}},\\]\nde sorte que \\(Y \\sim \\Gamma(1/2,1/2)\\).\n\n\n\nExercice 12  \n\nSoit \\(X \\sim \\exp(1)\\). Calculer la densité des variables suivantes :\n\n\\(Y = aX+b\\), où \\(a&gt;0\\) et \\(b \\in \\mathbb{R}\\). Qu’observe-t-on dans le cas où \\(b=0\\)?\n\n\\(Z = X^2\\).\n\\(U = \\exp(-X)\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nOn a d’après l’exercice 2\n\\[f_Y(u) = \\frac{1}{a} \\exp\\left(- \\frac{u-b}{a}\\right)\\mathbf{1}_{\\{u \\ge 0\\}}, u \\in \\mathbb{R} \\]\nLorsque \\(b =0\\), on constate que \\(Y \\sim \\exp\\left(\\frac{1}{a}\\right)\\)\nD’après l’exercice précédent\n\\[f_Z(u) = \\frac{\\exp(- \\sqrt{u})}{\\sqrt{u}} \\mathbf{1}_{\\{u&gt;0\\}}, u \\in \\mathbb{R}.\\]\nSoit \\(\\phi : \\mathbb{R} \\to \\mathbb{R}_+\\) borélienne, on a, en effectuant le changement de variables \\(u=\\exp(-x)\\) à la troisième ligne ci-dessous\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(U)]\n& = \\mathbb{E}[\\phi(\\exp(-X)] \\\\\n& =  \\int_0^{\\infty} \\phi(\\exp(-x)) \\exp(-x) dx \\\\\n& =  \\int_{0}^1 \\phi(u) du,\n\\end{align*}\\]\net, comme cette égalité est valable pour tout \\(\\phi : \\mathbb{R} \\to \\mathbb{R}_+\\) borélienne, on conclut que \\(U \\sim \\mathrm{Unif}[0,1]\\).\n\n\n\n\nExercice 13  \n\nTrouver la loi de \\(\\arcsin(X)\\) lorsque\n\n\\(X \\sim \\mathrm{Unif}[0,1]\\),\n\\(X \\sim \\mathrm{Unif}[-1,1]\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nSoit \\(\\phi : \\mathbb{R} \\to \\mathbb{R}_+\\) borélienne, on a, en effectuant le changement de variables \\(u=\\arcsin(x)\\) à la deuxième ligne ci-dessous\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(\\arcsin(X))]\n& =  \\int_0^1 \\phi(\\arcsin(x)) dx  \\\\\n& =  \\int_0^{\\pi/2} \\phi(u) \\cos(u) du.\n\\end{align*}\\]\n\nOn conclut que \\(Y=\\arcsin(X)\\) possède la densité \\(u \\to f_Y(u) = \\cos(u) \\mathbf{1}_{[0,\\pi/2]}(u)\\).\n\nSoit \\(\\phi : \\mathbb{R} \\to \\mathbb{R}_+\\) borélienne, on a, en effectuant le changement de variables \\(u=\\arcsin(x)\\) à la deuxième ligne ci-dessous\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(\\arcsin(X))]\n& =  \\frac{1}{2} \\int_{-1}^1 \\phi(\\arcsin(x)) dx  \\\\\n& =  \\frac{1}{2} \\int_{-\\pi/2}^{\\pi/2} \\phi(u) \\cos(u) du.    \n\\end{align*}\\]\nOn conclut que \\(Y=\\arcsin(X)\\) possède la densité \\(u \\to f_Y(u) = \\frac{\\cos(u)}{2} \\mathbf{1}_{[-\\pi/2,\\pi/2]}(u)\\).\n\n\n\n\nExercice 14  \n\nOn souhaite peindre un mur (infini!) en utilisant un arroseur automatique qui effectue des demi-révolutions successives. Pour simplifier le modèle, on représentera le mur par une droite verticale \\(\\Delta\\), et l’arroseur par une source ponctuelle \\(O\\) située à \\(1\\) mètre du mur, et émettant en tout instant \\(t\\) de façon parfaitement rectiligne dans la direction \\(\\overset{\\rightarrow}{u}(t)\\). On note \\(M\\) la projection orthogonale de \\(O\\) sur \\(\\Delta\\) et \\(\\theta(t)\\) l’angle entre \\(O \\overset{\\rightarrow}{u}(t)\\) et \\(\\overset{\\rightarrow}{OM}\\). L’intersection de \\(O\\overset{\\rightarrow}{u}\\) avec \\(\\Delta\\) est notée \\(H(\\theta)\\).\nOn suppose en outre que lors d’une demi-révolution, \\(\\theta(t)\\) parcourt exactement l’intervalle \\((-\\pi/2,\\pi/2)\\).\nOn fait l’hypothèse que la demi-révolution s’effectue à vitesse angulaire constante, et on se demande quelle sera la répartition de l’épaisseur de la couche de peinture le long de \\(\\Delta\\) après un nombre entier de demi-révolutions.\n\nJustifier qu’une particule de peinture choisie uniformément au hasard parmi toutes les particules est envoyée suivant un angle \\(\\theta \\sim \\mathrm{Unif}(-\\pi/2,\\pi/2)\\). Une telle particule se pose alors en \\(H(\\theta)\\), On note \\(h(\\theta)\\) l’ordonnée de \\(H(\\theta)\\) (c’est également la distance algébrique entre \\(O\\) et \\(H\\)).\nExprimer \\(h(\\theta)\\) en fonction de \\(\\theta\\). Quelle est la loi de \\(h(\\theta)\\)? Que pouvez-vous en déduire sur la distribution de l’épaisseur de la couche de peinture le long du mur?\nA posteriori, quelle critique peut-on formuler sur le modèle?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nLa vitesse angulaire étant constante, et le nombre de révolutions étant entier, aucune direction dans \\(]-\\pi/2,\\pi/2[\\) n’est privilégiée. Plus précisément, si \\(-\\pi/2 &lt; \\theta_0 &lt; \\theta_0 + \\theta_1 &lt; \\pi/2\\), le nombre de particules envoyées lors des \\(n\\) révolutions dans un secteur angulaire \\((\\theta_0, \\theta_0+\\theta_1)\\) ne dépend pas de \\(\\theta_0\\) et est proportionnel à \\(\\theta_1\\).\nAinsi, lorsqu’on choisit une des particules de peinture envoyées uniformément au hasard, la probabilité qu’elle ait été envoyée dans ce secteur angulaire est proportionnelle à \\(\\theta_1\\),, elle vaut donc \\(\\frac{2}{\\pi} \\theta_1\\) (en effet la probabilité est \\(1\\) pour \\(\\theta_0=-\\pi/2, \\theta_1=\\pi\\). Toujours pour \\(-\\pi/2&lt; \\theta_0&lt; \\theta_0 +\\theta_1&lt;\\pi/2\\), \\(\\frac{1}{\\pi} \\theta_1=\\int_{\\theta_0}^{\\theta_0+\\theta_1} \\frac{1}{\\pi} dx\\), et comme ceci est valable pour tous \\(-\\pi/2&lt; \\theta_0&lt; \\theta_0 +\\theta_1&lt;\\pi/2\\), cela caractérise la loi de l’angle, et on déduit que celui-ci suit bien une loi \\(\\mathrm{Unif}]-\\pi/2,\\pi/2[\\).\nOn a \\(h(\\theta) = \\tan(\\theta)\\), et donc si on pose \\(X = h(\\theta)\\) et si \\(\\phi : \\mathbb{R} \\to \\mathbb{R}_+\\) est borélienne on a (on effectue le changement de variables \\(x=\\tan(\\theta)\\) à la troisième ligne ci-dessous)\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(X)]\n& = \\mathbb{E}[\\phi(\\tan(\\theta))] \\\\\n& = \\int_{-\\pi/2}^{\\pi/2} \\frac{2}{\\pi} \\phi(\\tan(\\theta)) d\\theta \\\\\n& = \\int_{-\\infty}^{\\infty} \\frac{1}{\\pi} \\frac{\\phi(x)}{1+x^2} dx  \n\\end{align*}\\]\nde sorte que \\(X\\) a pour densité \\(f_X: x \\to \\frac{1}{\\pi(1+x^2)}\\).\nOn a \\(\\mathbb{E}[|X|]=+\\infty\\), autrement dit, la distance moyenne à l’axe des abcisses à laquelle une particule choisie uniformément atterit est … infinie.\n\n\n\n\nExercice 15  \n\nSoit \\(X \\sim \\mathrm{Cauchy}\\) (de paramètre \\(1\\)).\nQuelle est la loi de\n\n\\(Y:=\\frac{1}{X}\\)?\n\\(Z:= \\frac{1}{1+X^2}\\)?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nComme \\(\\mathbb{P}(X=0)=0\\), la variable \\(Y\\) est bien définie.\nSi \\(\\phi : \\mathbb{R} \\to \\mathbb{R}_+\\) est borélienne on a (on effectue le changement de variables \\(y=\\frac{1}{x}\\) à la toisième ligne ci-dessous)\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(Y)]\n& =  \\mathbb{E}[\\phi(\\frac{1}{X})] \\\\\n& =  \\int_{-\\infty}^{0} \\frac{\\phi(1/x)}{\\pi(1+x^2)} dx  + \\int_{0}^{\\infty} \\frac{\\phi(1/x)}{\\pi(1+x^2)} dx \\\\\n& = \\int_{-\\infty}^0 \\frac{\\phi(y)}{\\pi y^2(1+\\frac{1}{y^2})} dy + \\int_0^{\\infty} \\frac{\\phi(y)}{\\pi y^2(1+\\frac{1}{y^2})} dy \\\\  \n& =  \\int_{-\\infty}^{\\infty}  \\frac{\\phi(y)}{\\pi (1+y^2)} dy\n\\end{align*}\\]\nde sorte que \\(Y \\sim \\mathrm{Cauchy}(1)\\).\nSi \\(\\phi : \\mathbb{R} \\to \\mathbb{R}_+\\) est borélienne on a (on effectue le changement de variables \\(z=\\frac{1}{1+x^2}\\) à la toisième ligne ci-dessous)\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(Z)]\n& =  \\mathbb{E}[\\phi(\\frac{1}{1+X^2})] \\\\\n& =  \\int_{-\\infty}^{0} \\frac{\\phi(\\frac{1}{1+x^2})}{\\pi(1+x^2)} dx  + \\int_{0}^{\\infty} \\frac{\\phi(\\frac{1}{1+x^2})}{\\pi(1+x^2)} dx \\\\\n& =  2 \\int_{0}^1 \\frac{z \\phi(z)}{\\pi} \\frac{1}{2 z^2\\sqrt{\\frac{1}{z}-1}}  \\\\\n& =  \\int_{0}^{1} \\frac{\\phi(z)}{\\pi\\sqrt{z(1-z)}}\n\\end{align*}\\]\nde sorte que \\(Z \\sim \\mathrm{Beta}(1/2,1/2)\\).\n\n\n\n\nExercice 16  \n\nSoit \\(Z \\sim \\mathcal{N}(0,1)\\). Montrer que pour tout \\(x&gt;0\\),\n\\[\\left(x^{-1}-x^{-3}\\right) \\exp(-x^2/2) \\le \\sqrt{2\\pi} \\mathbb{P}(Z&gt;x) \\le x^{-1} \\exp(-x^2/2).\\]\nIndication : on pourra penser à utiliser le changement de variable \\(y=x+z\\) pour obtenir l’inégalité de droite, et on commencera par calculer la dérivée de \\(\\left(x^{-1}-x^{-3}\\right) \\exp(-x^2/2)\\) pour obtenir celle de gauche.\n\n\n\n\n\n\nNoteSolution\n\n\n\nOn a pour \\(x &gt;0\\), en effectuant le changement de variables \\(y=x+z\\) suggéré dans l’énoncé\n\\[\\begin{align*}\n\\sqrt{2\\pi} \\mathbb{P}(Z&gt;x) & =  \\int_{x}^{\\infty} \\exp(-y^2/2) dy\n\\\\ & =  \\int_0^{\\infty} \\exp(-x^2/2-xz-z^2/2) \\mathrm{d}z\n\\\\ & =  \\exp(-x^2/2) \\int_{\\mathbb{R}_+} \\exp(-zx - z^2/2) \\mathrm{d}z\n\\\\ & \\le  \\exp(-x^2/2) \\int_{\\mathbb{R}_+} \\exp(-zx) \\mathrm{d}z = x^{-1} \\exp(-x^2/2)\n\\end{align*}\\]\nPar ailleurs, si \\(g : x \\to (x^{-1}-x^{-3})(\\exp(-x^2/2)\\), on a pour \\(x&gt;0\\)\n\\[\\begin{align*}\ng'(x) & =  \\left(-\\frac{1}{x^2}-\\frac{3}{x^4} -x (x^{-1}-x^{-3}) \\right)\\exp(-x^2/2)   \\\\\n& = \\left( -1 -\\frac{3}{x^4} \\right) \\exp(-x^2/2)\n\\end{align*}\\]\nPar ailleurs, si \\(h : x \\to \\sqrt{2\\pi} \\mathbb{P}(Z&gt;x)\\), on a \\(h'(x) = - \\exp(-x^2/2)\\) de sorte que \\(g'(x) &lt;h'(x),\\) pour tout \\(x &gt;0\\).\nComme \\(g(x) \\to -\\infty\\) lorsque \\(x \\searrow 0\\), alors que \\(h(0) = \\sqrt{\\pi/2}\\), on déduit, comme souhaité, que\n\\[g(x)  \\le h(x), \\ \\forall x &gt;0 \\]\n\n\n\nExercice 17 (Calcul d’une loi conditionnelle discrète)  \n\nSoient \\(X_1,...,X_n\\) des variables de Poisson, indépendantes, de paramètres respectifs \\(\\lambda_1,...,\\lambda_n\\).\n\nDéterminer la loi de \\(Y:=\\sum_{k=1}^n X_k\\).\nPour \\(r \\in \\mathbb{N}\\), que vaut la loi conditionnelle de \\((X_1,...,X_n)\\) sachant \\(Y=r\\)?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nOn a pour tout \\(t \\in \\mathbb{R}\\), en utilisant l’indépendance des \\((X_k, 1 \\le k \\le n)\\) à la première ligne ci-dessous,\n\n\\[\\begin{align*}\n\\Phi_Y(t) & =  \\prod_{k=1}^n \\Phi_{X_k}(t) \\\\ & =  \\prod_{k=1}^n \\exp(\\lambda_k (\\exp(it)-1)) \\\\ & =  \n\\exp(\\Lambda (\\exp(it)-1)\n\\end{align*}\\]\noù \\(\\Lambda = \\sum_{k=1}^n \\lambda_k\\). La fonction caractéristique caractérisant la loi, on déduit que \\(Y \\sim \\mathrm{Poisson}(\\Lambda)\\). 1. Soient \\((\\ell_1,\\ell_2, \\dots,\\ell_n) \\in \\mathbb{N}^n\\). Si \\(\\sum_{k=1}^n \\ell_k \\ne r\\) on a bien sûr \\(\\{(X_1,...,X_n)= (\\ell_1,... \\ell_n)\\} \\cap \\{Y=r\\} = \\emptyset\\) donc \\[ \\mathbb{P}((X_1,X_2, \\dots ,X_n) = (\\ell_1, \\ell_2, \\dots, \\ell_n)  \\mid Y=r) = 0\\] Si \\((\\ell_1,\\ell_2, \\dots, \\ell_n) \\in \\mathbb{N}^n\\) sont tels que \\(\\sum_{k=1}^n \\ell_k = n\\), on a \\(\\{Y=r\\} \\supset \\{(X_1,X_2, \\dots,X_n)= (\\ell_1, \\ell_2, \\dots, \\ell_n)\\}\\) et donc en utilisant l’indépendance des $(X_k)_{1 k n} $ à la deuxième ligne ci-dessous\n\\[\\begin{align*}\n\\mathbb{P}((X_1, X_2, \\dots,X_n) = (\\ell_1,\\ell_2, \\dots, \\ell_n) \\mid Y =r) & =  \\frac{\\mathbb{P}(X_1=\\ell_1, X_2= \\ell_2, \\dots, X_n = \\ell_r)}{\\mathbb{P}(Y=r)} \\\\ & =  \n\\frac{\\prod_{k=1}^n \\frac{\\lambda_k^{\\ell_k}\\exp(-\\lambda_k)}{\\ell_k!}}{\\frac{\\exp(-\\Lambda) \\Lambda^r}{r!} }\n\\\\ & =  {r \\choose \\ell_1, \\ell_2, \\dots,\\ell_n} \\prod_{k=1}^n \\left(\\frac{\\lambda_k}{\\Lambda}\\right)^{\\ell_k},\n\\end{align*}\\]\navec \\({r \\choose \\ell_1, \\ell_2, \\dots, \\ell_n} =\\frac{r!}{\\ell_1! \\ell_2! \\dots \\ell_n!}\\) le coefficient multinômial de \\((\\ell_1,...,\\ell_n)\\) parmi \\(r\\).\nOn conclut que la loi conditionnelle de \\((X_1,...,X_n)\\) sachant \\(\\{Y=r\\}\\) est une loi mutinômiale de paramètres \\(\\left(r,\\frac{\\lambda_1}{\\Lambda}, \\frac{\\lambda_2}{\\Lambda}, \\dots, \\frac{\\lambda_n}{\\Lambda}\\right)\\)."
  },
  {
    "objectID": "corriges/td1.html#exemples-divers",
    "href": "corriges/td1.html#exemples-divers",
    "title": "Variables aléatoires réelles",
    "section": "Exemples divers",
    "text": "Exemples divers\n\nExercice 18  \n\nOn suppose que le nombre \\(X\\) d’oeufs pondus par un insecte suit une loi de Poisson de paramètre \\(\\lambda&gt;0\\) et que la probabilité qu’un oeuf meurt sans éclore est, indépendamment des autres oeufs, égale à \\(1-p\\), où \\(p \\in ]0,1[\\).\n\nDémontrer que le nombre \\(Y\\) d’oeufs qui arrivent à éclosion suit une loi de Poisson de paramètre \\(\\lambda p\\).\nQuelle est la loi jointe de \\((Y,Z)\\), où \\(Z= X-Y\\) est le nombre d’oeufs morts avant éclosion?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\nQuitte à introduire des variables \\((\\xi_i, i \\ge 1)\\) indépendantes de \\(X\\), et i.i.d suivant la loi de Bernoulli de paramètre \\(p\\) (\\(\\xi_i=1\\) si le \\(i\\)-ième oeuf arrive à éclosion),\n\\[Y = \\sum_{k=1}^X \\xi_i, \\quad Z = \\sum_{k=1}^X (1-\\xi_i),\\]\noù par convention \\(\\sum_{k=1}^0 \\dots = 0\\).\nOn a donc pour \\((s,t) \\in \\mathbb{R}^2\\), en utilisant l’indépendance de \\(X\\) et des \\((\\xi_i, i \\ge 1)\\) à la quatrième ligne ci-dessous, puis le fait que les \\((\\xi_i, i \\ge 1)\\) sont i.i.d suivant une loi Ber\\((p)\\) à la suivante :\n\\[\\begin{align*}\n\\Phi_{(Y,Z)}(s,t) & =  \\mathbb{E}[\\exp(isY + itZ)] \\\\\n& =  \\mathbb{E}\\left[\\exp\\left(is \\sum_{k=1}^X \\xi_k + it \\sum_{k=1}^X (1-\\xi_k)\\right)\\right]\n\\\\ & =  \\sum_{j \\in \\mathbb{N}} \\mathbb{E}\\left[\\mathbf{1}_{\\{X= j\\}} \\exp\\left(is \\sum_{k=1}^j \\xi_k + it \\sum_{k=1}^j (1-\\xi_k)\\right)\\right]\n\\\\ & =  \\sum_{j \\in \\mathbb{N}} \\frac{\\lambda^j \\exp(-\\lambda)}{j!} \\mathbb{E}\\left[\\prod_{k=1}^j  \\exp(is \\xi_k + it(1-\\xi_k))\\right]\n\\\\ & =  \\sum_{j \\in \\mathbb{N}} \\frac{\\lambda^j \\exp(-\\lambda)}{j!}  \\bigg(p\\exp(is) + (1-p)\\exp(it)\\bigg)^j\n\\\\ & =  \\exp(-\\lambda + \\lambda p \\exp(is) + \\lambda(1-p)\\exp(it))\n\\\\ & =  \\exp(\\lambda p (\\exp(is)-1)) \\exp(\\lambda(1-p) (\\exp(it)-1))\n\\end{align*}\\]\nOn déduit que \\((Y,Z)\\) est un couple de variables de Poisson indépendantes, de paramètres respectifs \\(\\lambda p, \\lambda(1-p)\\).\n\n\n\nExercice 19  \n\n(Somme d’exponentielles et \\(\\chi^2\\))\nSoit \\(\\lambda&gt;0\\) et \\((Y_i)_{1 \\le i \\le n}\\) des variables i.i.d, \\(\\sim \\exp(\\lambda)\\).\n\nCalculer \\(\\mathbb{E}[\\exp(-tY_1)]\\) pour \\(t \\ge 0\\).\n\nCalculer \\(\\mathbb{E}\\left[\\exp(-t\\sum_{i=1}^n) Y_i \\right]\\) pour \\(t \\ge 0\\).\nMontrer que la densité de la variable \\(X= \\sum_{i=1}^n Y_i\\) est proportionnelle à \\(x^{n-1} \\exp(-\\lambda x) \\mathbf{1}_{x \\ge 0}\\). En déduire la valeur de cette densité.\nSoient \\(X_n, n \\ge 1\\) des variables i.i.d, \\(\\sim \\mathcal{N}(0,1)\\). On pose \\(\\chi^2(n):=\\sum_{i=1}^n X_i^2\\), \\(Z:=X_1X_2 +X_3X_4\\). \\ Calculer \\(\\Phi_{\\chi^2(n)}, \\Phi_Z\\). Pouvez-vous deviner la distribution de \\(Z\\) (on pourra utiliser un résultat d’un exercice précédent)?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\\(\\mathbb{E}[\\exp(-tY_1)] = \\frac{\\lambda}{\\lambda+t}, t \\ge 0\\)\n\\(\\mathbb{E}\\left[\\exp(-t\\sum_{i=1}^n) Y_i \\right] = \\left(\\frac{\\lambda}{\\lambda+t}\\right)^n, \\ t \\ge 0\\).\nOn montre cette assertion par récurrence sur \\(n \\in \\mathbb{N}^*\\). L’assertion est évidente pour \\(n=1\\) (avec facteur de proportionalité \\(c_1:=\\lambda\\)). Supposons la vraie au rang \\(n\\) et posons \\(Z_{n+1} = \\sum_{i=1}^{n+1} Y_i\\). Comme \\(Y_{n+1}\\) est indépendante de \\(Z_n\\) le vecteur \\((Z_n, Y_{n+1})\\) possède la densité jointe sur \\(\\mathbb{R}^2\\) : \\[ f_{(Z_n, Y_{n+1})}(z,y) = f_{Z_n}(z) f_{Y_{n+1}}(y) = c_n  \\lambda z^{n-1} \\exp(-\\lambda z - \\lambda y) \\mathbf{1}_{\\{z \\ge 0, y \\ge 0\\}}, (z,y) \\in \\mathbb{R}^2\\] Soit \\(\\phi : \\mathbb{R} \\to \\mathbb{R}\\) borélienne positive\n\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(Z_{n+1})] & =  \\mathbb{E}[\\phi(Z_n + Y_{n+1})]\n\\\\ & =  \\int_{\\mathbb{R}_+^2} \\phi(z+y) f_{Z_n}(z) f_{Y_{n+1}}(y) \\mathrm{d}z dy\n\\\\ & =  \\int_{\\mathbb{R}_+^2} \\phi(z+y)     c_n  \\lambda z^{n-1} \\exp(-\\lambda z - \\lambda y)  \\mathrm{d}z dy\n\\\\ & =  \\int_{u \\in \\mathbb{R}_+, u \\ge v}  \\phi(u) c_n \\lambda v^{n-1} \\exp(-\\lambda u) du dv\n\\end{align*}\\]\noù à la dernière ligne on a utilisé le changement de variable \\((u,v) = (z+y,z)\\) de \\(\\mathbb{R}_+^2\\) dans \\(\\{(u,v) \\in \\mathbb{R}_+^2 : v \\le u\\}\\), de jacobien \\(1\\). On a donc\n\\[\\begin{align*}\n  \\mathbb{E}[\\phi(Z_{n+1})] & =   \\int_{u \\in \\mathbb{R}_+} \\lambda c_n \\phi(u)  \\exp(-\\lambda u) \\int_{v =0}^u  v^{n-1} dv  du \\\\\n& =  \\int_{\\mathbb{R}_+} \\phi(u) \\lambda \\frac{c_n}{n} u^n \\exp(-\\lambda u) du\n\\end{align*}\\]\net on obtient la conclusion souhaitée, avec \\(c_{n+1} = \\frac{\\lambda c_n}{n}\\).\nOn déduit par une récurrence immédiate que \\(c_n = \\frac{\\lambda^n}{(n-1)!}, n \\in \\mathbb{N}^*\\).\n\nOn a vu (exercice 12) que \\(X_i^2 \\sim \\mathrm{Gamma}(1/2,1/2)\\), et par un raisonnement similaire à celui de la question précédente, on obtient que \\(\\chi^2(n) \\sim \\mathrm{Gamma}(n/2,1/2)\\). On peut aussi raisonner directement avec les fonctions caractéristiques, pour obtenir que \\[ \\Phi_{\\chi^2(n)}(t) = \\Phi_{X_1^2}(t)^n = \\left(\\frac{1/2}{1/2-it}\\right)^{n/2}.\\] Par ailleurs pour \\(t \\in \\mathbb{R}\\),\n\n\\[\\begin{align*}\n\\Phi_{X_1X_2}(t) & =  \\mathbb{E}[\\exp(i t X_1 X_2)] = \\int_{\\mathbb{R}^2} \\frac{1}{2\\pi} \\exp(it x_1 x_2 - x_1^2/2 - x_2^2/2) dx_1 dx_2 \\\\\n& =  \\int_{\\mathbb{R}} dx_1 \\frac{1}{\\sqrt{2\\pi}} \\exp(-x_1^2 /2) \\int_{\\mathbb{R}} dx_2 \\frac{1}{\\sqrt{2\\pi}} \\exp(it x_1 x_2 - x_2^2/2)\n\\\\ & =  \\int_{\\mathbb{R}} \\frac{1}{\\sqrt{2\\pi}} \\exp(-x_1^2/2) \\Phi_{X_2}(tx_1) dx_1\n\\\\ & =  \\int_{\\mathbb{R}}  \\frac{1}{\\sqrt{2\\pi}} \\exp(-x_1^2/2) \\exp(-t^2 x_1^2 /2) dx_1\n\\\\ & =  \\int_{\\mathbb{R}} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(- \\frac{x_1^2 (1+t^2)}{2} \\right) dx_1\n\\\\ & =  \\frac{1}{\\sqrt{1+t^2}}\n\\end{align*}\\]\net donc les \\((X_i, i \\ge 1)\\) étant i.i.d, pour \\(t \\in \\mathbb{R}\\),\n\\[\\Phi_Z(t) = \\Phi_{X_1X_2}(t) \\Phi_{X_3X_4}(t) = \\frac{1}{1+t^2}.\\]\nIl s’agit de la fonction caractéristique d’une variable “exponentielle symétrique”, de densité \\(x \\to \\frac{1}{2}\\exp(-|x|)\\) (c’est d’ailleurs une manière d’effectuer le calcul de la fonction caractéristique d’une Cauchy\\((1)\\)). On peut le vérifier directement. Soit \\(\\xi \\sim \\mathrm{Ber}(1/2)\\), et \\((X,Y)\\) i.i.d suivant une loi exp\\((1)\\). La variable \\(T=\\xi X -(1-\\xi)Y\\) a une loi exponentielle symétrique. De plus pour \\(t \\in \\mathbb{R}\\),\n\\[\\begin{align*}\n\\Phi_T(t) & =  \\mathbb{E}[\\exp(it \\xi X -it(1-\\xi)Y)]  \\\\\n& =  \\frac{1}{2} \\mathbb{E}[\\exp(itX)] + \\frac{1}{2}\\mathbb{E}[\\exp(-itY)] \\\\\n& =  \\frac{1}{2} \\frac{1}{1-it} + \\frac{1}{2} \\frac{1}{1+it}\n\\\\ & =  \\frac{\\frac{1}{2}(1+it) + \\frac{1}{2}(1-it)}{1+t^2} = \\frac{1}{1+t^2}\n\\end{align*}\\]\ncomme souhaité.\n\n\n\nExercice 20  \n\nSoit \\(Z = (X, Y )\\), une variable aléatoire à valeurs dans \\(\\mathbb{R}^2\\). On suppose que \\(Z\\) admet une densité \\(f\\) définie par\n\\[f(x, y) = \\frac{4}{\\pi \\sigma^2} \\exp\\left(-\\frac{x^2+y^2}{\\sigma^2}\\right) \\mathbf{1}_{\\{x \\ge |y|\\}},\\]\noù \\(\\sigma&gt;0\\).\n\nVérifier que \\(f\\) est bien une densité de probabilité.\nCalculer les lois de \\(X\\) et de \\(Y\\) . Les variables aléatoires \\(X\\) et \\(Y\\) sont-elles indépendantes ?\nCalculer la loi de \\((X - Y, X + Y )\\) et montrer que \\(X - Y\\) et \\(X + Y\\) sont indépendantes.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\\(f : \\mathbb{R}^2 \\to \\mathbb{R}_+\\) est continue donc borélienne Par symétrie des rôles de \\(x\\) et \\(y\\),\n\n\\[\\int_{\\mathbb{R}^2} f(x,y) dx dy = \\int_{\\mathbb{R}^2}\\frac{4}{\\pi \\sigma^2} \\exp\\left(-\\frac{x^2+y^2}{\\sigma^2}\\right) \\mathbf{1}_{\\{y \\ge |x|\\}}.\\]\nPar parité de \\(x \\to \\exp(-\\frac{x^2}{2})\\),\n\\[\\int_{\\mathbb{R}^2} f(x,y) dx dy = \\int_{\\mathbb{R}^2}\\frac{4}{\\pi \\sigma^2} \\exp\\left(-\\frac{x^2+y^2}{\\sigma^2}\\right) \\mathbf{1}_{\\{x \\le -|y|\\}}.\\]\nA nouveau par symétrie des rôles de \\(x\\) et \\(y\\)\n\\[\\int_{\\mathbb{R}^2} f(x,y) dx dy = \\int_{\\mathbb{R}^2}\\frac{4}{\\pi \\sigma^2} \\exp\\left(-\\frac{x^2+y^2}{\\sigma^2}\\right) \\mathbf{1}_{\\{y  \\le -|x|\\}}.\\]\nEnfin, la réunion des \\(4\\) ensembles \\(\\{(x,y) \\in \\mathbb{R}^2: x \\ge |y|\\}, \\{(x,y) \\in \\mathbb{R}^2 : y \\ge |x|\\},  \\{(x,y) \\in \\mathbb{R}^2 : x \\le -|y|\\}, \\{(x,y) \\in \\mathbb{R}^2 : y \\le -|x|\\}\\) est \\(\\mathbb{R}^2\\) tout entier. L’intersection des \\(2\\) premiers est \\(\\{(x,y) : x = y \\ge 0\\}\\), de mesure de Lebesgue nulle, et des considérations similaires permettent d’assurer que c’est également le cas pour l’intersection de n’importe quelle paire parmi ces \\(4\\) ensembles. On conclut que\n\\[\\begin{align*}\n\\int_{\\mathbb{R}^2} f(x,y) dx dy  \n& =  \\frac{1}{4} \\int_{\\mathbb{R}^2}   \\frac{4}{\\pi \\sigma^2} \\exp\\left(-\\frac{x^2+y^2}{\\sigma^2}\\right) \\\\\n& =  \\int_{0}^{\\infty} \\int_0^{2\\pi} \\frac{1}{\\pi \\sigma^2} \\exp\\left(-\\frac{r^2}{\\sigma^2}\\right) r dr d\\theta\\\\\n& = \\left[ \\exp\\left(-\\frac{r^2}{\\sigma^2}\\right) \\right]_0^{\\infty} = 1,  \n\\end{align*}\\]\ncomme souhaité. 1. Si \\(\\phi : \\mathbb{R} \\to \\mathbb{R}_+\\) est borélienne\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(X)] & =  \\int_{\\mathbb{R}^2} \\phi(x) f(x,y) dx dy \\\\\n& =  \\int_{\\mathbb{R}_+} \\phi(x) \\frac{4}{\\pi \\sigma^2} \\exp\\left(-\\frac{x^2}{\\sigma^2}\\right) \\int_{-x}^x \\exp\\left(-\\frac{y^2}{\\sigma^2}\\right) dy dx\n\\end{align*}\\]\nOn déduit que \\(X\\) possède la densité \\(f_X\\) telle que \\[f_X(x) =  \\frac{4}{\\pi \\sigma^2} \\exp\\left(-\\frac{x^2}{\\sigma^2}\\right) \\int_{-x}^x \\exp\\left(-\\frac{y^2}{\\sigma^2}\\right) dy \\mathbf{1}_{\\{x \\ge 0\\}}\\]\nRemarque : Avec \\(\\mathrm{erf}(x) = \\int_{0}^x \\frac{2}{\\sqrt{\\pi}} \\exp\\left(-u^2\\right) du\\), on peut réexprimer\n\\[\\frac{1}{\\sigma \\sqrt{\\pi}} \\int_{-x}^x \\exp\\left(-\\frac{y^2}{\\sigma^2}\\right) dy =  \\mathrm{erf}\\left(\\frac{x}{\\sigma}\\right),\\]\nde sorte que\n\\[f_X(x) =  \\frac{4}{\\sigma \\sqrt{\\pi} } \\exp\\left(-\\frac{x^2}{\\sigma^2}\\right) \\mathrm{erf}\\left(\\frac{x}{\\sigma}\\right) \\mathbf{1}_{\\{x \\ge 0\\}}\\]\nToujours pour \\(\\phi: \\mathbb{R} \\to \\mathbb{R}_+\\) borélienne, on a\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(Y)] & =  \\int_{\\mathbb{R}^2} \\phi(y) f(x,y) dx dy \\\\\n& =  \\int_{\\mathbb{R}} \\phi(y) \\frac{4}{\\pi \\sigma^2} \\exp\\left(-\\frac{y^2}{\\sigma^2}\\right) \\int_{|y|}^{\\infty} \\exp\\left(-\\frac{x^2}{\\sigma^2}\\right) dx dy\n\\end{align*}\\]\nOn déduit que \\(Y\\) possède la densité \\(f_Y\\) telle que \\[f_Y(y) =  \\frac{4}{\\pi \\sigma^2} \\exp\\left(-\\frac{y^2}{\\sigma^2}\\right) \\int_{|y|}^{\\infty} \\exp\\left(-\\frac{x^2}{\\sigma^2}\\right) dx \\]\nRemarque : On a \\[\\int_{|y|}^{\\infty} \\frac{2}{\\sqrt{\\pi}} \\exp\\left(-u^2\\right) du = \\frac{1}{2}(1 - \\mathrm{erf}(|y|))= \\frac{1}{2}\\mathrm{erfc}(|y|)\\] et donc \\[\\int_{|y|}^{\\infty} \\frac{1}{\\sigma \\sqrt{\\pi}} \\exp \\left( -\\frac{x^2}{\\sigma^2} \\right)  dx  = \\frac{1}{2}\\mathrm{erfc}\\left(\\frac{|y|}{\\sigma}\\right),\\]\nde sorte que \\[ f_Y(y) =  \\frac{2}{ \\sigma \\sqrt{\\pi}} \\exp\\left(-\\frac{y^2}{\\sigma^2}\\right)  \\mathrm{erfc}\\left(\\frac{|y|}{\\sigma}\\right).\\] Enfin puisque par exemple \\(\\mathbb{P}(X \\ge Y \\ge 1) =\\mathbb{P}(Y \\ge 1)\\) on a \\(\\mathbb{P}(X \\ge 1, Y \\ge 1) \\ne \\mathbb{P}(X \\ge 1) \\mathbb{P}(Y \\ge 1)\\) et les variables \\(X\\) et \\(Y\\) ne sont pas indépendantes.\n\nSoit \\(\\psi : \\mathbb{R}^2 \\to \\mathbb{R}_+\\) borélienne, on a \\[\\begin{align*}\n\\mathbb{E}[\\psi(X-Y,X+Y)] & =  \\int_{\\mathbb{R}^2} \\psi(x-y,x+y) f(x,y) dx dy \\\\\n& =  \\int_{\\mathbb{R}^2} \\psi(x-y,y+y)  \\frac{4}{\\pi \\sigma^2} \\exp\\left(-\\frac{x^2+y^2}{\\sigma^2}\\right)  \\mathbf{1}_{\\{x \\ge |y|\\}}\n\\end{align*}\\]\n\nNotons que \\(G: \\begin{cases} & \\{(x,y) \\in \\mathbb{R}^2 : x \\ge |y|\\} \\to \\{(u,v) \\in \\mathbb{R}_+^2\\} \\\\ & (x,y) \\to (u,v) =(x+y,x-y) \\end{cases}\\) est un \\(\\mathcal{C}^1\\)-difféomorphisme (d’inverse \\(G^{-1} : (u,v) \\to (x,y)\\) avec \\(x = \\frac{u+v}{2}, y = \\frac{u-v}{2}\\)), de jacobien \\(2\\). Par la formule de changement de variables, et en remarquant que \\(u^2 + v^2 = 2(x^2+y^2)\\), on obtient\n\\[\\begin{align*}\n\\mathbb{E}[\\psi(X-Y, X+Y)]\n& = \\int_{\\mathbb{R}_+^2}  \\psi(u,v) \\frac{2}{\\pi \\sigma^2} \\exp\\left(-\\frac{u^2 + v^2}{2\\sigma^2}\\right)  du dv \\\\\n& =  \\int_{\\mathbb{R}^2} \\psi(u,v) f_U(u) f_V(v) du dv\n\\end{align*}\\]\navec \\[f_U(u) = f_V(u) = \\frac{2}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{u^2}{2\\sigma^2} \\right) \\mathbf{1}_{\\{u \\ge 0\\}},\\] qui est la densité de \\(|Z|\\) où \\(Z \\sim \\mathcal{N}(0,1)\\).\nOn conclut que \\((X+Y, X-Y)\\) sont i.i.d. suivant la loi de \\(|Z|\\).\n\n\n\nExercice 21  \n\n\nSoit \\(X_1,...,X_n\\) des variables i.i.d, \\(\\sim \\mathcal{N}(0,1)\\), et \\(Z=\\sum_{i=1}^n \\alpha_i X_i\\) (où les \\(\\alpha_i, i=1,...,n\\) sont de rééls fixés). Quelle est la loi de \\(Z\\)?\nSoit \\((X_1,...,X_n) \\sim \\mathcal{N}(0,A)\\). Quelle est la loi de \\(Z=\\sum_{i=1}^n \\alpha_i X_i\\)?\nSoit \\((X_1,...,X_n) \\sim \\mathcal{N}(0,A)\\). Quelle est la loi de \\((Z_1,Z_2)\\), où, pour des rééls \\(\\alpha_{i,1}, \\alpha_{i,2}, i=1,...,n\\) fixés, \\[  Z_1=\\sum_{i=1}^n \\alpha_{i,1} X_i,  Z_2=\\sum_{i=1}^n \\alpha_{i,2} X_i.\\]\nGénéraliser la question précédente en exprimant la loi de \\[ (Z_1,...,Z_n) =  (X_1,...,X_n) P,\\] où \\(P\\) est une matrice \\(n \\times n\\).\nSoient \\(X,Y\\) deux variables indépendantes, \\(\\sim \\mathrm{Unif}[0,1]\\). Quelle est la loi de \\(S=X+Y\\)?\nSoient \\(X, Y\\) des variables indépendantes de loi respectives \\(\\Gamma(a,c), \\Gamma(b,c)\\), où \\(a,b,c&gt;0\\). On pose \\(S=X+Y,\nT= \\frac{X}{X+Y}\\) Quelle est la loi du couple \\((S,T)\\)?\nSoient \\(X,Y\\) deux variables indépendantes, \\(\\sim \\mathrm{Unif}[0,1]\\). On pose \\(U = \\sqrt{-2\\log(X)} \\cdot \\cos(2\\pi Y), V = \\sqrt{-2\\log(X)} \\cdot \\sin(2\\pi Y)\\). Quelle est la loi du couple \\((X,Y)\\)?\nSoient \\(X,Y\\) deux variables indépendantes, \\(\\sim \\mathcal{N}(0,1)\\). On pose \\(T = \\frac{Y}{X}\\). Quelle est la loi de \\(T\\)?\nSoient \\((X,Y)\\) un couple de variables indépendantes, \\(\\sim \\mathcal{N}(0,1)\\). On pose \\(U=X, V= X^2 + Y^2\\). Quelle est la loi de \\((U,V)\\)?\nSoit \\(X\\) de densité \\(\\exp(-x) \\mathbf{1}_{\\mathbb{R}_+}(x)\\). On pose \\(U = [X]\\) et \\(V= X-[X]\\), la partie entière, resp. la partie décimale de \\(X\\). Quelle est la loi de \\((U,V)\\)?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nEn utilisant d’abord l’indépendance des \\((X_k, 1 \\le k \\le n)\\) à la deuxième ligne ci-dessous, puis le fait que \\(\\Phi_{X_k}(u) =\\exp(-u^2/2)\\) à la suivante, on obtient que pour \\(t \\in \\mathbb{R}\\),\n\n\\[\\begin{align*}\n\\Phi_{Z}(t) & =  \\mathbb{E}\\left[\\exp\\left(it \\sum_{k=1}^n \\alpha_k X_k \\right) \\right] \\\\\n& =  \\prod_{k=1}^n \\Phi_{X_k}(t \\alpha_k) \\\\\n& =  \\exp\\left( - \\frac{t^2}{2} \\sum_{k=1}^n \\alpha_k^2 \\right)\n\\end{align*}\\]\net on déduit que \\(Z \\sim \\mathcal{N}\\left(0, \\sum_{k=1}^n \\alpha_k^2\\right)\\). 1. Lorsque \\(X\\) est un vecteur gaussien, toute combinaison linéaire des coordonnées de \\(X\\) suit une loi gaussienne. Comme les coordonnées de \\(X\\) sont ici centrées, il en va de même pour \\(Z\\). Par ailleurs\n\\[\\begin{align*}\n\\mathrm{Var}(Z)\n& = \\sum_{k=1}^n \\sum_{\\ell=1}^n \\alpha_k \\alpha_{\\ell} \\mathrm{Cov}(X_k, X_{\\ell}) \\\\\n& = \\alpha^T A \\alpha.\n\\end{align*}\\]\nFinalement \\(Z \\sim \\mathcal{N}(0,\\alpha^T A \\alpha)\\), autrement dit \\[\\Phi_Z(t) = \\exp\\left(-t^2\\frac{\\alpha^T A \\alpha}{2} \\right).\\]\n1. Soit \\((u,v) \\in \\mathbb{R}^2\\), on a \\[uZ_1+vZ_2 = \\sum_{i=1}^n (\\alpha_{i,1} + v \\alpha_{i,2}) X_i,\\] et d’après la question précédente, ceci est distribué suivant une loi \\(\\mathcal{N}(0, (u \\alpha_1 + v \\alpha_2)^T A (u\\alpha_1+v \\alpha_2))\\), et donc\n\\[\\Phi_{uZ_1+vZ_2}(1) = \\mathbb{E}[\\exp (iu Z_1+ivZ_2)] = \\Phi_{(Z_1,Z_2)}(u,v) = \\exp\\left( - \\frac{(u\\alpha_1+ v \\alpha_2)^T A (u \\alpha_1+v\\alpha_2)}{2} \\right).\\] Reste à observer que \\(u\\alpha_1+v\\alpha_2\\) est le produit d’une matrice, disons \\(P\\) à \\(n\\) lignes et deux colonnes, la première ayant les coordonnées de \\(\\alpha_1\\) la deuxième celle de \\(\\alpha_2\\), par le vecteur \\(\\begin{pmatrix} u \\\\ v \\end{pmatrix}\\). On conclut que \\(Z \\sim \\mathcal{N}(0, P^T A P)\\)\n\nLa matrice \\(P\\) joue le même rôle que la matrice de la question précédent (à ceci près qu’elle possède désormais \\(n\\) lignes et \\(n\\) colonnes). Par le même raisonnement qu’à la question précédente, on trouve que \\(Z \\sim \\mathcal{N}(0,P^T A P)\\).\n\nPuisque \\(X\\) et \\(Y\\) sont indépendantes et à densité, \\((X,Y)\\) possède la densité jointe \\(f_{(X,Y)}\\) telle que \\(f_{(X,Y)}(x,y) = f_X(x) f_Y(y)\\). On a donc pour \\(\\phi: \\mathbb{R}^2 \\to \\mathbb{R}_+\\) borélienne,\n\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(X+Y)] & =  \\int_{[0,1]^2} \\phi(x+y) \\mathrm{d}x \\mathrm{d}y\n\\end{align*}\\]\nOn fait le changement de variables \\(\\begin{cases} & [0,1]^2 \\to \\{(s,t) \\in [0,2] \\times [0,1] :  t+1 \\ge s \\ge t \\} \\\\ & (x,y) \\to ( s=x+y, t=y) \\end{cases}\\), de jacobien \\(1\\), et on trouve\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(X+Y)]\n& =  \\int_{0}^2 ds \\phi(s) \\int_{\\max(s-1,0)}^{\\min(s,1)} \\mathrm{d}t\\\\\n& =   \\int_0^2 \\phi(s) \\min(s, 1-s) \\mathrm{d}s\n\\end{align*}\\]\net on déduit que \\(S\\) possède la densité \\(f_S\\), où \\[f_S(s) = \\begin{cases} & s \\mbox{ si } 0 \\le s \\le 1 \\\\ & (1-s) \\mbox{ si } 1 \\le s \\le 2 \\\\ & 0 \\mbox{ sinon.} \\end{cases}\\]\n\nD’après l’énoncé, \\((X,Y)\\) possède la densité\n\\[f_{(X,Y)}(x,y) = \\frac{c^{a+b}}{\\Gamma(a)\\Gamma(b)} x^{a-1} y^{b-1} \\exp(-c(x+y)) \\mathbf{1}_{\\{x &gt; 0, y &gt; 0\\}}.\\] Pour \\(\\phi : \\mathbb{R}^2 \\to \\mathbb{R}_+\\) borélienne on a donc \\[\\mathbb{E}[\\phi(S,T)] = \\int_{(\\mathbb{R}_+^*)^2} \\phi\\left(x+y, \\frac{x}{x+y}\\right)   \\frac{c^{a+b}}{\\Gamma(a)\\Gamma(b)} x^{a-1} y^{b-1} \\exp(-c(x+y)) dx dy \\] On effectue alors le changement de variables via le \\(\\mathcal{C}^1\\)-difféomorphisme :\n\\[G : \\begin{cases} & (\\mathbb{R}_+^*)^2 \\to \\mathbb{R}_+^* \\times (0,1) \\\\ & (x,y) \\to \\left(x+y, \\frac{x}{x+y}\\right) \\end{cases}\\] d’inverse \\[G^{-1} : \\begin{cases} &  \\mathbb{R}_+^* \\times (0,1)  \\to (\\mathbb{R}_+^*)^2  \\\\ & (s,t) \\to (st, s(1-t) \\end{cases}\\] dont le jacobien est \\(|J^{-1}| = s\\), pour obtenir \\[\\mathbb{E}[\\phi(S,T)]  =  \\int_{\\mathbb{R}_+^*} ds \\int_0^1 dt  \\phi(s,t)   \\frac{c^{a+b}}{\\Gamma(a)\\Gamma(b)} s^{a+b-1}  \\exp(-c s) t^{a-1}(1-t)^{b-1} \\] et on conclut que \\(S \\sim \\mathrm{Gamma}(a+b,c)\\) est indépendant de \\(T \\sim \\mathrm{Beta}(a,b)\\).\nL’application \\[\\Psi : \\begin{cases}  & [0,1]^2 \\to \\mathbb{R}^2  \\\\ & (x,y) \\to (u,v) = \\left(\\sqrt{-2\\ln(x)} \\cos(2\\pi v),  \\sqrt{-2\\ln(x)} \\sin(2\\pi y)\\right) \\end{cases}\\] est un \\(\\mathcal{C}^1\\)-difféomorphisme, composée de \\((x,y) \\to (r,\\theta) = (\\sqrt{-2\\ln(x)},2\\pi y)\\) et \\((r, \\theta) \\to (u,v) = (r \\cos(\\theta), r \\sin(\\theta))\\). Le Jacobien de \\(\\Psi\\) est \\(2\\pi \\exp((u^2+v^2)/2)\\) et on déduit que pour \\(\\phi\\) continue bornée de \\(\\mathbb{R}^2\\) dans \\(\\mathbb{R}\\), \\[\\begin{align*}\n\\mathbb{E}[\\phi(U,V)] & =  \\int_{0}^1 \\int_0^1 dx dy \\phi \\left(\\sqrt{-2\\ln(x)} \\cos(2\\pi y), \\sqrt{-2\\ln(x)} \\sin(2\\pi y)\\right) \\\\ & =  \\int_{\\mathbb{R}^2} du dv \\phi(u,v) \\frac{1}{2\\pi} \\exp(-u^2/2) \\exp(-v^2/2),\n\\end{align*}\\]\n\net finalement que \\((U,V) \\sim \\mathcal{N}(0,I_2)\\).\n\nNotons d’abord que \\(\\mathbb{P}(X=0)=0\\) de sorte que \\(Z\\) est définie p.s. Par ailleurs \\((X,Y) \\sim \\mathcal{N}(0,Id_2)\\) et a la densité correspondante.\n\nOn a, pour \\(\\phi: \\mathbb{R}^2 \\to \\mathbb{R}\\) bornée mesurable, gr^ace au changement de variables de \\(\\mathbb{R}^* \\times \\mathbb{R}^*\\) dans lui-même, qui à \\((x,y)\\) associe \\((x,z)=(x,\\frac{y}{x})\\) (et dont l’inverse du jacobien vaut \\(|x|\\)) :\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(X,Z)]\n& = \\frac{1}{2\\pi} \\int_{(\\mathbb{R}^*)^2} \\phi(x, \\frac{y}{x}) \\exp(-x^2/2-y^2/2) dx dy \\\\\n& = \\frac{1}{2\\pi} \\int_{(\\mathbb{R}^*)^2} \\phi(x, z) |x| \\exp\\left(-\\frac{x^2}{2} (1+z^2 \\right) dx \\mathrm{d}z   \n\\end{align*}\\]\net donc \\((X,Z)\\) a densité\n\\[f_{(X,Z)}(x,z) = \\frac{1}{2\\pi}  |x| \\exp\\left(-\\frac{x^2}{2} (1+z^2)\\right).\\]\nOn en déduit que\n\\[\\begin{align*}\nf_Z(z) & =  2 \\int_{\\mathbb{R}_+} \\frac{1}{2\\pi}  x z^2 \\exp\\left(-\\frac{x^2}{2} (1+z^2)\\right) dx = \\frac{1}{\\pi} \\frac{1}{1+z^2},\n\\end{align*}\\]\nde sorte que \\(Z \\sim \\mathrm{Cauchy}(1)\\).\n\nQuitte à considérer les \\(\\mathcal{C}^1\\)-difféomorphisme\n\\[G : \\begin{cases} & \\mathbb{R} \\times \\mathbb{R}_-^* \\to \\{(u,v) \\in \\mathbb{R} \\times \\mathbb{R}_+^* : v \\ge u^2\\} \\\\ (x,y) \\to (x, x^2+y^2) \\end{cases}, \\ H: \\begin{cases} & \\mathbb{R} \\times \\mathbb{R}_+ \\to \\{(u,v) \\in \\mathbb{R} \\times \\mathbb{R}_+^* : v \\ge u^2\\} \\\\ (x,y) \\to (x, x^2+y^2) \\end{cases},\\]\ndont les jacobiens sont tous deux égaux à \\(2|y| = 2 \\sqrt{u-v^2}\\), on obtient pour \\(\\phi : \\mathbb{R}^2 \\to \\mathbb{R}_+\\) borélienne (les deux intégrales fournissent des contributions identiques) :\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(U,V)]  \n&  =  2 \\int_{\\mathbb{R} \\times \\mathbb{R}_+} \\phi(u,v) \\frac{\\exp\\left(-\\frac{v}{2}\\right)}{4 \\pi \\sqrt{u-v^2}} \\mathbf{I}_{\\{v \\ge u^2\\}}du dv\n\\end{align*}\\]\net on conclut que \\((U,V)\\) possède la densité \\(f_{(U,V)}\\) telle que pour tout \\((u,v) \\in \\mathbb{R}^2\\),\n\\[f_{(U,V)}(u,v) = \\frac{\\exp\\left(-\\frac{v}{2}\\right)}{2 \\pi \\sqrt{u-v^2}} \\mathbf{I}_{\\{v \\ge u^2\\}}.\\]\nRemarque : On peut recalculer à partir de cette densité les marginales, mais il est évident que \\(X \\sim \\mathcal{N}(0,1)\\), et on avait vu plus haut que \\(X^2+Y^2 \\sim \\exp(1/2)\\).\nOn a pour \\(\\phi : \\mathbb{R}^2 \\to \\mathbb{R}_+\\) borélienne\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(U,V)]\n& =  \\int_{\\mathbb{R}_+} \\phi( [x], x-[x])  \\exp(-x) dx \\\\\n& =  \\sum_{n \\ge 0} \\int_n^{n+1} \\phi(n, x-n) \\exp(-x) dx \\\\\n& =  \\sum_{n \\ge 0} \\int_0^1 \\phi(n,v) \\exp(-n) \\exp(-v) du\n\\end{align*}\\]\nSi \\(g : \\mathbb{R} \\to \\mathbb{R}_+\\), \\(h : \\mathbb{R} \\to \\mathbb{R}_+\\) boréliennes, quitte à considérer \\(\\phi(u,v) = g(u) h(v)\\) on obtient\n\\[\\begin{align*}\n\\mathbb{E}[g(U) h(V)]\n& =  \\sum_{n \\ge 0} g(n) \\exp(-1)^n (1-\\exp(-1)) \\int_0^1 h(v) \\frac{\\exp(-v)}{1-\\exp(-1)} du \\\\  \n& =  \\mathbb{E}[g(U)] \\mathbb{E}[h(V)]  \n\\end{align*}\\]\nOn conclut que \\(U\\) et \\(V\\) sont indépendantes, avec \\(U + 1 \\sim  \\mathrm{Geom}(1-\\exp(-1))\\) et \\(V\\) de densité \\(f_V\\) avec, pour tout \\(v\\in \\mathbb{R}\\),\n\\[f_V(v) = \\frac{1}{1-\\exp(-1)} \\exp(-v) \\mathbf{1}_{(0,1)}(v).\\]\nRemarque : La loi de \\(V\\) est celle d’une variable exponentielle de paramètre \\(1\\) conditionnée à être plus petite que \\(1\\).\n\n\n\n\nExercice 22  \n\nSoient \\(X_1, X_2\\) deux variables indépendantes et identiquement distribuées suivant la loi \\(\\mathrm{Unif}\\{1,2,3\\}\\).\nOn note \\(U = \\min\\{X_1,X_2\\}\\), \\(V= \\max\\{X_1,X_2\\}\\) et enfin \\(S= U+V\\).\n\nDéterminer la loi jointe de \\((U, V)\\) et \\((V, S)\\).\nEn déduire les lois de \\(U, V\\), et \\(S\\). Calculer les lois de \\(UV\\) et \\(VS\\).\nCalculer les covariances et les coefficients de corrélation de \\((U, V)\\) et \\((V, S)\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nPar hypothèse, la loi de \\((X_1,X_2)\\) est uniforme sur \\(\\{1,2,3\\}^2\\), et on déduit, quitte à disjoindre les cas, que\n\\[\\begin{align*}\n& \\mathbb{P}((U,V) =(1,1)) = \\mathbb{P}(X_1=1, X_2=1) = 1/9 \\\\  \n& \\mathbb{P}((U,V)=(1,2)) = \\mathbb{P}(X_1=1,X_2=2) + \\mathbb{P}(X_1=2,X_2=1) = 2/9 \\\\\n& \\mathbb{P}((U,V) = (1,3)) =  \\mathbb{P}(X_1=1,X_2=3) + \\mathbb{P}(X_1=3,X_2=1) = 2/9 \\\\\n& \\mathbb{P}((U,V) = (2,2)) = \\mathbb{P}(X_1=2, X_2=2) = 1/9 \\\\  \n& \\mathbb{P}((U,V)=(2,3)) = \\mathbb{P}(X_1=2,X_2=3) + \\mathbb{P}(X_1=3,X_2=2) = 2/9 \\\\\n& \\mathbb{P}((U,V)=(3,3)) = \\mathbb{P}(X_1=3,X_2=3) = 1/9\n\\end{align*}\\]\nLa même disjonction de cas conduit à la loi de \\((V,S)\\) :\n\\[\\begin{align*}\n& \\mathbb{P}(V=1,S=2) = 1/9 \\\\  \n& \\mathbb{P}(V=2,S=3) = 2/9 \\\\  \n& \\mathbb{P}(V=3,S=4) = 2/9 \\\\\n& \\mathbb{P}(V=2,S=4) = 1/9 \\\\\n& \\mathbb{P}(V=3, S=5) = 2/9 \\\\  \n& \\mathbb{P}(V=3, S=6) = 1/9  \n\\end{align*}\\]\nOn en déduit (on peut également faire un raisonnement direct)\n\\[\\begin{align*}\n& \\mathbb{P}(U=1) = 5/9, \\ \\mathbb{P}(U=2)=1/3, \\ \\mathbb{P}(U=3)=1/9 \\\\  \n& \\mathbb{P}(V=1) = 1/9, \\ \\mathbb{P}(V=2) = 1/3,\\ \\mathbb{P}(V=3)=5/9 \\\\\n& \\mathbb{P}(S=2) = \\mathbb{P}(S=6) = 1/9, \\ \\mathbb{P}(S=3)=\\mathbb{P}(S=5) = 2/9. \\ \\mathbb{P}(S=4)=1/3\n\\end{align*}\\]\nLa loi de \\(UV\\) se déduit facilement de la loi jointe de \\((U,V)\\) calculée à la question précédente\n\\[\\mathbb{P}(UV=1) = \\mathbb{P}(UV=4) = pp(UV=9) = 1/9, \\ \\mathbb{P}(UV=2) = \\mathbb{P}(UV=3) = \\mathbb{P}(UV=6) = 2/9.\\]\n\nDe même pour la loi de \\(VS\\)\n$$\\mathbb{P}(VS=2) = \\mathbb{P}(VS=8) = \\mathbb{P}(VS=18) = 1/9, \\ \\mathbb{P}(VS=6) = \\mathbb{P}(VS=12) = \\mathbb{P}(VS=15) = 2/9$$ \n\nOn déduit de la question précédente\n\\[\\mathbb{E}[U]=\\frac{14}{9}, \\ \\mathbb{E}[V]=\\frac{22}{9}, \\ \\mathbb{E}[UV]=4,\\ \\mathrm{Cov}(U,V) = \\frac{16}{81},\\]\net\n\\[\\mathrm{Var}(U) = \\frac{38}{81} = \\mathrm{Var}(V), \\ \\ \\rho(U,V)= \\frac{8}{19}.\\]\nPar ailleurs\n\\[\\mathbb{E}[V]= \\frac{22}{9}, \\ \\mathbb{E}[S]=4, \\ \\mathbb{E}[VS]=\\frac{94}{9}, \\ \\mathrm{Cov}(V,S) = \\frac{2}{3}.\\]\net\n\\[\\mathrm{Var}(V) = \\frac{38}{9}, \\mathrm{Var}(S) = \\frac{2}{3}, \\rho(V,S) \\approx 0.397\\]"
  },
  {
    "objectID": "corriges/td1.html#le-cadre-gaussien",
    "href": "corriges/td1.html#le-cadre-gaussien",
    "title": "Variables aléatoires réelles",
    "section": "Le cadre gaussien",
    "text": "Le cadre gaussien\n\nExercice 23  \n\nOn considère deux variables indépendantes \\(Y \\sim \\mathcal{N}(0,1)\\) et\n\\[\\varepsilon= \\begin{cases} & 1 \\mbox{ avec probabilité } p \\\\ & -1 \\mbox{ avec probabilité } 1-p, \\end{cases}\\]\noù \\(p \\in (0,1)\\).\n\nQuelle est la loi de \\(Z= \\varepsilon Y\\)\nQuelle est la loi de \\(Y+Z\\)?\nLe vecteur \\((Y,Z)\\) est-il un vecteur gaussien?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nSoit \\(\\phi : \\mathbb{R} \\to \\mathbb{R}_+\\) borélienne\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(\\varepsilon)]\n& =  \\mathbb{E}[\\phi(Y) \\mathbf{1}_{\\varepsilon=1}] + \\mathbb{E}[\\phi(-Y) \\mathbf{1}_{\\varepsilon=-1}]  \\\\\n& =  p \\mathbb{E}[\\phi(Y)] + (1-p) \\mathbb{E}[\\phi(-Y)] \\\\\n& =  p \\int_{\\mathbb{R}} \\phi(z) \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2) \\mathrm{d}z + (1-p) \\int_{\\mathbb{R}} \\phi(-z) \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2) \\mathrm{d}z  \\\\\n& =  \\int_{\\mathbb{R}} \\phi(z) \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2) \\mathrm{d}z\n\\end{align*}\\]\noù on a effectué le changement de variables \\(u=-z\\) pour voir que \\(\\int_{\\mathbb{R}} \\phi(-z) \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2) \\mathrm{d}z = \\int_{\\mathbb{R}} \\phi(u) \\frac{1}{\\sqrt{2\\pi}} \\exp(-u^2/2) du\\). On conclut que \\(Z \\sim \\mathcal{N}(0,1)\\).\nOn a $Y+Z = (1+et donc pour \\(\\phi\\) comme ci-dessus\n\\[\\mathbb{E}[\\phi(Y+Z)] = (1-p)\\phi(0) + p\\int_{\\mathbb{R}} \\phi(u) \\frac{1}{2 \\sqrt{2\\pi}} \\exp(-u^2/8) du.\\]\nEn particulier la loi de \\(Y+Z\\) n’est ni discrète ni à densité, donc pas gaussienne.\nNon, puisque \\(Y+Z\\) n’est pas gaussienne.\n\n\n\n\nExercice 24  \n\nSoit \\((X,Y,Z)\\) le vecteur aléatoire gaussien d’espérance \\((1,1,0)\\) et de matrice de covariance\n\\[K = \\begin{pmatrix} 2 & 1 & 1 \\\\ 1 & 2 & 2 \\\\ 1 & 2 & 2  \\end{pmatrix}.\\]\n\nEcrire la fonction caractéristique de \\((X,Y,Z)\\).\n\nTrouver la loi de \\(2X+Y+Z\\), de \\(4X-2Y+Z\\), enfin de \\(Y-Z\\).\nLe vecteur \\((X,Y)\\) admet-il une densité dans \\(\\mathbb{R}^2\\). Si oui, laquelle?\nPour \\(a \\in \\mathbb{R}\\) on définit \\(u_a : \\mathbb{R}^3 \\to \\mathbb{R}^3\\) de matrice\n\\[A = \\begin{pmatrix} 1/\\sqrt{2} & 0 & 0 \\\\ a & -2a & 0 \\\\ 0 & 1 & -1\\end{pmatrix}.\\]\nDéterminer la loi de \\(u_a(X-1,Y-1,Z)\\) en fonction de \\(a\\).\nPour quelle valeur de \\(a\\) les deux premières coordonnées de \\(u_a(X-1,Y-1,Z)\\) suivent-ils une loi normale centrée réduite sur \\(\\mathbb{R}^2\\)?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\nDans les questions 2,3,4 ci-dessous, on fait usage de la Prop 3 du cours : si \\(V \\sim \\mathcal{N}(m,K)\\) est un vecteur gaussien de dimension \\(d\\) et \\(P\\) est une matrice à \\(p\\) lignes et \\(d\\) colonnes, on a \\(X \\sim \\mathcal{N}(Pm, P K P^T)\\). Ici \\(d=3\\), \\(m\\) est le vecteur de coordonnées \\((1,1,0)\\), et \\(K\\) la matrice précisée dans l’énoncé.\n\nD’après le cours (Prop 2), pour tout \\(t \\in \\mathbb{R}^3\\) de coordonnées \\((t_1,t_2,t_3)\\),\n\\[\\begin{align*}\n\\Phi_X(t)\n& = \\exp(it^Tm + PKP^T) \\\\  \n&  =  \\exp\\bigg(i(t_1+t_2) - (t_1^2+t_2^2+t_3^2+t_1 t_2+t_1 t_3+2t_2 t_3)\\bigg)\n\\end{align*}\\]\nOn applique le résultat général avec \\(P = (2 \\ \\ 1 \\ \\ 1)\\) puis avec \\(P=(4 \\ -2 \\ \\ 1)\\), et enfn avec \\(P=(0 \\ \\ 1 \\ -1)\\) pour obtenir\n\\[2X+Y+Z \\sim \\mathcal{N}(3,24), \\quad 4X-2Y+Z \\sim \\mathcal{N}(2, 26), \\quad Y-Z \\sim \\mathcal{N}(1,0)\\]\nDans le troisième cas notons que \\(\\mathbb{P}(Y-Z = 1)=1\\), de sorte que \\(Y-Z\\) n’a pas de densité.\nOn a \\(\\begin{pmatrix} X \\\\ Y \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mu=\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\Sigma = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\\right)\\). On a \\(\\det(\\Sigma)=3\\), et est \\(\\Sigma^{-1} = \\frac{1}{3} \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix}\\), et donc ce vecteur possède la densité \\(f\\) sur \\(\\mathbb{R}^2\\), où pour \\(x \\in \\mathbb{R}^2\\) de coordonnées \\((x_1,x_2)\\),\n\\[\\begin{align*}\nf(x)\n& =  \\frac{1}{2\\pi\\sqrt{3}} \\exp(-\\frac{1}{2} (x-\\mu)^T \\Sigma^{-1} (x-\\mu) \\\\\n& =  \\frac{1}{2 \\pi \\sqrt{3}} \\exp\\left(-\\frac{1}{3} ((x_1-1)^2 + (x_2-1)^2 - (x_1-1)(x_2-1)) \\right)\n\\end{align*}\\]\nOn obtient ici que\n\\[A \\begin{pmatrix} X \\\\ Y \\\\ Z \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 6a^2 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} \\right),\\]\nde sorte qu’on a le résultat souhaité pour \\(a \\in \\left\\{-\\frac{1}{\\sqrt{6}}, \\frac{1}{\\sqrt{6}} \\right\\}\\)\n\n\n\n\nExercice 25  \n\nSoit \\(\\rho\\in ]-1,1[\\) et \\((X,Y)\\) un vecteur gaussien centré de matrice de covariances \\(M = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}\\). On notera \\(\\sigma = \\sqrt{1-\\rho^2}\\).\n\nCalculer det(\\(M\\)), \\(M^{-1}\\), puis exprimer la densité \\(f_{(X,Y)}\\) du vecteur \\((X,Y)\\).\nMontrer que\n\\[g_x(y) := \\frac{f_{(X,Y)}(x,y)}{f_X(x)} = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp \\left( - \\frac{1}{2(1-\\rho^2)} \\left(y-\\rho x\\right)^2\\right).\\]\nMontrer que pour tout \\(x \\in \\mathbb{R}\\), \\(y \\to g_x(y)\\) définit une densité.\nSi on note \\(Y_x\\) une variable de densité \\(g_x\\), que pouvez-vous dire sur la loi de \\(Y_x\\)?\n\nTrouver \\(\\alpha\\), \\(\\beta\\) deux réels tels que \\((X, \\alpha X + \\beta Y)\\) suit la loi normale centrée réduite.\n\nRemarquer que l’on peut écrire \\(Y= -\\frac{\\alpha}{\\beta} X +\\frac{1}{\\beta}(\\alpha X + \\beta Y)\\). Sauriez-vous dire pourquoi cette écriture est intéressante?\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nOn a \\(\\det(M) = 1-\\rho^2 \\ne 0\\), \\(M^{-1} =  \\frac{1}{1-\\rho^2}  \\begin{pmatrix} 1 & - \\rho \\\\ -\\rho & 1 \\end{pmatrix}\\), et donc \\(\\begin{pmatrix} X \\\\ Y \\end{pmatrix}\\) possède la densité \\(f_{(X,Y)}\\) où, pour \\(\\begin{pmatrix} x \\\\ y \\end{pmatrix} \\in \\mathbb{R}^2\\),\n\\[\\begin{align*}\nf_{(X,Y)}(x)\n& = \\frac{1}{2\\pi \\sqrt{1-\\rho^2}} \\exp\\left(-\\frac{1}{2} \\begin{pmatrix} x \\\\ y \\end{pmatrix}^T M^{-1} \\begin{pmatrix} x \\\\ y \\end{pmatrix}  \\right)\\\\  \n& =  \\frac{1}{2\\pi \\sigma} \\exp\\left(- \\frac{1}{2 \\sigma^2} \\left(x^2 + y^2 - 2 \\rho x y \\right) \\right)\n\\end{align*}\\]\n\\(X \\sim \\mathcal{N}(0,1)\\) donc \\(f_X(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-x^2/2), x \\in \\mathbb{R}\\). On a donc, pour \\(y \\in \\mathbb{R}\\)\n\\[\\begin{align*}\ng_x(y)\n& =  \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp\\left( \\frac{1}{2(1-\\rho^2)} (x^2+y^2 - 2\\rho x y) + \\frac{1}{2} x^2 \\right) \\\\  \n& = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left( \\frac{1}{2(1-\\rho^2)} \\left( - \\frac{1}{2} (\\rho^2 x^2 - 2 \\rho x y + y ^2) \\right)\\right) \\\\\n& =   \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left( \\frac{1}{2(1-\\rho^2)} \\left( - \\frac{1}{2} (y-\\rho x)^2 \\right) \\right)\n\\end{align*}\\]\ncomme souhaité, et donc \\(Y_x \\sim \\mathcal{N}(\\rho x, 1-\\rho^2)\\).\nComme \\(\\begin{pmatrix} X \\\\ \\alpha X + \\beta Y \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ \\alpha & \\beta \\end{pmatrix}\\) d’après Prop 3 du cours\n\\[\\begin{pmatrix} X \\\\ \\alpha X + \\beta Y \\end{pmatrix}  \\sim \\mathcal{N}\\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 1 & 0 \\\\ \\alpha & \\beta \\end{pmatrix} \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ \\alpha & \\beta \\end{pmatrix}^T\\right)\\]\net donc\n\\[\\begin{pmatrix} X \\\\ \\alpha X + \\beta Y \\end{pmatrix}  \\sim \\mathcal{N}\\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 1 & 0 \\\\ \\alpha & \\beta \\end{pmatrix}\\begin{pmatrix} 1 & \\alpha+\\rho \\beta \\\\ \\rho & \\rho \\alpha + \\beta \\end{pmatrix} \\right) \\sim \\mathcal{N}\\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 1 & \\alpha+ \\rho \\beta \\\\ \\alpha + \\rho \\beta  & \\alpha^2 + 2 \\rho \\alpha \\beta + \\beta^2  \\end{pmatrix} \\right)\\]\nPour obtenir le résultat souhaité il faut et il suffit que \\(\\begin{cases} & \\alpha + \\rho \\beta = 0 \\\\ & \\alpha^2 + 2\\rho \\alpha \\beta + \\beta ^2 = 1 \\end{cases}\\),\nEn remplaçant \\(\\alpha= -\\rho \\beta\\) dans la deuxième équation il vient que nécessairement\n\\[\\rho^2 \\beta^2 - 2 \\rho \\alpha \\beta + \\beta ^2  = (1-\\rho)^2 \\beta ^2 = 1,\\]\net on obtient que \\((\\alpha,\\beta) \\in \\left\\{ \\left( \\frac{\\rho}{1-\\rho}, -\\frac{1}{1-\\rho}\\right), \\left(\\frac{-\\rho}{1-\\rho},\\frac{1}{1-\\rho}\\right) \\right\\}\\)\nOn a alors, comme indiqué dans l’énoncé\n\\[Y =  \\rho X + (1-\\rho) \\left(-\\frac{\\rho}{1-\\rho} X + \\frac{1}{1-\\rho} Y \\right),\\]\net quitte à poser \\(Z =  -\\frac{\\rho}{1-\\rho} X + \\frac{1}{1-\\rho} Y\\) qui est une normale centrée réduite indépendante de \\(X\\), on a\n\\[Y = \\rho X + (1-\\rho) Z,\\]\nqui permet de comprendre comment \\(Y\\) dépend de \\(X\\).\n\nLa décomposition sera en particulier très utile pour caculer espérance et loi conditionnelle de \\(Y\\) sachant \\(X\\).\n\n\n\nExercice 26  \n\nMontrer que le vecteur aléatoire de dimension \\(3\\) de moyenne \\(m = (7,0,1)\\) et de matrice de covariances\n\\[K = \\begin{pmatrix} 10 & -1 & 4 \\\\ -1 & 1 & -1 \\\\ 4 & -1 & 2 \\end{pmatrix}\\]\nappartient presque sûrement à un hyperplan affine de \\(\\mathbb{R}^3\\) que l’on déterminera.\n\n\n\n\n\n\nNoteSolution\n\n\n\nEn résolvant \\(K x = 0\\) on trouve \\(\\ker(K) = \\mathrm{Vect} \\left\\{ \\begin{pmatrix} -1\\\\ 2\\\\  3 \\end{pmatrix}  \\right\\}\\). On a donc\n\\[\\ker(K)^{\\perp} = \\left\\{ (x,y,z) \\in \\mathbb{R}^3 : -x + 2y + z =0 \\right\\}\\]\nD’après le cours, \\(\\mathbb{P}(X - m \\in \\ker(K)^{\\perp}) =1\\), de sorte que p.s.\n\\[ X \\in \\left\\{  (x,y,z) \\in \\mathbb{R}^3 : -x + 2y + z = -6 \\right\\}.\\]"
  },
  {
    "objectID": "corriges/td4.html",
    "href": "corriges/td4.html",
    "title": "MA1AY010 Probabilités",
    "section": "",
    "text": "NoteTD IV : Espérance conditionnelle/Catactérisations\n\n\n\n\n29 Septembre 2025-2 octobre 2025\nMaster I ISIFAR\nProbabilités\n\n\n\n\n\n\n\n\n\nNoteConventions\n\n\n\nDans les 3 exercices qui suivent, \\(X_1, \\ldots, X_n, ...\\) constituent une famille indépendante de variables aléatoires identiquement distribuées à valeur dans \\(\\{-1,1\\}\\).\nL’univers des possibles est \\(\\Omega = \\{-1,1\\}^{\\mathbb{N}}\\). Les \\(X_i\\) sont les projections canoniques.\nOn note \\(\\mathcal{F}_n\\) la tribu engendrée par les \\(n\\) premières coordonnées :\n\\[\n\\mathcal{F}_n =  \\sigma(X_1, \\ldots, X_n) \\,\n\\]\nL’univers est muni de la tribu des cylindres \\(\\mathcal{F} = \\sigma\\left(\\bigcup_n \\mathcal{F}_n\\right)\\).\nOn note \\(\\Delta\\) une constante à valeur dans \\((0,1)\\) (la dérive de la marche aléatoire).\nOn note \\(\\mathbb{P}\\) la loi produit infini, telle que pour tout \\(x \\in \\{-1, 1\\}^n\\)\n\\[\n\\mathbb{P}\\left\\{\\bigwedge_{i=1}^n X_i = x_i\\right\\} = \\prod_{i=1}^n \\frac{1}{2}\\left(1 + x_i \\Delta\\right)\n\\]\nOn étudie la marche alétoire sur \\(\\mathbb{Z}\\) de dérive \\(\\Delta\\).\nOn note \\(S_n = \\sum_{i=1}^n X_i\\).\nL’indice \\(n\\) représente le temps, \\(S_n\\) la position à l’instant \\(n\\).\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nExercice 1 (Marches aléatoires biaisées i)  \n\n\nQuelle est la loi de \\(S_n\\) ?\n\\(S_n\\) est-elle \\(\\mathcal{F}_n, \\mathcal{F}_{n-1}, \\mathcal{F}_{n+1}\\) mesurable ?\nQuelle est l’espérance de \\(S_n\\) ?\nQuelle est la variance de \\(S_n\\) ?\n\n\n\n\n\n\n\nNoteOn admettra l’inégalité de Hoeffding:\n\n\n\nSi \\(Y_1, \\ldots, Y_n\\) sont des variables aléatoires indépendantes telles que \\(a_i \\leq Y_i \\leq b_i\\) (les \\(Y_i\\) sont bornées), alors\n\\[\nP \\left\\{  Z - \\mathbb{E} Z \\geq t  \\right\\} \\leq \\mathrm{e}^{- 2 \\frac{t^2}{\\sum_{i=1}^n (b_i-a_i)^2}}\n\\]\navec \\(Z = \\sum_{i=1}^n Y_i\\).\n\n\n\nExercice 2 (Marches aléatoires biaisées ii)  \n\nPour \\(0 \\leq \\tau \\leq  n \\Delta\\),\n\nMajorer \\(\\mathbb{P}\\{ S_n \\leq \\tau \\}\\) à l’aide de l’inégalité de Chebyshev\nMajorer \\(\\mathbb{P}\\{ S_n \\leq \\tau \\}\\) à l’aide de l’inégalité de Hoeffding\nL’ensemble \\[\nE = \\left\\{ \\omega :  \\forall n,  S_n(\\omega) &lt; \\tau \\right\\}\n\\] appartient-il à la tribu \\(\\mathcal{F}_m\\) pour un \\(m\\) donné ? est-il un événement de \\(\\mathcal{F}\\) ?\nSi \\(E\\) est un événement, quelle est sa probabilité ?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\n\n\n\n\n\nNoteConvention\n\n\n\nOn suppose \\(\\tau \\in \\mathbb{N} ∖  \\{0\\}\\).\nOn note \\(T = \\inf \\left\\{ n : S_n \\geq \\tau \\right\\}\\). Si \\(\\forall n, \\quad S_n(\\omega)&lt; \\tau\\), alors \\(T(\\omega) = \\infty\\).\nOn note \\(S_T\\), la fonction définie par \\[\nS_T(\\omega) = \\sum_{n=1}^\\infty \\mathbb{I}_{T(\\omega)=n} S_n(\\omega)\\qquad \\text{si } T(\\omega) &lt; \\infty\n\\] et \\(S_T(\\omega) =0\\) si \\(T(\\omega) =\\infty\\).\n\n\n\nExercice 3 (Marches aléatoires biaisées iii)  \n\n\nPourquoi peut-on considérer que \\(T\\) est une variable aléatoire (à valeur dans \\(\\mathbb{N} \\cup \\{\\infty\\}\\)) ?\nQuelle est la probabilité que \\(T = \\infty\\) ?\nL’événement \\(\\{ T \\leq n \\}\\) est-il \\(\\mathcal{F}_{n-1}, \\mathcal{F}_n, \\mathcal{F}_{n+1}\\) mesurable ?\nPourquoi peut-on considérer que \\(S_T\\) est une variable aléatoire ?\nQuelle est l’espérance de \\(S_T\\) ?\nMontrer que \\(\\mathbb{E} S_T = \\Delta \\mathbb{E} T\\) En déduire \\(\\mathbb{E} T\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nExercice 12 (Binomiale négative) Les variables \\(X_1, :::\nX_2, \\ldots, X_n, \\ldots\\) sont des variables de Bernoulli de probabilité de succès \\(p \\in (0,1)\\), indépendantes. On définit \\(T_1 = \\min \\{i : X_i=1\\}\\) (temps du premier succès), \\(T_2 = \\min \\{i : i &gt; T_1, X_i=1\\}\\) (temps du premier succès après \\(T_1\\)), et récursivement \\(T_{n+1} = \\min \\{ i : i &gt; T_n, X_i =1\\}\\) (temps du \\(n+1\\)eme succès).\nOn admet l’existence d’un espace de probabilité \\((\\Omega, \\mathcal{F}, P)\\) où \\(\\Omega = \\{0,1\\}^{\\mathbb{N}}\\), \\(\\mathcal{F}\\) est une tribu pour laquelle les \\(X_i\\) sont mesurables, et \\(P\\) tel que \\(X_1, \\ldots, X_n, \\ldots\\) est une famille indépendante.\n\n\\(T_1\\) et plus généralement \\(T_n\\) sont-elles des variables aléatoires?\nCalculer \\(P \\{ T_1 &gt; k  \\}\\) pour \\(k \\in \\mathbb{N}\\).\nCalculer \\(P \\{ T_1 = k  \\}\\) pour \\(k \\in \\mathbb{N}\\)\nCalculer \\(\\mathbb{E}T_1\\).\nCalculer \\(P \\{ T_1 = k  \\wedge T_2 = k+j\\}\\) pour \\(k, j \\in \\mathbb{N}\\)\nCalculer \\(P \\{ T_2 = k  \\}\\) pour \\(k \\in \\mathbb{N}\\)\nCalculer \\(\\mathbb{E}T_2\\)\nCalculer \\(\\mathbb{E} T_n\\)\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nLes événements de la forme \\(\\{ T_n \\leq m\\}\\) sont dans \\(\\sigma(X_1, \\ldots X_m)\\), car la seule connaissance de \\(X_1, \\ldots, X_m\\) suffit pour déterminer si le \\(i^{\\text(eme)}\\) succès survient avant le temps \\(m\\). Donc les événements de la forme \\(\\{ T_n \\leq m\\}\\) sont tous dans la tribu des cylindres \\(\\sigma\\left(\\cup_m \\sigma(X_1, \\ldots X_m) \\right)\\). Tout événement de la forme \\(\\{ T_n \\in A\\}\\) avec \\(A \\subset \\mathbb{N} \\cup \\{ \\infty\\}\\), appartient à la tribu engendrée par les événements \\(\\{ T_n \\leq m\\}\\), donc à la tribu \\(\\sigma\\left(\\cup_m \\sigma(X_1, \\ldots X_m) \\right)\\).\n\\(P\\{ T_1 &gt; k  \\} = (1-p)^k\\) (\\(T_1\\) suit une loi géométrique)\n\\(P\\{ T_1 = k  \\} = P\\{ T_1 &gt; k-1  \\} - P\\{ T_1 &gt; k  \\} = (1-p)^{k-1} p\\) pour \\(k\\geq 1\\)\n\\(\\mathbb{E} T_1 =  \\sum_{k=0}^\\infty P\\{ T_1 &gt; k  \\} = \\frac{1}{p}\\)\n\\(P \\{ T_1 = k  \\wedge T_2 = k+j\\} = (1-p)^{k-1} p (1-p)^{j-1} p\\) pour \\(k, j \\in \\mathbb{N}\\setminus \\{0\\}\\), \\(T_1 \\perp\\!\\!\\!\\perp T_2-T_1\\) et \\(T_2-T_1 \\sim T_1\\)\n\n\\(P \\{ T_2 = k  \\} = \\sum_{j=1}^{k-1} (1-p)^{j-1} p (1-p)^{k-j-1} p = p \\binom{k-1}{1} (1-p)^{k-2} p\\) pour \\(k\\geq 2\\)\n\\(\\mathbb{E} T_2 = 2 \\mathbb{E} T_1 = \\frac{2}{p}\\)\n\\(\\mathbb{E} T_n = n \\mathbb{E} T_1 = \\frac{n}{p}\\)\n\n\n\n\nExercice 4 (Allocations aléatoires)  \n\nOn dispose de \\(n\\) urnes numérotées de \\(1\\) à \\(n\\) et de \\(n\\) boules. Les boules sont réparties de manière uniforme dans les urnes (chaque boule se comporte de manière indépendante des autres et a probabilité \\(1/n\\) de tomber dans chaque urne). On note \\(U_i\\) la variable aléatoire désignant le nombre de boules qui tombent dans l’urne \\(i\\). Dans la suite \\(\\alpha &gt;1\\) est un réel.\n\nDéterminer la loi de \\(U_i\\).\nMontrer que l’on a : \\[\\mathbb{P}( \\max_{1 \\leq i \\leq n} U_i &gt; \\alpha \\ln n) \\leq n \\mathbb{P}( U_1 &gt; \\alpha \\ln n).\\]\nCalculer \\(\\mathbb{E}(\\exp(U_1))\\).\nMontrer que pour tout \\(\\beta &gt; -n\\), on a \\((1+\\beta/n)^n \\leq \\exp(\\beta)\\).\nMontrer que \\(\\mathbb{P}(U_1 &gt; \\alpha \\ln n) \\leq \\frac{\\exp(\\exp(\\alpha)-1)}{n^\\alpha}\\).\nEn déduire que si \\(\\alpha &gt;1\\), on a \\[\\mathbb{P}( \\max_{1 \\leq i \\leq n} U_i &gt; \\alpha \\ln n) \\rightarrow_{n \\rightarrow \\infty} 0.\\]\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\\(U_i  \\sim \\text{Binom}(n, 1/n)\\)\nLes \\(U_i\\) ne sont pas indépedantes (on a toujours \\(\\sum_{i=1}^n U_i =n\\)), mais elles sont identiquement distribuées.\n\\[\\begin{align*}\n\\mathbb{P}( \\max_{1 \\leq i \\leq n} U_i &gt; \\alpha \\ln n)\n   & = \\mathbb{P}( \\cup_{1 \\leq i \\leq n} \\{ U_i &gt; \\alpha \\ln n\\} ) \\\\\n   & \\leq \\sum_{i=1}^n  \\mathbb{P}( \\{ U_i &gt; \\alpha \\ln n\\} ) \\\\\n   & = n \\mathbb{P}( \\{ U_1 &gt; \\alpha \\ln n\\} )\n\\end{align*}\\]\n\\(\\mathbb{E}(\\exp(U_1)) = \\left(1 + \\frac{1}{n} \\left(\\mathrm{e}-1\\right) \\right)^n \\leq \\exp\\left( \\mathrm{e}-1 \\right)\\)\nEn utilisant l’inégalité de Markov,\n\\[\\begin{align*}\n   \\mathbb{P}( \\max_{1 \\leq i \\leq n} U_i &gt; \\alpha \\ln n)\n     & \\leq n \\mathbb{P}( \\{ U_1 &gt; \\alpha \\ln n\\} ) \\\\\n     & \\leq n \\frac{\\mathbb{E}(\\exp(U_1))}{n^{\\alpha}} \\\\\n     & \\leq \\frac{\\exp\\left( \\mathrm{e}-1 \\right)}{n^{\\alpha-1}}\n\\end{align*}\\]\n\n\n\n\nExercice 5 (Restitution Organisée de Connaissances)  \n\n\nSoient \\(A, B, C\\) trois événements dans un espace probabilisé. A-t-on toujours: \\(A \\perp\\!\\!\\!\\perp B \\text{ et } B \\perp\\!\\!\\!\\perp C \\Rightarrow A \\perp\\!\\!\\!\\perp C\\)?\nSoient \\(P\\) et \\(Q\\) deux lois de probabilités sur \\((\\Omega, \\mathcal{F})\\), on définit l’ensemble \\(\\mathcal{M} = \\big\\{ A : A \\in \\mathcal{F}, P(A)=Q(A)\\}\\). Répondre par vrai/faux/je ne sais pas aux questions suivantes:\n\n\\(\\mathcal{M}\\) est-il toujours une classe monotone ?\n\\(\\mathcal{M}\\) est-il toujours une \\(\\sigma\\)-algèbre ?\n\\(\\mathcal{M}\\) est-il toujours une \\(\\pi\\)-classe ?\n\nSoient \\(G\\) et \\(F\\) sont deux fonctions génératrices de probabilité. Répondre par vrai/faux aux questions suivantes:\n\nEst-il vrai que \\(G \\times F\\) est toujours une fonction génératrice ?\nEst-il vrai que \\(G + F\\) est toujours une fonction génératrice de probabilité ?\nEst-il vrai que \\(\\lambda G + (1-\\lambda) F\\) avec \\(\\lambda \\in [0,1]\\) est toujours une fonction génératrice de probabilité ?\n\nSi \\(\\widehat{F}\\) est la fonction caractéristique de la loi de \\(X\\), et si \\(\\epsilon \\perp\\!\\!\\!\\perp X\\), avec \\(P\\{\\epsilon=1\\}= P\\{\\epsilon=-1\\}=1/2\\), quelle est la fonction caractéristique de la loi de \\(\\epsilon X\\)?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nExercice 6 (Distributions biaisées par la taille)  \n\nSi \\(X\\) est une variable aléatoire positive intégrable, la version biaisée par la taille de \\(X\\) est la variable aléatoire \\(X^*\\) dont la loi \\(Q\\) est absolument continue par rapport à celle de \\(X\\) (notée \\(P\\)) et dont la densité (par rapport à celle de \\(X\\)) est proportionnelle à \\(X\\): \\[\n\\frac{\\mathrm{d}Q}{\\mathrm{d}P}(x) = \\frac{x}{\\mathbb{E}X} \\, .\n\\]\n\nCaractériser \\(X^*\\) lorsque \\(X\\) est une Bernoulli.\nCaractériser \\(X^*\\) lorsque \\(X\\) est binomiale.\nCaractériser \\(X^*\\) lorsque \\(X\\) est Poisson.\nCaractériser \\(X^*\\) lorsque \\(X\\) est Gamma.\nSi \\(X\\) est à valeurs entières, exprimer la fonction génératrice de \\(X^*\\) en fonction de celle de \\(X\\).\nExprimer la transformée de Laplace de \\(X^*\\) en fonction de celle de \\(X\\).\nSi \\(U\\) est une transformée de Laplace, dérivable à droite en \\(0\\), \\(U'/U'(0)\\) est-elle la transformée de Laplace d’une loi sur \\([0, \\infty)\\)?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nExercice 7 (Amies des gaussiennes)  \n\n\n\n\n\n\n\nNoteRappel\n\n\n\nLa loi normale centrée réduite \\(\\mathcal{N}(0,1)\\) admet pour densité \\(x\\mapsto \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)\\),\n\n\n\nSi \\(X \\sim \\mathcal{N}(0,1)\\), donner une densité de la loi de \\(Y=\\exp(X)\\) (Loi log-normale). Calculer l’espérance et la variance de \\(Y\\).\nMême question si \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\).\nSi \\(X, Y \\sim \\mathcal{N}(0,1)\\), avec \\(X \\perp\\!\\!\\!\\perp Y\\), donner une densité de la loi de \\(Z=Y/X\\) (Loi de Student à 1 degré de liberté)\nSi \\(X, Y \\sim \\mathcal{N}(0,1)\\), avec \\(X \\perp\\!\\!\\!\\perp Y\\), donner une densité de la loi de \\(W = Y/ \\sqrt{X^2}\\).\nSi \\(X \\sim \\mathcal{N}(0,1)\\) et \\(\\epsilon\\) vaut \\(\\pm 1\\) avec probabilité \\(1/2\\) (variable de Rademacher) avec \\(X \\perp\\!\\!\\!\\perp \\epsilon\\), donner une densité de la loi de \\(Y = \\epsilon X\\). \\(Y\\) et \\(X\\) sont-elles indépendantes ?\n\n\nExercice 8  \n\n\nExercice 9 (Principe de réflexion)  \n\nPrincipe de réflexion\nDans cet exercice, \\(X_1, X_2, \\ldots\\) sont des variables de Rademacher indépendantes (\\(P\\{X_i = \\pm 1\\} = \\frac{1}{2}\\)), \\(S_n =\\sum_{i=1}^n X_i, S_0=0\\) et \\(M_n = \\max_{k \\leq n} S_n\\).\nMontrer que, pour \\(a&gt; 0\\),\n\\[P\\left\\{ M_n &gt; a \\right\\}\\leq 2 P\\left\\{ S_n &gt; a \\right\\}\\]\n\n\n\n\n\n\nNoteStatistique des rangs/Statistiques d’ordre\n\n\n\nLes statistiques d’ordre \\(X_{1:n}\\leq X_{2:n}\\leq X_{n:n}\\) d’un \\(n\\)-échantillon \\(X_1,\\ldots,X_n\\) d’observations indépendantes identiquement distribuées sont formées par le réarrangement croissant (convention) de l’échantillon.\nQuand \\(n\\) est clair d’après le contexte on peut les noter \\(X_{(1)} \\leq \\ldots \\leq X_{(n)}\\).\n\n\n\nExercice 11 (Statistiques d’ordre)  \n\nVérifier que la l::: oi jointe des statistiques d’ordre est absolument continue par rapport à la loi de l’échantillon.\nOn suppose que \\(X\\) est une variable aléatoire réelle, absolument continue, de densité continue. Montrer que l’échantillon est presque sûrement formé de valeurs deux à deux distinctes.\nDonner la densité de la loi jointe des statistiques d’ordre.\nSi la loi des \\(X_i\\) définie par sa fonction de répartition \\(F\\), admet une densité \\(f\\), quelle est la densité de la loi de \\(X_{k:n}\\) pour \\(1\\leq k\\leq n\\) ?\n\nMontrer que conditionnellement à \\(X_{k:n}=x\\), la suite\n\\[(X_{i:n}-X_{k:n})_{i=k+1,\\ldots, n}\\]\nest distribuée comme les statistiques d’ordre d’un \\(n-k\\) échantillon de la loi d’excès au dessus de \\(x\\) (fonction de survie \\(\\overline{F}(x+\\cdot)/\\overline{F}(x))\\) avec la convention \\(\\overline{F}=1-F\\)).\n\n(Représentation de Rényi)\n\nExercice 10 (Statistiques d’ordre d’un échantillon exponentiel) Cet exercice reprend::: les conventions de l’exercice précédent. On s’intéresse maintenant aux statistiques d’ordre d’un échantillon exponentiel.\n\nSi \\(X_1,\\ldots,X_n\\) est un échantillon i.i.d. de la loi exponentielle d’espérance \\(1\\) (densité \\(\\mathbb{I}_{x&gt;0} \\mathrm{e}^{-x}\\)), et \\(X_{n:n}\\geq X_{n-1:n}\\geq  \\ldots \\geq X_{1:n}\\) les statistiques d’ordre associées, montrer que:\navec la convention \\(X_{0:n}=0\\), les écarts \\((X_{i:n}-X_{i-1:n})_{1\\leq i\\leq n}\\) (spacings) forment une collection de variables aléatoires indépendantes ;\n\\(X_{i:n}-X_{i-1:n}\\) est distribuée selon une loi exponentielle d’espérance \\(\\tfrac{1}{i}\\) .\nMaintenant \\((k_n)_n\\) est une suite croissante d’entiers qui tend vers l’infini, telle que \\(k_n/n\\) tend vers une limite finie (éventuellement nulle). Montrer que\n\\[\\frac{X_{k_n:n} -\\mathbb{E} X_{k_n:n} }{\\sqrt{\\operatorname{var}(X_{k_n:n} )}}\\]\nconverge en loi vers une Gaussienne centrée réduite.\n\n\n\n\n\n\n\nNoteSolution"
  },
  {
    "objectID": "corriges/td2.html",
    "href": "corriges/td2.html",
    "title": "MA1AY010 Probabilités",
    "section": "",
    "text": "NoteTD II : Espérances et lois conditionnelles\n\n\n\n22 Septembre 2025-26 Septembre 2025\n\nMaster I Isifar\nProbabilités\n\n\n\n\nExercice 1 (Espérance conditionnelle/tribu atomique)  \n\n\n\n\n\n\n\nNoteCours\n\n\n\nEspérance conditionnelle par rapport à une tribu engendrée par une partition dénombrable.\n\n\n\nSoit \\((A_n, n \\in \\mathbb{N}^*)\\) une partition de \\(\\Omega\\) et \\(\\mathcal{F}= \\sigma(A_n, n \\ge 1)\\) la tribu engendrée par les \\(A_n, n \\ge 1\\). Rappelons qu’une v.a.r. \\(Y\\) est \\(\\mathcal{F}\\)-mesurable si et seulement si il existe une suite de réls \\((a_n)\\) telle que \\(Y= \\sum_{n \\ge 1} a_n \\mathbf{1}_{A_n}\\). Exprimer \\(\\mathbb{E}[X \\mid \\mathcal{F}]\\).\nSoient \\(X,Y\\) deux variables i.i.d. \\(\\sim\\) Ber\\((p)\\). On considère \\(\\mathcal{G} = \\sigma(\\{X+Y=0\\})\\). Calculer \\(\\mathbb{E}[X \\mid \\mathcal{G}], \\mathbb{E}[Y\\mid \\mathcal{G}]\\). Les variables obtenues sont-elles toujours indépendantes?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nNécessairement \\(Y:=\\mathbb{E}[X \\mid \\mathcal{F}]\\) est \\(\\mathcal{F}\\)-mesurable et donc on peut le chercher sous la forme \\(\\sum_{n \\ge 1} a_n \\mathbb{I}_{A_n}\\).\n\nComme \\(A_n \\in \\mathcal{F}\\) on doit nécessairement avoir de plus \\[\\mathbb{E}[X \\mathbb{I}_{A_n}] = \\mathbb{E}[Y \\mathbb{I}_{A_n}] = a_n  \\mathbb{P}(A_n),\\]\ncar \\((A_n, n \\ge 1)\\) est une partition de \\(\\Omega\\). On déduit que \\[a_n = \\frac{\\mathbb{E}[X \\mathbb{I}_{A_n}]}{ \\mathbb{P}(A_n)}, \\ n \\ge 1.\\]\net donc\n\\[\\mathbb{E}[X \\mid \\mathcal{F}] = \\sum_{n \\ge 1}   \\frac{\\mathbb{E}[X \\mathbb{I}_{A_n}]}{ \\mathbb{P}(A_n)} \\mathbb{I}_{A_n}.\\]\n\nOn a \\(\\mathcal{G} = \\{\\emptyset, \\{X = Y = 0\\}, \\{X=1\\} \\cup \\{Y=1\\}, \\Omega\\}\\), et on est dans la situation précédente avec une partition à deux éléments non dégénérés \\(A_1 = \\{X=Y = 0\\}, A_2 = A_1^c = \\{X=1\\} \\cup \\{Y=1\\}\\).\n\nOn a donc\n\\[\\begin{align*}\n\\mathbb{E}[X \\mid \\mathcal{G}] & =  \\frac{\\mathbb{E}[X \\mathbb{I}_{A_1}]}{ \\mathbb{P}(A_1)} \\mathbb{I}_{A_1} +  \\frac{\\mathbb{E}[X \\mathbb{I}_{A_2}]}{ \\mathbb{P}(A_2)} \\mathbb{I}_{A_2} \\\\\n& =  \\frac{2}{3}  \\mathbb{I}_{A_2}\n\\end{align*}\\]\nen utilisant que \\(\\mathbb{E}[X \\mathbb{I}_{A_1}] =0, \\mathbb{E}[X \\mathbb{I}_{A_2}] = \\frac{1}{2},  \\mathbb{P}(A_2) = \\frac{3}{4}\\).\nPar le même raisonnement (\\(X\\) et \\(Y\\) jouent des rôles symétriques)\n\\[\\mathbb{E}[Y \\mid \\mathcal{G}] = \\frac{2}{3}  \\mathbb{I}_{A_2}\\]\nOn obtient que \\(\\mathbb{E}[X \\mid \\mathcal{G}] = \\mathbb{E}[Y \\mid \\mathcal{G}] = \\frac{2}{3} \\mathbb{I}_{A_2}\\), ces variables ne sont clairement pas indépendantes.\n\n\n\nExercice 2 (Conditionnement continu)  \n\nSoient \\((X,Y)\\) un couple de v.a. réelles intégrables de densité jointe \\(f\\), \\(g : \\mathbb{R}^2 \\to \\mathbb{R}\\) borélienne telle que \\(g(X,Y) \\in \\mathbb{L}^1\\).\nRappeler l’expression de \\(\\phi, \\psi\\) telles que \\[\\mathbb{E}[g(X,Y)\\mid Y] = \\phi(Y), \\quad \\mathbb{E}[g(X,Y)|X] = \\psi(X).\\]\n\nOn considère \\((X,Y)\\) de densité jointe \\(f(x,y)= \\frac{1}{x} \\mathbf{1}_{\\{0 \\le y \\le x \\le 1\\}}.\\) Quelle est la loi de \\(X\\)? Calculer la distribution conditionnelle \\(f_{Y \\mid X}\\) de \\(Y\\) sachant \\(X\\). Calculer \\(\\mathbb{P}(X^2 +Y^2 \\le 1 |X)\\), puis en déduire \\(\\mathbb{P}(X^2+Y^2 \\le 1)\\).\nPour simplifier l’expression obtenue on pourra utiliser que \\(x \\to \\sqrt{1-x^2} - \\tanh^{-1}(\\sqrt{1-x^2}) = \\sqrt{1-x^2}-\\frac{1}{2} \\ln(1+\\sqrt{1-x^2}) + \\frac{1}{2} \\ln(1-\\sqrt{1-x^2})\\) est une primitive de \\(x \\to \\frac{\\sqrt{1-x^2}}{x}\\).\nDans le cas général, montrer que \\(\\mathbb{E}[\\mathbb{E}[Y|X]] = \\mathbb{E}[Y]\\). Que vaut \\(\\mathbb{E}[Y]\\) dans l’exemple de la question précédente?\nMontrer, dans le cas général, que \\[\\mathbb{E}[\\mathbb{E}[Y|X] g(X)] = \\mathbb{E}[Yg(X)],\\] pour toute fonction \\(g\\) telle que les deux espérances sont définies. Que vaut \\(\\mathbb{E}[Y g(X) \\mid X]\\)?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\nLorsque \\((X,Y)\\) a densité jointe \\(f\\), rappelons que si on pose\n\\[\nf_{Y \\mid X}(y \\mid x) = \\begin{cases} \\frac{f(x,y)}{f_X(x)}  & \\mbox{ si } f_X(x)&gt;0 \\\\\n0 & \\mbox{ sinon} \\end{cases}, \\quad f_{X \\mid Y}(x \\mid y) = \\begin{cases} \\frac{f(x,y)}{f_Y(y)}  & \\mbox{ si } f_Y(y)&gt;0 \\\\\n0 & \\mbox{ sinon} \\end{cases},\n\\]\n\\[\\phi(y) = \\int_{\\mathbb{R}} g(x,y) f_{X \\mid Y}(x \\mid y) dx \\quad \\ \\psi(x) = \\int_{\\mathbb{R}} g(x,y) f_{Y \\mid X}(y \\mid x) dy,\n\\]\nalors \\[\\mathbb{E}[g(X,Y) \\mid Y]= \\phi(Y), \\qquad \\mathbb{E}[g(X,Y) \\mid X] = \\psi(X).\\]\nMontrons par exemple la deuxième assertion : si \\(A \\in \\sigma(X)\\), i.e. il existe \\(B \\in \\mathcal{B}(\\mathbb{R})\\) tel que \\(A = X^{-1}(B),\\) et \\(\\mathbb{I}_A(\\omega) = \\mathbb{I}_B(X(\\omega))\\), de sorte que (l’usage de Fubini à la troisième ligne ci-dessous est justifié car \\((x,y) \\to |g(x,y)|\\mathbb{I}_B(x)\\) est \\(\\mathbb{P}_{(X,Y)}\\)-intégrable puisque \\((x,y) \\to |g(x,y)|\\) l’est ) :\n\\[\\begin{align*}\n\\mathbb{E}[g(X,Y) \\mathbb{I}_A] & =  \\int_{\\mathbb{R}^2} g(x,y) \\mathbb{I}_B(x) f(x,y) dx dy  \\\\\n& =  \\int_{\\mathbb{R}^2} g(x,y) f_{Y \\mid X}(y \\mid x) f_X(x)  \\mathbb{I}_B(x) dx dy \\\\\n& =  \\int_{\\mathbb{R}}  \\left(\\int_{\\mathbb{R}}  g(x,y) f_{Y \\mid X}(y \\mid x)\\right) \\mathbb{I}_B(x) f_X(x) dx \\\\\n& =  \\mathbb{E}[\\psi(X) \\mathbb{I}_B(X)] = \\mathbb{E}[\\psi(X) \\mathbb{I}_A]\n\\end{align*}\\]\ncomme souhaité.\n\n\n\n\n\n\n\n\nNoteSolution (suite)\n\n\n\n\n\\(X\\) a densité a pour \\(f_X\\) avec \\(f_X\\) nulle en dehors de \\([0,1]\\) et\n\\[f_X(x) = \\int_{\\mathbb{R}} f(x,y) dy = \\frac{1}{x} \\int_{0}^x dy = 1, 0 \\le x \\le 1,\\]\non déduit que \\(X \\sim \\mathrm{Unif}[0,1]\\).\nPar ailleurs\n\\[f_{Y \\mid X} (y \\mid x) = \\frac{1}{x} \\mathbb{I}_{\\{0 \\le y \\le x\\}}.\\]\nRemarque : Cela signifie que sachant \\(X\\), \\(Y \\sim \\mathrm{Unif}[0,X]\\).\nOn en déduit\n\\[\\mathbb{P}(X^2 + Y^2 \\le 1 \\mid X) =  \\mathbb{P}(Y^2 \\le 1-X^2 \\mid X) = \\begin{cases} 1 & \\mbox{ si } X \\le \\frac{1}{\\sqrt{2}}  \\\\ \\frac{\\sqrt{1-X^2}}{X} & \\mbox{ sinon. } \\end{cases}.\\]\nOn a alors, puisque \\(X \\sim \\mathrm{Unif}[0,1]\\), et en utilisant l’indication\n\\[\\begin{align*}\n  &  \\mathbb{P}(X^2+Y^2 \\le 1)  =  \\mathbb{E}[ \\mathbb{P}(X^2+Y^2\\le 1 \\mid X)] \\\\ & =  \\frac{1}{\\sqrt{2}} + \\int_{\\frac{1}{\\sqrt{2}}}^{1} \\frac{\\sqrt{1-x^2}}{x} dx \\\\  & = \\frac{1}{\\sqrt{2}} + \\left[  \\sqrt{1-x^2}-\\frac{1}{2} \\ln(1+\\sqrt{1-x^2}) + \\frac{1}{2} \\ln(1-\\sqrt{1-x^2})\\right]_{\\frac{1}{\\sqrt{2}}}^{1}\\\\   & =  \\frac{1}{\\sqrt{2}} - \\frac{1}{\\sqrt{2}} + \\frac{1}{2} \\ln\\left(1+ \\frac{1}{\\sqrt{2}}\\right) - \\frac{1}{2} \\ln\\left(1-\\frac{1}{\\sqrt{2}}\\right) = \\ln(\\sqrt{2}+1).\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n\nNoteSolution (suite)\n\n\n\n\nComme \\(Y\\) est intégrable on peut appliquer Fubini à la troisième ligne ci-dessous et se servir du fait que \\(\\forall (x,y) \\in \\mathbb{R}^2, \\ f_X(x) f_{Y \\mid X}(y \\mid x) = f(x,y)\\) pour voir que\n\\[\\begin{align*}\\mathbb{E}[\\mathbb{E}[Y \\mid X]] & =  \\mathbb{E}[\\psi(X)] = \\int_{\\mathbb{R}} \\psi(x) f_X(x) dx \\\\  & =  \\int_{\\mathbb{R}} \\int_{\\mathbb{R}} y  f_{Y\\mid X}(y \\mid x) dy f_X(x) dx \\\\ & =   \\int_{\\mathbb{R}^2} y f(x,y) dx dy = \\mathbb{E}[Y]\n\\end{align*}\\]\n\nDans l’exemple précédent on a \\(\\mathbb{E}[Y \\mid X] = \\frac{X}{2}\\) et donc\n\\[\\mathbb{E}[Y]= \\mathbb{E}[\\mathbb{E}[Y \\mid X]] = \\frac{\\mathbb{E}[X]}{2} = \\frac{1}{4}.\\]\n\nOn peut appliquer Fubini à la troisième ligne ci-dessous car \\(\\mathbb{E}[|Y g(X)|]&lt;\\infty\\), et se servir du fait que \\(\\forall (x,y) \\in \\mathbb{R}^2, \\ f_X(x) f_{Y \\mid X}(y \\mid x) = f(x,y)\\)\n\n\\[\\begin{align*}\n\\mathbb{E}[\\mathbb{E}[Y \\mid X] g(X)] & =  \\mathbb{E}[\\psi(X) g(X)] = \\int_{\\mathbb{R}} \\psi(x) g(x) f_X(x) dx \\\\ & =  \\int_{\\mathbb{R}} \\int_{\\mathbb{R}} y  f_{Y\\mid X}(y \\mid x) dy g(x) f_X(x) dx \\\\ & =  \\int_{\\mathbb{R}^2} y g(x) f(x,y) dx dy = \\mathbb{E}[Y g(X)]\n\\end{align*}\\]\nPour \\(B \\in \\mathcal{B}(\\mathbb{R})\\), quitte à considérer la fonction \\(\\hat{g} = g \\mathbb{I}_B\\), on déduit\n\\[\\mathbb{E}[Y g(X) \\mathbb{I}_B(X)] = \\mathbb{E}[\\psi(X) g(X) \\mathbb{I}_B]\\]\nde sorte que\n\\[\\mathbb{E}[Y g(X) \\mid X] = g(X) \\mathbb{E}[Y \\mid X] = g(X) \\psi(X)\\]\n\n\n\nExercice 3 (Partiel passé)  \n\n\n\n\n\n\n\nNotePartiel passé\n\n\n\n\n\n\nSoient \\(0 \\le r \\le p \\le 1\\) tels que \\(1-2p+r \\ge 0\\).\nSoient \\(X_1, X_2\\) tels que\n\\[\\begin{eqnarray*}\n&&  \\mathbb{P}(X_1=1, X_2=1)=r, \\quad   \\mathbb{P}(X_1=0, X_2=1)=p-r, \\\\\n&& \\mathbb{P}(X_1=1, X_2=0)=p-r, \\quad  \\mathbb{P}(X_1=0, X_2=0)=1-2p+r.\n\\end{eqnarray*}\\]\n\nQuelle est la loi de \\(X_1\\)? celle de \\(X_2\\)?\nCalculer \\(Y = \\mathbb{E}[X_1\\mid X_2]\\) et vérifier que\n\\[Y= \\begin{cases} & \\frac{p-r}{1-p} \\mbox{ avec probabilité } 1-p\\\\ & \\frac{r}{p} \\mbox{ avec probabilité } p.\\end{cases}\\]\nRappelons que par définition \\(\\text{Var}[X_1 \\mid X_2] = \\mathbb{E}[X_1^2\\mid X_2] - \\mathbb{E}[X_1\\mid X_2]^2\\). Montrer que\n\\[\\mathrm{Var}[X_1 \\mid X_2] = \\left( \\frac{p-r}{1-p} - \\left(\\frac{p-r}{1-p}\\right)^2\n\\right) \\mathbf{1}_{\\{X_2=0\\}} + \\left( \\frac{r}{p} - \\left(\\frac{r}{p}\\right)^2 \\right)\n\\mathbf{1}_{\\{X_2=1\\}}.\\]\nQue vaut \\(\\mathrm{Var}(\\mathbb{E}[X_1\\mid X_2])\\)? \\(\\mathbb{E}[\\mathrm{Var}[X_1\\mid X_2]]\\)? Vérifier qu’on a bien\n\\[\\mathrm{Var}(X_1) = \\mathrm{Var}(\\mathbb{E}[X_1\\mid X_2]) + \\mathbb{E}[\\mathrm{Var}[X_1\\mid X_2]].\\]\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\\(X_1\\), comme \\(X_2\\), prend ses valeurs dans \\(\\{0,1\\}\\). On a \\(\\mathbb{P}(X_1=1) = r+ p-r\\) de sorte que \\(X_1 \\sim \\mathrm{Ber}(p)\\), et \\(\\mathbb{P}(X_2=1) = r+ p-r\\) de sorte qu’également \\(X_2 \\sim \\mathrm{Ber}(p)\\).\nOn a (cf EF3)\n\\[\\begin{align*}\n\\mathbb{E}[X_1 \\mid X_2] & =  \\frac{\\mathbb{E}[X_1 \\mathbb{I}_{\\{X_2 = 1\\}}]}{ \\mathbb{P}(X_2 =1)} \\mathbb{I}_{\\{X_2=1\\}} +  \\frac{\\mathbb{E}[X_1 \\mathbb{I}_{\\{X_2 = 0\\}}]}{ \\mathbb{P}(X_2 =0)} \\mathbb{I}_{\\{X_2=0\\}}  \\\\ & =  \\frac{r}{p} \\mathbb{I}_{\\{X_2=1\\}} + \\frac{p-r}{1-p} \\mathbb{I}_{\\{X_2=0\\}}\n\\end{align*}\\]\n\nRemarquons que \\(\\mathbb{P}(Y = \\frac{r}{p}) =  \\mathbb{P}(X_2=1) = p,  \\mathbb{P}(Y = \\frac{p-r}{1-p}) =  \\mathbb{P}(X_2=0) = 1-p\\). Autrement dit \\(Y\\) est une variable qui prend deux valeurs, \\(\\frac{r}{p}\\) sur l’événement \\(\\{X_2=1\\}\\) (qui est bien de probabilité \\(p\\)) et \\(\\frac{p-r}{1-p}\\) sur l’événement complémentaire (qui est bien de probabilité \\(1-p\\)).\n\nOn a p.s. \\(X_1^2 = X_1\\) puisque \\(X_1\\) prend ses valeurs dans \\(\\{0,1\\}\\) et donc \\(\\mathbb{E}[X_1^2 \\mid X_2 ]= \\mathbb{E}[X_1 \\mid X_2]\\). Par ailleurs un rapide calcul assure que\n\\[\\mathbb{E}[X_1 \\mid X_2]^2 =  \\frac{r^2}{p^2} \\mathbb{I}_{\\{X_2=1\\}} + \\frac{(p-r)^2}{(1-p)^2} \\mathbb{I}_{\\{X_2=0\\}},\\]\net on obtient donc la formule souhaitée.\n\n\n\n\n\n\n\n\n\nNoteSolution (suite)\n\n\n\n\nD’après la question 2, \\(Y = c + \\left|\\frac{r}{p}- \\frac{p-r}{1-p}\\right| \\xi,\\) où \\(\\xi \\sim \\mathrm{Ber}(p)\\). On obtient donc\n\\[\\begin{align*}\n\\text{Var}(Y) & = \\left(\\frac{r}{p}- \\frac{p-r}{1-p}\\right)^2 p(1-p) & =   \\frac{r^2(1-p)}{p} + \\frac{p (p-r)^2}{1-p} - 2r (p-r) \\\\ & =   \\frac{r^2}{p} - r^2 + \\frac{(p-r)^2}{1-p} - (p-r)^2 - 2r (p-r) \\\\\n& = \\frac{r^2}{p} + \\frac{(p-r)^2}{1-p} -(r+(p-r))^2\n\\end{align*}\\]\nPar ailleurs d’après la question 3,\n\\[\\mathbb{E}[\\mathrm{Var}[X_1 \\mid X_2]]) = \\left( \\frac{p-r} - \\frac{(p-r)^2}{1-p} \\right)  + \\left( r - \\frac{r^2}{p} \\right) = p - \\frac{(p-r)^2}{1-p} - \\frac{r^2}{p}.\\]\nOn a donc\n\\[\\mathrm{Var}(Y) + \\mathbb{E}[\\mathrm{Var}[X_1 \\mid X_2]] =  p - p^2= \\mathrm{Var}[X_1].\\]\n\n\n\n\nExercice 4 (Conditionnement)  \n\nSoit \\((X_n)\\) une suite de v.a. .i.i.d intégrables, et \\(S_n = \\sum_{i=1}^n X_i\\).\n\nQue valent \\(\\mathbb{E}[X_1\\mid X_2], \\mathbb{E}[S_n \\mid X_1], \\mathbb{E}[S_n \\mid S_{n-1}]?\\)\nMontrer que si les paires de variables \\((X,Z)\\), \\((Y,Z)\\) ont la même loi jointe, alors pour toute fonction réelle positive (ou satisfaisant une condition d’intégrabilité), \\(\\mathbb{E}[f(X)\\mid Z] = \\mathbb{E}[f(Y)\\mid Z]\\). En déduire \\(\\mathbb{E}[X_1 \\mid S_n]\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nPuisque \\(X_1\\) est indépendant de \\(X_2\\) on a (EF2)\n\\[\\mathbb{E}[X_1 \\mid X_2] = \\mathbb{E}[X_1].\\] De même pour \\(i \\ge 2\\) \\(\\mathbb{E}[X_i \\mid X_1] = \\mathbb{E}[X_i] = \\mathbb{E}[X_1],\\) tandis que (EF1) : \\(\\mathbb{E}[X_1 \\mid X_1] = X_1\\). On conclut en faisant usage de la linéarité de \\(\\mathbb{E}[\\cdot \\mid \\cdot]\\) que \\[\\mathbb{E}[S_n \\mid X_1] = X_1 + (n-1) \\mathbb{E}[X_1].\\] Par un raisonnement similaire, \\(\\mathbb{E}[S_{n-1} \\mid S_{n-1}] = S_{n-1}\\), tandis que \\(X_n\\) étant indépendant de \\(S_{n-1}\\) on a \\(\\mathbb{E}[X_n \\mid S_{n-1}] = \\mathbb{E}[X_1]\\). En utilisant que \\(S_n = S_{n-1}+X_n\\), la linéarité de \\(\\mathbb{E}[\\cdot \\mid \\cdot]\\) permet de conclure que \\[\\mathbb{E}[S_n \\mid S_{n-1}] = S_{n-1} + \\mathbb{E}[X_1].\\]\n\nSupposons que \\(\\mathbb{P}_{(X,Z)} =  \\mathbb{P}_{(Y,Z)}\\), et que \\(X \\in \\mathbb{L}^1\\), notons \\(T= \\mathbb{E}[X \\mid Z]\\) (qui est, par définition, \\(\\sigma(Z)\\)-mesurable). Soit \\(A \\in \\sigma(Z)\\), de sorte que \\(A = Z^{-1}(B)\\) pour un \\(B\\) dans la tribu dont on a muni l’espace dans lequel \\(Z\\) prend ses valeurs. Alors \\[\\mathbb{E}[Y \\mathbb{I}_A] = \\mathbb{E}[Y \\mathbb{I}_B(Z)] = \\mathbb{E}[X \\mathbb{I}_B(Z)] = \\mathbb{E}[X \\mathbb{I}_A] = \\mathbb{E}[T \\mathbb{I}_A],\\] donc \\(T = \\mathbb{E}[Y \\mid Z]\\).\n\n\n\n\n\n\n\n\n\nNoteSolution (suite)\n\n\n\nOn peut faire le même raisonement avec \\(f(X), f(Y)\\), ou simplement remarquer que \\(\\mathbb{P}_{(X,Z)} =  \\mathbb{P}_{(Y,Z)} \\ \\Rightarrow \\  \\mathbb{P}_{(f(X),Z)} =  \\mathbb{P}_{(f(Y),Z)}\\).\nComme les \\(X_i, 1 \\le i \\le n\\) jouent des rôles parfaitement symétriques dans \\(S_n\\) puisqu’elles sont i.i.d, on a \\(\\mathbb{P}_{(X_i,S_n)} =  \\mathbb{P}_{(X_1, S_n)}\\) pour tout \\(1 \\le i \\le n\\). On déduit de ce qui précède que \\(\\mathbb{E}[X_1 \\mid S_n] = \\mathbb{E}[X_i \\mid S_n], 1 \\le i \\le n\\). Mais alors par linéarité \\[ S_n= \\mathbb{E}[S_n \\mid S_n]  = \\sum_{i=1}^n  \\mathbb{E}[X_i \\mid S_n] = n \\mathbb{E}[X_1 \\mid S_n],\\] et on conclut que \\(\\mathbb{E}[X_1 \\mid S_n] = \\frac{S_n}{n}\\).\n\n\n\nExercice 5 (Examen passé)  \n\n\n\n\n\n\n\nNote(Examen passé)\n\n\n\n\n\n\nSoit \\((X_n, n \\ge 0)\\) une suite de variables i.i.d, avec \\(X_1 \\sim \\text{Ber}(1/2)\\). On pose \\(S_n = \\sum_{i=1}^n (X_i -1/2)\\), \\(\\mathcal{F}_n= \\sigma(X_1,...,X_n)\\).\nCalculer \\(\\mathbb{E}[S_n \\mid \\mathcal{F}_5]\\) en fonction de \\(n\\). Quelle est la loi de cette variable aléatoire?\n\n\n\n\n\n\nNoteSolution\n\n\n\nSi \\(n \\le 5\\), \\(S_5\\) est \\(\\mathcal{F}_5\\) mesurable et donc (EF1) ;\n\\[\\mathbb{E}[S_n \\mid \\mathcal{F}_5] = S_n \\quad \\forall n \\le 5\\]. Comme dans l’exercice précédent, puisque \\(X_i\\) est indépendant de \\(\\mathcal{F}_5\\) pour tout \\(i \\ge 6\\), on a \\[\\mathbb{E}[(X_i-1/2) \\mid \\mathcal{F}_5] = \\mathbb{E}[X_i-1/2]= 0.\\] Donc \\[\\mathbb{E}[S_n \\mid \\mathcal{F}_5] = S_5 \\ \\ \\forall n \\ge 5.\\] Enfin \\(S_k+\\frac{k}{2} \\sim \\mathrm{Bin}(k,1/2)\\).\n\n\n\nExercice 6 (Partiel passé)  \n\nSoient \\(\\{\\mathbf{e}_i, i \\in \\mathbb{N} \\}\\) des variables i.i.d exponentielles de paramètre \\(1\\). Pour \\(n \\in \\mathbb{N}^*\\) on note \\(S_n := \\sum_{i=1}^n \\mathbf{e}_i\\).\n\nOn note \\(f_n\\) la fonction de densité de la variable \\(S_n\\). Montrer que pour tout \\(t \\ge 0\\) \\[ f_n(t) = \\frac{t^{n-1}}{(n-1)!} \\exp(-t).\\]\nPour \\(t &gt;0, n \\in \\mathbb{N}^*\\), que vaut \\(\\mathbb{P}(S_n \\le t)\\)?\nOn fixe \\(t&gt;0\\) et on suppose \\(X_t \\sim \\mathrm{Poisson}(t)\\). Que vaut \\(\\mathbb{P}(X_t \\ge n)\\), pour \\(n \\in \\mathbb{N}^*\\)?\nSur la demi-droite \\(\\mathbb{R}_+\\) on place les points \\(S_1, S_2, S_3,...\\). On note \\(N_t\\) le nombre de ces points qui tombent dans l’intervalle \\([0,t]\\). Exprimer l’événement \\(\\{N_t \\ge n\\} = \\{S_n \\le t\\}\\). Déterminer la loi de \\(N_t\\) à l’aide des questions préc'dentes.\n\nMontrer que, conditionnellement à \\(\\{N_t=1\\}\\), la loi de \\(\\mathbf{e}_1\\) est uniforme sur \\([0,t]\\).\nConditionnellement à \\(\\{N_t=2\\}\\), quelle est la loi du vecteur \\((\\mathbf{e}_1; \\mathbf{e}_2)\\)?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nOn montre l’assertion souhaitée par récurrence sur \\(n \\in \\mathbb{N}^*\\). L’assertion est trivialement vérifiée pour \\(n=1\\) puisqu’on reconna^t en \\(f_1\\) la densité d’une \\(\\exp(1)\\) et donc de \\(S_1 = \\mathbf{e}_1\\).\n\nSoit \\(n \\in \\mathbb{N}^*\\), supposons que \\(S_n\\) a densité \\(f_n\\), comme \\((S_n, \\mathbf{e}_{n+1})\\) sont indépendantes, le couple a densité\n\\[g(s,t) = f_n(s) \\exp(-t) \\mathbb{I}_{s \\ge 0, t \\ge 0}\\]\net donc\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(S_{n+1})] & =  \\mathbb{E}[\\phi(S_n + \\mathbf{e}_{n+1})] \\\\\n& =  \\int_{\\mathbb{R}_+^2} \\phi(s+t)  \\frac{s^{n-1}}{(n-1)!} \\exp(-s-t) ds dt\n\\end{align*}\\]\nAvec \\((u,v)=(s+t,t)\\) on a un \\(\\mathcal{C}^1\\)-difféomorphisme de \\(\\mathbb{R}_+^2\\) dans \\(\\{(u,v) \\in \\mathbb{R}_+^2 : v \\le u\\}\\), de jacobien \\(1\\), et donc par changement de variables, on obtient comme souhaité :\n\\[\\mathbb{E}[\\phi(S_{n+1}] = \\int_{\\mathbb{R}_+} du \\phi(u) \\exp(-u) \\left(\\int_0^u \\frac{(u-v)^{n-1}}{(n-1)!} dv\\right) = \\int_{\\mathbb{R}_+} \\phi(u) f_{n+1}(u) du\\]\nRemarque : Avec des exponentielles indépendantes de paramètre commun \\(\\lambda\\), on obtient la densité d’une \\(\\Gamma(n, \\lambda)\\) pour la somme, ici on est dans le cas \\(\\lambda=1\\).\n\n\n\n\n\n\n\n\nNoteSolution (suite)\n\n\n\n\nOn a\n\\[\\begin{align*}\n\\mathbb{P}(S_n \\ge t) & = \\int_{t}^{\\infty} f_n(u) du\n\\end{align*}\\]\nCette intégrale se calcule, en fonction de \\(n, t\\), au moyen d’intégrations par parties successives :\n\\[\\int_t^{\\infty} f_n(u) du = \\left[ \\frac{u^{n-1}}{(n-1)!} \\right]_t^{\\infty} + \\int_t^{\\infty} f_{n-1}(u) du.\\]\nComme \\(\\int_t^{\\infty} f_1(u) du = \\exp(-t)\\), une récurrence immédiate fournit donc que\n\\[\\int_t^{\\infty} f_n(u) du = \\exp(-t) \\sum_{k=0}^{n-1} \\frac{t^{n-1}}{(n-1)!}.\\]\nSoit \\(n \\in \\mathbb{N}^*\\), on a\n\\[\\mathbb{P}(X_t  \\ge n) = \\exp(-t) \\sum_{k \\ge n} \\frac{t^k}{k!}\\]\net on remarque d’après la question précédente que ceci vaut précisément \\(1- \\mathbb{P}(S_n \\ge t) =  \\mathbb{P}(S_n \\le t)\\) (pour la dernière égalité on a utilisé que \\(S_n\\) possède une densité pour assurer que \\(\\mathbb{P}(S_n=t) =0\\)).\n\n\n\n\n\n\n\n\n\nNoteSolution (suite)\n\n\n\n\nPar définition \\(N_t \\ge n\\) ssi au moins \\(n\\) points parmi \\(\\{S_1,S_2,\\dots,S_n, \\dots\\}\\) tombent dans l’intervalle \\([0,t]\\). Comme \\((S_k, k \\ge 0)\\) est p.s. croissante ceci se produit (p.s.) lorsque \\(S_n \\le t\\) et on on déduit que\n\\[\\{N_t \\ge n\\} = \\{S_n \\le t\\}\\]\nLa variable \\(N_t\\) est à valeurs dans \\(\\mathbb{N}\\), et on a pour tout \\(n \\in \\mathbb{N}\\) (cf la question précédente pour \\(n\\in \\mathbb{N}^*\\), on a ajouté la cas trivial \\(n=0\\)),\n\\[\\mathbb{P}(N_t \\ge n) =  \\mathbb{P}(X_t \\ge n). \\]\nMais ces valeurs caractérisent la fonction de répartition de \\(N_t\\), et donc la loi de \\(N_t\\), et on conclut que \\(N_t \\sim \\mathrm{Poisson}(t)\\).\n\n\n\n\n\n\n\n\n\nNoteSolution (suite)\n\n\n\n\nOn a \\(\\{N_t=1\\} = \\{\\mathbf{e}_1 \\le t, \\mathbf{e}_2 &gt; t- \\mathbf{e}_1\\}\\).\nPar ailleurs, \\((\\mathbf{e}_1, \\mathbf{e}_2)\\) sont indépendantes et possèdent donc la densité jointe\n\\[f_{(\\mathbf{e}_1,\\mathbf{e}_2}(u,v) = \\exp(-u)\\exp(-v) \\mathbb{I}_{\\{u \\ge 0\\}} \\mathbb{I}_{\\{v \\ge 0\\}}\\]\nPour \\(\\phi : \\mathbb{R} \\to \\mathbb{R}_+\\) borélienne, on en déduit que\n\\[\\begin{align*}\n  \\mathbb{E}[\\phi(\\mathbf{e}_1) \\mid N_t=1]  & =  \\frac{\\mathbb{E}[\\phi(\\mathbf{e}_1) \\mathbb{I}_{\\{N_t = 1\\}}]}{ \\mathbb{P}(N_t = 1)} \\\\ & = \\frac{\\mathbb{E}[\\phi(\\mathbf{e}_1) \\mathbb{I}_{\\{\\mathbf{e}_1 \\le t, \\mathbf{e}_2 &gt; t-\\mathbf{e}_1\\}}]}{t \\exp(-t)}\n  \\\\ & =  \\frac{\\exp(t)}{t} \\int_{\\mathbb{R}} \\int_{\\mathbb{R}} \\phi(u) \\exp(-u)\\exp(-v) \\mathbb{I}_{\\{0 \\le u \\le t\\}} \\mathbb{I}_{\\{0\\le t-u &lt; v\\}} du dv\n  \\\\ & =  \\frac{\\exp(t)}{t} \\int_{\\mathbb{R}} du \\phi(u) \\exp(-u) \\mathbb{I}_{[0,t]}(u) \\int_{t-u}^{\\infty} \\exp(-v) dv\n  \\\\ & =  \\frac{\\exp(t)}{t} \\int_{\\mathbb{R}} du \\phi(u) \\exp(-u) \\mathbb{I}_{[0,t]}(u) \\exp(u-t) \\\\\n& =  \\int_{\\mathbb{R}}\\phi(u) \\frac{\\mathbb{I}_{[0,t]}(u)}{t} du,\n\\end{align*}\\]\noù on a utilisé Fubini-Tonelli à la troisième ligne ci-dessus.\nOn conclut, gr^ace au théorème de caractérisation habituel, que la loi conditionnelle de \\(\\mathbf{e}_1\\) sachant \\(\\{N_t=1\\}\\) est \\(\\mathrm{Unif}[0,t]\\),\n\nOn effectue un raisonnement similaire à celui de la question qui précède.\n\nOn a\n\\[\\{N_t=2\\} = \\{S_1 \\le t, \\mathbf{e}_3 &gt; t- S_1\\}= \\left\\{\\mathbf{e}_1 \\le t, \\mathbf{e}_2 \\le t-\\mathbf{e}_1, \\mathbf{e}_3 &gt; t - (\\mathbf{e}_1+\\mathbf{e}_2)\\right\\}.\\]\nPar ailleurs, \\((\\mathbf{e}_1, \\mathbf{e}_2,\\mathbf{e}_3)\\) sont indépendantes et possèdent donc la densité jointe\n\\[f_{(\\mathbf{e}_1,\\mathbf{e}_2,\\mathbf{e}_3}(u,v,w) = \\exp(-u)\\exp(-v)\\exp(-w) \\mathbb{I}_{\\{u \\ge 0\\}} \\mathbb{I}_{\\{v \\ge 0\\}}\\mathbb{I}_{\\{w \\ge 0\\}}.\\]\nPour \\(\\phi : \\mathbb{R}^2 \\to \\mathbb{R}_+\\) borélienne, on en déduit que\n\\[\\begin{align*}\n   \\mathbb{E}[\\phi(\\mathbf{e}_1,\\mathbf{e}_2) \\mid N_t=2]\n   & =  \\frac{\\mathbb{E}[\\phi(\\mathbf{e}_1, \\mathbf{e}_2) \\mathbb{I}_{\\{N_t = 2\\}}]}{ \\mathbb{P}(N_t = 2)} \\\\\n   & = \\frac{\\mathbb{E}[\\phi(\\mathbf{e}_1, \\mathbf{e}_2) \\mathbb{I}_{\\{\\mathbf{e}_1 \\le t, \\mathbf{e}_2 \\le t-\\mathbf{e}_1, \\mathbf{e}_3 &gt; t - (\\mathbf{e}_1+\\mathbf{e}_2)\\}}]}{\\frac{t^2}{2} \\exp(-t)} \\\\\n   & =  \\frac{2\\exp(t)}{t^2} \\int_{\\mathbb{R}^3}  \\phi(u,v) \\exp(-u-v)\\exp(-w) \\mathbb{I}_{\\{0\\le u \\le u+v \\le t\\}} \\mathbb{I}_{\\{w&gt;t-(u+v)\\}} du dv dw \\\\\n   & =  \\frac{2\\exp(t)}{t^2} \\int_{\\mathbb{R}^2} du dv \\phi(u,v) \\exp(-u-v) \\mathbb{I}_{\\{0\\le u \\le u+v \\le t\\}} \\int_{t-(u+v)}^{\\infty} \\exp(-w) dw \\\\\n   & =  \\frac{2\\exp(t)}{t^2} \\int_{\\mathbb{R}} du \\phi(u) \\exp(-u-v) \\mathbb{I}_{\\{0\\le u \\le u+v \\le t\\}} \\exp(u+v-t) = \\int_{\\mathbb{R}}\\phi(u,v) \\frac{2\\mathbb{I}_{\\{0\\le u \\le u+v \\le t\\}}}{t^2} du,\n\\end{align*}\\]\n\net on conclut que la loi conditionnelle de \\((\\mathbf{e}_1, \\mathbf{e}_2)\\) sachant \\(\\{N_t=2\\}\\) a pour densité \\(\\frac{2\\mathbb{I}_{\\{u \\ge 0\\}} \\mathbb{I}_{\\{v \\ge 0\\}} \\mathbb{I}_{\\{u+v \\le t\\}}}{t^2}\\).\nAutrement dit, la loi conditionnelle de \\((\\mathbf{e}_1, \\mathbf{e}_2)\\) sachant \\(\\{N_t=2\\}\\) est uniforme sur le triangle \\(\\{(u,v) \\in [0,t]^2 : u+v \\le t\\}\\).\n\n\n\nExercice 7 (CC2 2023)  \n\nOn considère \\[ X \\sim \\mathcal{N} \\left( \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 2 & 1 & -1 & -1 \\\\ 1 & 2 & -1 & 0 \\\\ -1 & -1 & 3 & -1 \\\\ -1 & 0 & -1 & 5 \\end{pmatrix}\\right).\\]\n\nCalculer \\(\\mathbb{E}[X_3 \\mid X_4]\\), et déterminer la loi conditionnelle de \\(X_3\\) sachant \\(X_4\\).\n\nOn pose \\(A = \\begin{pmatrix} 2  & 1 \\\\ 1 & 2 \\end{pmatrix}\\), \\(B= \\begin{pmatrix} -1 & -1 \\\\ -1 & 0 \\end{pmatrix}\\). Calculer \\(BA^{-1}\\), puis vérifier que \\[B A^{-1} B^T = \\begin{pmatrix} \\frac{2}{3} & \\frac{1}{3} \\vspace{0.1cm} \\\\ \\frac{1}{3} & \\frac{2}{3}\\end{pmatrix}.\\]\nDéterminer \\(\\mathbb{E}\\left[\\begin{pmatrix} X_3 \\\\ X_4 \\end{pmatrix} \\ \\bigg| \\ \\begin{pmatrix} X_1 \\\\ X_2 \\end{pmatrix}\\right]\\). et la loi conditionnelle de \\(\\begin{pmatrix} X_3 \\\\ X_4 \\end{pmatrix}\\) sachant \\(\\begin{pmatrix} X_1 \\\\ X_2 \\end{pmatrix}\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nD’après l’énoncé \\(\\displaystyle{\\begin{pmatrix} X_3 \\\\ X_4 \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix}   3 & -1 \\\\  -1 & 5 \\end{pmatrix}\\right)}\\), et donc d’après la formule du cours, sachant \\(X_4\\), \\(X_3 \\sim \\mathcal{N}\\left(1 -\\frac{X_4}{5},\\frac{14}{5}\\right)\\). En particulier \\(\\mathbb{E}[X_3 \\mid X_4] = 1 - \\frac{X_4}{5}\\).\nOn a \\(A^{-1} = \\begin{pmatrix} \\frac{2}{3} & -\\frac{1}{3} \\\\ -\\frac{1}{3} & \\frac{2}{3} \\end{pmatrix}\\), et donc\n\n\\[B A^{-1} = \\begin{pmatrix} -\\frac{1}{3} & -\\frac{1}{3} \\\\ -\\frac{2}{3} & \\frac{1}{3} \\end{pmatrix},\\] et\n\\[BA^{-1}B^T = \\begin{pmatrix} -\\frac{1}{3} & -\\frac{1}{3} \\\\ -\\frac{2}{3} & \\frac{1}{3} \\end{pmatrix}  \\begin{pmatrix} -1 & -1 \\\\ -1 & 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3} & \\frac{1}{3} \\\\ \\frac{1}{3} & \\frac{2}{3}\\end{pmatrix},\\] comme souhaité.\n\n\n\n\n\n\n\n\nNoteSolution (suite)\n\n\n\n\nD’après le cours, sachant \\(\\begin{pmatrix} X_1 \\\\ X_2 \\end{pmatrix}\\), la loi conditionnelle de \\(\\begin{pmatrix} X_3 \\\\ X_4 \\end{pmatrix}\\) est gaussienne, centrée en \\[\\mathbb{E}\\left[\\begin{pmatrix} X_3 \\\\ X_4 \\end{pmatrix} \\ \\bigg| \\ \\begin{pmatrix} X_1 \\\\ X_2 \\end{pmatrix}\\right] = BA^{-1} \\begin{pmatrix} X_1 +1 \\\\ X_2 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}  = \\begin{pmatrix} \\frac{2}{3} - \\frac{1}{3}X_1 - \\frac{1}{3} X_2 \\\\ - \\frac{2}{3} -\\frac{2}{3}X_1 + \\frac{1}{3}X_2 \\end{pmatrix},\\] et de matrice de covariances \\[ \\begin{pmatrix} 3 & -1 \\\\ -1 & 5 \\end{pmatrix} - BA^{-1}B^T =  \\begin{pmatrix} \\frac{7}{3} & \\frac{-4}{3} \\vspace{0.1cm}\\\\ \\frac{-4}{3} & \\frac{13}{3}\\end{pmatrix}.\\]\n\n\n\n\nExercice 8 (Partiel passé)  \n\nSoit \\((X_1,X_2,X_3) \\sim \\mathcal{N}(\\mu, M)\\) où \\[\\mu = \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}, \\quad M = \\begin{pmatrix} 2 & 1 & 0 \\\\ 1 & 2 & 1 \\\\ 0 & 1 & 2 \\end{pmatrix}.\\]\n\nQuelle est la loi du couple \\((X_1,X_2)\\)?\nDéterminer \\(\\alpha\\) un réel tel que $Y = X_1 + X_2 $ est indépendante de \\(X_1\\). Que vaut \\(\\mathbb{E}[Y]\\)? \\(\\text{Var}(Y)\\)?\nEn déduire \\(\\mathbb{E}[X_2 \\mid X_1]\\). Quelle est la loi conditionnelle de \\(X_2\\) sachant \\(X_1\\)?\nDéterminer un réel \\(\\beta\\) tels que \\(Z=\\beta X_1 + X_3\\) est indépendante de \\(X_1\\). En déduire \\[\\mathbb{E}[X_3 \\mid X_1], \\quad \\mathbb{E}[X_3^2 \\mid X_1].\\]\nCalculer \\(\\mathbb{E}\\left[X_1^2X_2 + X_3^2 X_1 \\mid X_1\\right]\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\\((X_1,X_2)\\) est un vecteur gaussien (comme image d’un vecteur gaussien par une application linéaire, en l’occurrence une projection), et on lit directement sur \\(\\mu, M\\) moyennes et covariances. On a donc \\((X_1,X_2) \\sim \\mathcal{N}(m,A)\\), avec \\(m = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\) et \\(A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2\\end{pmatrix}\\).\nQuel que soit \\(\\alpha \\in \\mathbb{R}\\), \\((X_1,Y)\\) est un vecteur gaussien comme image du vecteur gaussien \\((X_1,X_2)\\) par l’application linéaire de matrice \\(\\begin{pmatrix} 1 & 0 \\\\ \\alpha & 1\\end{pmatrix}\\).\n\nPar théorème caractérisant l’indépendance des coordonnées d’un vecteur gaussien, \\(X_1\\) est indépendant de \\(Y\\) ssi \\(\\mathrm{Cov}(X_1,Y)=0\\). Or \\[\\mathrm{Cov}(X_1,Y)= \\alpha \\mathrm{Var}[X_1] + 1 = 2\\alpha +1,\\] et donc on a l’indépendance souhaitée lorsque \\(\\alpha=-\\frac{1}{2}\\). 1. Puisque \\(-\\frac{1}{2}X_1+X_2\\) est indépendant de \\(X_1\\) on a donc 2. \\[\\begin{align*}\n\\mathbb{E}[X_2 \\mid X_1 ] & =  \\mathbb{E}\\left[\\frac{1}{2}X_1 +\\left(-\\frac{1}{2}X_1+X_2\\right) \\mid X_1\\right] \\\\ & =  \\frac{1}{2}X_1 + \\mathbb{E}\\left[-\\frac{1}{2}X_1 + X_2\\right] = \\frac{1}{2}X_1 -\\frac{1}{2}.\n\\end{align*}\\] Par ailleurs, \\(\\mathrm{Var}\\left(-\\frac{1}{2}X_1+X_2\\right) = \\frac{1}{4} \\mathrm{Var}(X_1) - \\mathrm{Cov}(X_1,X_2) + \\mathrm{Var}(X_2) = \\frac{3}{2}\\) et donc \\(-\\frac{1}{2}X_1+X_2 \\sim \\mathcal{N}\\left(-\\frac{1}{2}, \\frac{3}{2}\\right)\\). L’écriture \\(X_2 = \\frac{1}{2}X_1 + \\left(-\\frac{1}{2}X_1+X_2\\right)\\) permet donc d’affirmer que sachant \\(X_1\\), la loi conditionnelle de \\(X_2\\) est \\(\\mathcal{N}\\left(\\frac{1}{2}X_1-\\frac{1}{2}, \\frac{3}{2} \\right)\\).\n\n\n\n\n\n\n\n\nNoteSolution (suite)\n\n\n\n\nIci \\(\\mathrm{Cov}(X_1,X_3) = 0\\) et donc \\(X_1\\) et \\(X_3\\) sont indépendants, il suffit donc de prendre \\(\\beta=0\\). On trouve donc ici que \\[\\mathbb{E}[X_3 \\mid X_1] = \\mathbb{E}[X_3] = -1\\] et que sachant \\(X_1\\), la loi conditionnelle de \\(X_3\\) reste la loi de \\(X_3\\), i.e. \\(\\mathcal{N}\\left(-1, 2 \\right)\\). Par ailleurs\n\n\\[\\begin{align*}\n\\mathbb{E}[X_3^2 \\mid X_1] & =   \\mathbb{E}[X_3^2] = \\mathbb{E}[X_3]^2 + \\mathrm{Var}[X_3] \\\\\n& =  1 + 2 = 3.\n\\end{align*}\\]\n\nOn a, en utilisant les propriétés de l’espérance conditionnelle et les question précédentes, puis en simplifiant\n\\[\\begin{align*}\n\\mathbb{E}[X_1^2 X_2 + X_3^2 X_1 \\mid X_1] & =  X_1^2 \\mathbb{E}[X_2 \\mid X_1] + X_1 \\mathbb{E}[X_3^2\\mid X_1 ] \\\\\n& =  X_1^2 \\left(\\frac{1}{2}X_1 -\\frac{1}{2}\\right) + 3X_1  \\\\\n& =  \\frac{1}{2}X_1^3 -\\frac{1}{2} X_1^2 + 3X_1\n\\end{align*}\\]\n\n\n\n\nExercice 9 (Examen passé)  \n\nSoit \\((X_1,X_2,X_3) \\sim \\mathcal{N}(\\mu,M)\\), où \\[ \\mu = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\qquad M=  \\begin{pmatrix} 1 & 1/2 & 2 \\\\ 1/2 & 1 & 1 \\\\ 2 & 1 & 3 \\end{pmatrix}.\\] Calculer \\(\\mathbb{E}[X_1+2X_2 \\mid X_3].\\) Quelle est la loi conditionnelle de \\(X_1+2X_2\\) sachant \\(X_3\\)?\n\n\n\n\n\n\nNoteSolution\n\n\n\nComme dans l’exercice précédent on peut commencer par chercher \\(\\alpha\\) tel que \\(Y=\\alpha X_3 + X_1 + 2X_2\\) est indépendant de \\(X_3\\). Bien s^ur, \\((Y,X_3)\\) est un vecteur gaussien puisque c’est l’image de \\((X_1,X_2,X_3)\\) par une application linéaire. Donc on a l’indépendance voulue lorsque \\(\\mathrm{Cov}(Y,X_3) =0\\), i.e. lorsque \\[ 0 = \\alpha \\mathrm{Var}(X_3) + \\mathrm{Cov}(X_1,X_3)+2 \\mathrm{Cov}(X_2,X_3) = 3\\alpha + 2 + 2,\\] et donc il faut prendre \\(\\alpha = -\\frac{4}{3}\\).\nOn a alors \\[\\mathbb{E}[X_1+2X_2 \\mid X_3] = \\frac{4}{3}X_3 + \\mathbb{E}[-\\frac{4}{3}X_3 + X_1 + 2X_2] = \\frac{4}{3}X_3+2.\\]\nPar ailleurs,\n\\[\\begin{align*}\n\\text{Var}(Y) & =  \\frac{16}{9} \\mathrm{Var}(X_3) + \\mathrm{Var}(X_1) + 4 \\mathrm{Var}(X_2) - \\frac{8}{3} \\mathrm{Cov}(X_3,X_1) - \\frac{16}{3} \\mathrm{Cov}(X_2,X_3) + 4 \\mathrm{Cov}(X_1,X_2)  \\\\\n& =  \\frac{16}{3} + 1 + 4 - \\frac{16}{3} -\\frac{16}{3} + 2 = \\frac{5}{3}\n\\end{align*}\\] et on déduit que sachant \\(X_3\\), la loi conditionnelle de \\(X_1+2X_2\\) est \\(\\mathcal{N}\\left( \\frac{4}{3}X_3+2, \\frac{5}{3} \\right)\\).\n\n\n\n\n\n\n\n\nNoteSolution (suite)\n\n\n\nAlternativement, on peut utiliser les formules du cours. D’abord, avec \\(K=  \\begin{pmatrix} 1 & 2 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\\)\n\\[\n\\begin{pmatrix} X_1+2X_2 \\\\ X_3 \\end{pmatrix} = K \\begin{pmatrix} X_1 \\\\ X_2 \\\\X_3 \\end{pmatrix} \\sim \\mathcal{N}\\left( K \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, K M K^T \\right) \\sim \\mathcal{N} \\left( \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 7 & 4 \\\\ 4 & 3 \\end{pmatrix}\\right).\n\\]\nOn peut alors appliquer la méthode précédente à ce vecteur, ou la formule du cours pour le conditionnement avec \\(\\theta = X_1+2X_2, \\xi=X_3\\), \\(\\mu_{\\theta} = 2, \\mu_{\\xi} = 0, \\ M_{\\theta \\xi} = M_{\\xi \\theta} = 4, M_{\\xi \\xi} = 3, M_{\\theta \\theta}=7\\), pour obtenir\n\\[\\mathbb{E}[X_1+2X_2 \\mid X_3] = \\mu_{\\theta} + M_{\\theta \\xi} M_{\\xi \\xi}^{-1} (\\xi-\\mu_{\\xi}) = 2 + \\frac{4}{3} X_3,\\]\net\n\\[\\mathrm{Var}[X_1+2X_2 \\mid X_3] = M_{\\theta \\theta} - M_{\\theta \\xi} M_{\\xi \\xi}^{-1} M_{\\xi \\theta} = 7 - \\frac{16}{3} = \\frac{5}{3}.\\]\n\n\n\nExercice 10 (CC2 2023)  \n\nOn suppose dans cet exercice que \\((X,Y)\\) est un couple de variables aléatoires tel que pour toute \\(\\phi : \\mathbb{R}^2\\to \\mathbb{R}_+\\) borélienne, \\[\\mathbb{E}[\\phi(X,Y)] = \\sum_{n \\ge 1} \\frac{2}{3^{n}\\sqrt{2\\pi n}}  \\int_{\\mathbb{R}} \\phi(n,y) \\exp\\left(-\\frac{y^2}{2n}\\right) dy.\\]\n\nMontrer que \\(X \\sim \\mathrm{Geom}(2/3)\\).\nVérifier que pour une fonction \\(f : \\mathbb{R} \\to \\mathbb{C}\\) telle que \\(f(Y) \\in \\mathbb{L}^1\\), on a\n\\[\\mathbb{E}[f(Y) \\mid X] = \\sum_{n \\ge 1} \\left(\\int_{\\mathbb{R}} \\frac{1}{\\sqrt{ 2\\pi n}} f(y) \\exp\\left(-\\frac{y^2}{2n} \\right) dy \\right) \\mathbb{I}_{\\{X=n\\}}\\] %1. En déduire que pour tout \\(k \\in \\mathbb{N}\\), %\\[\\mathbb{E}[Y^k \\mid X] = \\frac{k!}{X^{2k}}\\]\nCalculer \\(\\mathbb{E}[\\exp(itY) \\mid X]\\), \\(t \\in \\mathbb{R}\\), quelle est la loi conditionnelle de \\(Y\\) sachant \\(X\\) ?\nDéduire que si \\(t\\in \\mathbb{R}\\) \\[\\mathbb{E}[\\exp(itY)] = \\frac{2 \\exp\\left(-\\frac{t^2}{2}\\right)}{3-\\exp\\left(-\\frac{t^2}{2}\\right)}.\\]\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nNotons que pour tout \\(n \\ge 1\\), \\(\\int_{\\mathbb{R}} \\frac{1}{\\sqrt{2\\pi n}} \\exp\\left(-\\frac{y^2}{2n}\\right) dy =1\\) (on intègre sur \\(\\mathbb{R}\\) la densité d’une variable de loi \\(\\mathcal{N}(0,n)\\)). On en déduit (quitte à considérer \\(\\phi(X,Y) = \\mathbb{I}_{\\{X= n\\}}\\))\n\\[  \\mathbb{P}(X=n) = \\mathbb{E}[\\mathbb{I}_{\\{X=n\\}}] =  \\frac{2}{3^{n}}  \\int_{\\mathbb{R}} \\frac{1}{\\sqrt{2\\pi n}} \\exp\\left(-\\frac{y^2}{2n}\\right) dy = \\frac{2}{3^n},\\] et il découle que \\(X \\sim \\mathrm{Geom}(2/3)\\).\n\nLes événements \\(\\{\\{X=n\\}, n \\ge 1\\}\\) forment une partition de \\(\\Omega\\), on est dans le cadre de EF3 pour \\(\\mathrm{Re}(f), \\mathrm{Im}(f)\\) et quitte à utiliser la linéarité de l’espérance, on obtient\n\\[\\mathbb{E}[f(Y) \\mid X] = \\sum_{n \\ge 1} \\frac{\\mathbb{E}[f(Y) \\mathbb{I}_{\\{X=n\\}}]}{ \\mathbb{P}(X=n)} \\mathbb{I}_{\\{X=n\\}}.\\] Quitte à considérer \\(\\phi(X,Y) = \\mathrm{Re(f(Y))} \\mathbb{I}_{\\{X=n\\}}\\) puis \\(\\phi_2(X,Y) = \\mathrm{Im(f(Y))} \\mathbb{I}_{\\{X=n\\}}\\) et utiliser la linéarité de l’espérance, on obtient\n\\[\\mathbb{E}[f(Y) \\mathbb{I}_{\\{X=n\\}}] = \\frac{2}{3^{n} \\sqrt{2 \\pi n}}  \\int_{\\mathbb{R}}  f(y) \\exp\\left(-\\frac{y^2}{2n}\\right)  dy,\\] et donc \\[ \\frac{\\mathbb{E}[f(Y) \\mathbb{I}_{\\{X=n\\}}]}{ \\mathbb{P}(X=n)} = \\int_{\\mathbb{R}} \\frac{1}{\\sqrt{2 \\pi n}} f(y) \\exp\\left(-\\frac{y^2}{2n}\\right) dy,\\] ce qui conduit à la formule souhaitée.\n\n\n\n\n\n\n\n\n\nNoteSolution (suite)\n\n\n\n\nPuisque la fonction caractéristique d’une variable suivant la loi \\(\\mathcal{N}(0,n)\\) est \\(t \\to \\exp\\left( -\\frac{t^2 n}{2} \\right)\\) on a \\[  \\int_{\\mathbb{R}} \\exp(ity) \\frac{1}{\\sqrt{2 \\pi n}}  \\exp\\left(-\\frac{y^2}{2n}\\right) dy = \\exp\\left(-\\frac{t^2 n}{2} \\right)\\] de sorte que \\[\\mathbb{E}[\\exp(itY) \\mid X] = \\exp\\left(-\\frac{t^2 X}{2} \\right).\\] La loi conditionnelle de \\(Y\\) sachant \\(X\\) est donc \\(\\mathcal{N}(0,X)\\).\nOn a grâce à la propriété de tour et la question précédente\n\\[\\begin{align*}\n\\mathbb{E}[\\exp(itY)] & =  \\mathbb{E}[\\mathbb{E}[\\exp(itY) \\mid X]] = \\mathbb{E}\\left[\\exp\\left(-\\frac{t^2 X}{2}\\right)\\right] \\\\\n& =  \\sum_{n \\ge 1} \\frac{2}{3^n} \\exp\\left(-\\frac{t^2 n}{2}\\right) \\\\\n& =  \\frac{2 \\exp\\left(-\\frac{t^2}{2}\\right)}{3} \\sum_{n \\ge 1}  \\left(\\frac{\\exp\\left(-\\frac{t^2}{2}\\right)}{3}\\right)^{n-1}\n\\\\ & =  \\frac{2 \\exp\\left(-\\frac{t^2}{2}\\right)}{3} \\sum_{n' \\ge 0}  \\left(\\frac{\\exp\\left(-\\frac{t^2}{2}\\right)}{3}\\right)^{n'}\n\\\\ & =   \\frac{2 \\exp\\left(-\\frac{t^2}{2}\\right)}{3} \\frac{1}{1-\\frac{\\exp\\left(-\\frac{t^2}{2}\\right)}{3}}\n\\\\ & =  \\frac{2 \\exp\\left(-\\frac{t^2}{2}\\right)}{3-\\exp\\left(-\\frac{t^2}{2}\\right)}\n\\end{align*}\\]\ncomme souhaité.\n\n\n\n\nExercice 11 (Partiel passé)  \n\n\nPartie I\nOn considère le couple \\((X,Z)\\) de densité jointe \\[ f(x,z) := (z-x)\\exp(-z) \\mathbf{1}_{\\{z \\ge x \\ge 0\\}}.\\]\n\nCalculer la loi de \\(X\\), puis celle de \\(Z\\).\nEn déduire que\n\\[f_{X \\mid Z}(x \\mid z) = \\frac{2(z-x)}{z^2} \\mathbf{1}_{\\{0 \\le x \\le z, z &gt;0\\}}.\\]\nCalculer \\(\\mathbb{E}[X \\mid Z]\\), puis \\(\\mathrm{Var}[X\\mid Z]\\).\nCalculer \\(f_{Z \\mid X}(z \\mid x)\\), puis démontrer que \\(\\mathbb{E}[Z \\mid X] = X + 2\\).\nQuelle est la loi du couple \\((X, Z-X)\\)? En déduire la loi de \\(Z-X\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nLa variable de \\(X\\) possède la densité \\(f_X\\) avec pour \\(x \\in \\mathbb{R}\\),\n\\[\\begin{align*}\nf_X(x) & =  \\int_{\\mathbb{R}} dz f(x,z) = \\mathbb{I}_{\\{x \\ge 0\\}} \\int_{x}^{\\infty} (z-x) \\exp(-z) dz\n\\\\ & =  \\mathbb{I}_{\\{x \\ge 0\\}} \\int_0^{\\infty} y \\exp(-(y+x)) dy \\\\   \n& =  \\mathbb{I}_{\\{ x \\ge 0\\}} \\exp(-x)\n\\end{align*}\\]\n\nen utilisant le changement de variables \\(y = z-x\\) et le fait que \\(\\int_{0}^{\\infty} y \\exp(y)\\) vaut \\(1\\) (par exemple en reconnaissant l’espérance d’une exponentielle standard, ou alors en effectuant une i.p.p). On conclut que \\(X \\sim \\exp(1)\\).\nLa variable \\(Z\\) possède la densité \\(f_Z\\) avec pour \\(z \\in \\mathbb{R}\\),\n\\[\\begin{align*}\nf_Z(z)\n    & =  \\int_{\\mathbb{R}} dx f(x,z) \\\\\n    & = \\mathbb{I}_{\\{z \\ge 0\\}} \\exp(-z) \\int_{0}^{z} (z-x)  dz \\\\\n    & =  \\mathbb{I}_{\\{z \\ge 0\\}} \\exp(-z) \\frac{z^2}{2}\n\\end{align*}\\]\net on conclut que \\(Z \\sim \\Gamma(2,1)\\).\n\n\n\n\n\n\n\n\nNoteSolution (suite)\n\n\n\n\nOn a donc\n\\[f_{X \\mid Z}(x \\mid z) = \\begin{cases} \\frac{f(x,z)}{f_Z(z)} & \\mbox{si } z &gt; 0 \\\\ 0 & \\mbox{sinon}\\end{cases} = \\frac{2(z-x)}{z^2} \\mathbf{1}_{\\{0 \\le x \\le z, z &gt;0\\}}.\\]\nOn déduit pour \\(z &gt;0\\),\n\\[\\begin{align*}\nPhi(z)\n& :=  \\int_{\\mathbb{R}} x f_{X \\mid Z}(x \\mid z) dx \\\\\n& =  \\int_{0}^{x} \\frac{2 x(z-x)}{z^2} dx = \\frac{z^3 - \\frac{2}{3}z^3}{z^2} = \\frac{z}{3}\n\\end{align*}\\]\net on conclut d’après le résultat EF4 que \\(\\mathbb{E}[X \\mid Z] = \\Phi_1(Z) = \\frac{Z}{3}\\).\nDe plus pour \\(z&gt;0\\),\n\\[\\begin{align*}\n\\Phi_2(z)\n& : =  \\int_{\\mathbb{R}} x^2 f_{X \\mid Z}(x \\mid z) dx \\\\\n& =  \\int_0^x \\frac{2x^2(z-x)}{z^2} dx = \\frac{2}{3} z^2 - \\frac{1}{2} z^2 = \\frac{z^2}{6}\n\\end{align*}\\]\nde sorte, toujours par le même résultat, que \\(\\mathbb{E}[X^2 \\mid Z] = \\Phi_2(Z) = \\frac{Z^2}{6}\\).\nOn déduit que\n\\[\\mathrm{Var}[X \\mid Z] = \\mathbb{E}[X^2 \\mid Z] - (\\mathbb{E}[X \\mid Z])^2 = \\frac{Z^2}{6} - \\frac{Z^2}{9} = \\frac{Z^2}{18}.\\]\n\n\n\n\n\n\n\n\n\nNoteSolution (suite)\n\n\n\n\nOn a\n\n\\[\\begin{align*}\nf_{Z\\mid X}(z \\mid x) = \\begin{cases} \\frac{f(x,z)}{f_X(x)} & \\mbox{si } x &gt; 0 \\\\ 0 & \\mbox{sinon}\\end{cases} = (z-x) \\exp(-(z-x)) \\mathbf{1}_{\\{0 &lt; x \\le z\\}}.\n\\end{align*}\\]\nOn déduit que pour \\(x &gt;0\\),\n\\[\\begin{align*}\n\\Psi(x) & :=  \\int_{\\mathbb{R}} z f_{Z \\mid X}(z \\mid x) dz \\\\\n& =  \\int_{x}^{\\infty} z (z-x) \\exp(-(z-x)) dx \\\\\n& =   \\int_{0}^{\\infty} (x+u) u \\exp(-u) du \\\\\n& =  \\left[ -(x+u)u \\exp(-u) \\right]_{0}^{\\infty} + \\int_0^{\\infty} (x+2u) \\exp(-u) du \\\\\n& =  \\left[ -(x+2u) \\exp(-u)\\right]_0^{\\infty} + \\int_0^{\\infty} 2 \\exp(-u) du = x + 2,\n\\end{align*}\\]\net on obtient, toujours par EF4, comme souhaité, que \\(\\mathbb{E}[Z \\mid X] = \\Psi(X)=X+2\\).\n\nSoit \\(\\phi : \\mathbb{R}^2 \\to \\mathbb{R}_+\\) borélienne, par le changement de variables \\((x,z) \\to (x,z-x)\\) de \\(\\{(z,x) : 0 \\le x \\le z\\}\\) dans \\(\\mathbb{R}_+^2\\) on obtient\n\n\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(X, Z-X)]\n& =  \\int_{\\mathbb{R}_+^2} \\phi(x, z-x) (z-x) \\exp(-z) \\mathbb{I}_{\\{z \\ge x\\}} dx dz \\\\\n& =  \\int_{\\mathbb{R}_+^2} \\phi(u,v) \\exp(-u) v \\exp(-v) du dv\n\\end{align*}\\]\net on obtient que \\(X \\sim \\exp(1)\\) est indépendante de \\(Z-X \\sim \\mathrm{Gamma}(2,1)\\).\n\n\n\n\nPartie II\n\nSoit \\(z &gt;0\\). On suppose que \\(U_1^z \\sim \\mathrm{Unif}[0,z]\\), \\(U_2^z \\sim \\mathrm{Unif}[0,z]\\) et que \\(U_1^z\\) est indépendante de \\(U_2^z\\). Calculer la densité de \\(\\min(U_1^z, U_2^z)\\).\nOn suppose à présent que conditionnellement à \\(Z\\), \\(U_1^Z \\sim \\mathrm{Unif}[0,Z]\\), \\(U_2^Z \\sim \\mathrm{Unif}[0,Z]\\) et que \\(U_1^Z\\) est (toujours conditionnellement à \\(Z\\)) indépendante de \\(U_2^Z\\). Montrer que, conditionnellement à \\(Z\\), \\(\\mathrm{min}(U_1^Z, U_2^Z)\\) a la même loi que X.\nSoient \\(X_1,X_2,X_3\\) trois variables indépendantes, toutes trois distribuées suivant la distribution exponentielle de paramètre \\(1\\). On note \\(S= X_1+X_2+X_3\\). Déterminer la loi de \\((X_1,S)\\). Que vaut \\(\\mathbb{E}[X_1\\mid S]\\)? \\(\\mathbb{E}[S \\mid X_1]\\)? Montrer finalement que conditionnellement à \\(S\\), le couple \\((X_1,X_1+X_2)\\) a la même loi que \\(\\left(\\mathrm{min}(U_1^S, U_2^S), \\mathrm{max}(U_1^S, U_2^S)\\right)\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nLa fonction de répartition \\(F\\) de \\(U_1^z\\) (et donc de \\(U_2^z\\) puisqu’elle a la même loi est donnée entre \\(0\\) et \\(z\\) par \\(F(x) = \\frac{x}{z}, 0 \\le x \\le z\\). On déduit que pour \\(0 \\le x \\le z\\), en utilisant l’indépendance de \\(U_1^z, U_2^z\\) à la deuxième ligne ci-dessous,\n\\[\\begin{align*}\n\\mathbb{P}(\\min(U_1^z, U_2^z) &gt; x)\n& =   \\mathbb{P}(U_1^z &gt; x)  \\mathbb{P}(U_2^z &gt; x) \\\\\n& =  (1 - F(x))^2 = \\left(1-\\frac{x}{z}\\right)^2\n\\end{align*}\\]\net on déduit que la densité de \\(\\min(U_1^z,U_2^z)\\) est donnée par\n\\[g_z(x) = \\frac{2}{z} \\left(1-\\frac{x}{z}\\right)\\mathbb{I}_{[0,z]}(x), \\ x \\in \\mathbb{R}.\\]\nD’après la question précédente, conditionnellement à \\(Z\\), \\(\\min(U_1^Z,U_2^Z)\\) possède la densité conditionnelle \\(g_Z\\). Par ailleurs, la densité conditionnelle de \\(X\\) sachant \\(Z\\) est \\(f_{X|Z}\\) calculée à la question I.2 est p.p. égale à \\(g_Z\\). On conclut que conditionnellement à \\(Z\\), les variables \\(X\\) et \\(\\min(U_1^Z,U_2^Z)\\) ont la même loi.\n\n\n\n\n\n\n\n\n\nNoteSolution (suite)\n\n\n\n\nTout d’abord, par indépendance des trois variables exponentielles, \\((X_1,X_2,X_3)\\) a densité donnée par\n\\[f(x_1,x_2,x_3) = \\exp(-x_1-x_2-x_3) \\mathbb{I}_{\\{x_1\\ge 0, x_2 \\ge 0, x_3 \\ge 0\\}}, \\quad (x_1,x_2,x_3) \\in \\mathbb{R}^3.\\]\nOn en déduit pour \\(\\phi : \\mathbb{R}^2 \\to \\mathbb{R}_+\\) borélienne, en utilisant à la deuxiième ligne le changement de variables \\((x_1,x_2,x_3) \\to (u=x_1,v=x_1+x_2,w=x_1+x_2+x_3)\\) de \\(\\mathbb{R}_+^3\\) dans \\(\\{(u,v,w) \\in \\mathbb{R}_+^3 : u \\le v \\le w\\}\\)\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(X_1,S)]\n& =  \\int_{\\mathbb{R}_+^3} \\phi(x_1,x_1+x_2+x_3) \\exp(-x_1-x_2-x_3) dx_1 dx_2 dx_3 \\\\\n& =  \\int_{\\mathbb{R}_+^3 : u \\le v \\le w} \\phi(u,w) \\exp(-w) du dv dw  \\\\\n& =   \\int_{\\mathbb{R}_+^2 : u \\le w} \\phi(u,w) (w-u) \\exp(-w)\n\\end{align*}\\]\net on déduit que \\((X_1,S)\\) a même loi que \\((X,Z)\\).\n\n\n\n\n\n\n\n\n\nNoteSolution (suite)\n\n\n\nPuisque les deux vecteurs ont même loi jointe, on peut utiliser la partie I pour déduire que\n\\[\\mathbb{E}[X_1 \\mid S] = \\frac{S}{3}, \\quad \\mathbb{E}[S \\mid X_1] = X_1 +2.\\]\nOn peut aussi prouver ces résultats directement (cf exercice 5)\nD’après le calcul en début de question, la densité du triplet \\((X_1.X_1+X_2.S)\\) est donnée par\n\\[h(u,v,w) = \\mathbb{I}_{\\{0&lt; u \\le v \\le w\\}} \\exp(-w) \\quad (u,v,w) \\in \\mathbb{R}^3.\\]\nQuitte à noter \\(T=X_1.V=X_1+X_2\\) on a donc\n\\[h_{(T,V) \\mid S} ((t,v) \\mid s) = \\frac{2}{s^2} \\mathbb{I}_{0&lt; t &lt; v &lt; s}.\\]\nPar ailleurs, la densité conditionnelle de \\((U_1^S,U_2^S)\\) sachant \\(S\\) est donnée par\n\\[\\frac{1}{w^2}\\mathbb{I}_{[0,w]^2}(u,v), (u,v) \\in \\mathbb{R}^2.\\]\nCeci peut être récrit\n\\[\\frac{1}{w^2} \\mathbb{I}_{\\{0&lt;u&lt;v&lt;w\\}} + \\mathbb{I}_{\\{0&lt;v&lt;u&lt;w\\}}, (u,v) \\in \\mathbb{R}^2,\\]\nla première partie correspondant aux cas où la première uniforme réalise le \\(\\min\\) des deux, et la deuxième partie aux cas où elle réalise le \\(\\max\\).\nComme \\((U_1^S,U_2^S)\\) jouent, conditionnellement à \\(S\\), des rôles parfaitement symétriques, on déduit que \\(h_{(U,V) \\mid S}\\) est la densité de la statistique d’ordre de ces deux variables, ce qui est le résultat souhaité.\n\n\n\nExercice 12 (Partiel passé)  \n\nPour \\((x,y) \\in \\mathbb{R}^2\\) on définit\n\\[f(x,y) := \\frac{4y}{x^3} \\mathbf{1}_{\\{0&lt;x&lt;1, 0&lt;y &lt;x^2\\}}.\\]\nVérifier que \\(f\\) est bien une densité de probabilité, puis calculer les densités marginales \\(f_X\\), \\(f_Y\\).\n\n\n\n\n\n\nNoteSolution\n\n\n\nIl est clair que \\(f\\) est à valeurs dans \\(\\mathbb{R}_+\\). Reste à vérifier que \\(\\int_{\\mathbb{R}^{2}} f(x,y)dx dy =1\\). Comme \\(f\\) est positive, on peut appliquer Fubini pour voir qu’on peut choisir un ordre quelconque d’intégration. Commençons par exemple par intégrer en \\(y\\), on obtient :\n\\[\\begin{align*}\n\\int_{\\mathbb{R}^{2}} f(x,y)dx dy\n& = \\int_{0}^1 \\left(\\int_{0}^{x^2} f(x,y) dy \\right) dx \\\\\n& = \\int_{0}^1 \\left(\\int_{0}^{x^2} y dy \\right) \\frac{4}{x^3} dx \\\\\n& = \\int_{0}^1 \\frac{x^4}{2} \\frac{4}{x^3} dx \\\\\n& = \\int_0^1 2x dx = \\left[ x^2 \\right]_0^1 =1,\n\\end{align*}\\]\net on conclut que \\(f\\) est bien une densité de probabilité sur \\(\\mathbb{R}^2\\) (on remarquera qu’étant donnée la présence de l’indicatrice, un vecteur \\((X,Y)\\) de densité \\(f\\) est presque sûrement à valeurs dans le carré ouvert \\((0,1)^2\\), et même presque sûrement à valeurs dans la partie du carré qui se trouve strictement sous la parabole \\(y=x^2\\). En particulier, les lois marginales sont toutes deux supportées par \\((0,1)\\).).\n\n\n\n\n\n\n\n\nNoteSolution (suite)\n\n\n\nPour \\(x \\in (0,1)\\),\n\\[f_X(x) = \\int_{0}^{x^2} f(x,y) dy = 2x.\\]\nde sorte que \\(f_X(x) = 2x \\mathbf{1}_{(0,1)}(x)\\).\nEnfin, pour \\(y \\in (0,1)\\), on a\n\\[\\begin{align*}\nf_Y(y)\n&= \\int_{\\sqrt{y}}^1 f(x,y) dx \\\\\n&= 2y \\int_{\\sqrt{y}}^1 \\frac{2}{x^3} dx \\\\\n&= 2y \\left[ \\frac{-1}{x^2} \\right]_{\\sqrt{y}}^1 = 2y \\left(-1+\\frac{1}{y}\\right) = 2(1-y)\n\\end{align*}\\]\nde sorte que \\(f_Y(y) = 2(1-y) \\mathbf{1}_{(0,1)}(y)\\).\n\n\nCalculer \\(f_{Y\\mid X}(y \\mid x)\\) et en déduire que\n\\[\\mathbb{E}[Y \\mid X] = \\frac{2}{3} X^2.\\]\n\n\n\n\n\n\nNoteSolution\n\n\n\nRappelons que\n\\[f_{Y\\mid X}(y \\mid x) = \\begin{cases} & \\frac{f(x,y)}{f_X(x)} \\mbox{ si } f_X(x) \\ne 0 \\\\ & 0 \\mbox{ sinon.}\\end{cases}\\]\nOn a donc\n\\[f_{Y\\mid X}(y \\mid x) = \\begin{cases} & \\frac{2y}{x^4} \\mathbf{1}_{\\{0&lt;y &lt;x^2\\}} \\text{ si } x \\in (0,1) \\\\ & 0 \\mbox{ sinon.} \\end{cases}\\]\nOn a alors \\(\\mathbb{E}[Y \\mid X] = \\psi(X)\\), où\n\\[\\psi(x)  =  \\int_{\\mathbb{R}} y f_{Y \\mid X}(y \\mid x) dy.\\]\nEn particulier \\(\\psi\\) a pour support \\((0,1)\\) et si \\(x \\in (0,1)\\),\n\\[\\begin{align*}\n\\psi(x)\n  & =  \\int_{0}^{x^2} y \\frac{2y}{x^4}  dy \\\\\n  & =  \\frac{2}{x^4} \\left[ \\frac{y^3}{3} \\right]_0^{x^2} = \\frac{2x^2}{3}.\n\\end{align*}\\]\nOn conclut que\n\\[\\mathbb{E}[Y \\mid X] = \\frac{2}{3} X^2.\\]\n\n\nMontrer que\n\\[f_{X \\mid Y}(x\\mid y) = \\frac{2y}{1-y} \\frac{1}{x^3} \\mathbf{1}_{\\{0&lt;x&lt;1, 0&lt;y &lt;x^2\\}},\\]\npuis calculer \\(\\mathbb{E}[X \\mid Y]\\).\n\n\n\n\n\n\nNoteSolution\n\n\n\nComme dans la question précédente,\n\\[\\begin{align*}\nf_{X\\mid Y}(x \\mid y)\n& =  \\begin{cases} & \\frac{f(x,y)}{f_Y(y)} \\mbox{ si } f_Y(y) \\ne 0 \\\\ & 0 \\text{ sinon,} \\end{cases} \\\\\n& =  \\begin{cases} \\frac{4y}{2(1-y)x^3}\\mathbf{1}_{\\{0&lt;x&lt;1, 0&lt;y&lt;x^2\\}} & \\text{ si } y \\in (0,1) \\\\  0 & \\text{ sinon,} \\end{cases}\n\\end{align*}\\]\nce qui est le résultat recherché puisque si \\(0&lt;x&lt;1\\) et \\(0&lt;y&lt;x^2\\), on a bien \\(y \\in (0,1).\\)\nOn a alors \\(\\mathbb{E}[X \\mid Y] = \\phi(Y)\\), où\n\\[\\phi(y)  =  \\int_{\\mathbb{R}} x f_{X \\mid Y}(x \\mid y) dx. \\]\nEn particulier \\(\\phi\\) a pour support \\((0,1)\\) et si \\(y \\in (0,1)\\),\n\\[\\begin{align*}\n\\phi(y)\n& = \\frac{2y}{1-y} \\int_{\\sqrt{y}}^{1} \\frac{1}{x^2}  dx \\\\\n& = \\frac{2y}{1-y} \\left[ -\\frac{1}{x} \\right]_{\\sqrt{y}}^{1} \\\\\n& = \\frac{2y}{1-y} \\left(-1+\\frac{1}{\\sqrt{y}} \\right) \\\\\n& = 2 \\sqrt{y} \\frac{1-\\sqrt{y}}{1-y} = 2\\frac{\\sqrt{y}}{1+\\sqrt{y}}.\n\\end{align*}\\]\nFinalement\n\\[\\mathbb{E}[X \\mid Y] = 2 \\frac{\\sqrt{Y}}{1+\\sqrt{Y}}.\\]\n\n\n\nExercice 13 (CC2 2023)  \n\nDans cet exercice on suppose que\n\\[\n\\begin{pmatrix} X \\\\ Y\\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 2 & 1 \\\\ 1 & 1 \\end{pmatrix} \\right),\n\\]\net on pose \\(U= X^2\\).\n\nVérifier que \\(U \\sim \\mathrm{Gamma}(1/2,1/4)\\).\nMontrer que \\((X,Y)\\) possède une densité jointe \\(g\\) que l’on déterminera.\n\nMontrer que \\((U,Y)\\) possède la densité jointe \\[ f(u,y) = \\frac{1}{{ 4} \\pi \\sqrt{u}} \\left( \\exp\\left(- \\frac{u}{2} - y^2 + y \\sqrt{u} \\right) + \\exp\\left(-\\frac{u}{2}- y^2 - y\\sqrt{u} \\right) \\right) \\mathbb{I}_{\\{u &gt;0\\}}. \\]\nCalculer \\(f_{Y \\mid U}(y \\mid u)\\). En déduire \\(\\mathbb{E}[Y \\mid U], \\mathbb{E}[Y^2 \\mid U]\\) et \\(\\mathrm{Var}(Y \\mid U)\\). Vérifier qu’on a bien \\[\\mathrm{Var}[Y] = \\mathbb{E}[\\mathrm{Var}[Y \\mid U]] + \\mathrm{Var}[\\mathbb{E}[Y \\mid U]]\\, .\\]\nOn suppose que conditionnellement à \\(U\\), \\(\\xi\\) et \\(Z\\) sont indépendantes avec \\(\\xi \\sim \\mathrm{Ber}(1/2)\\) et \\(Z \\sim \\mathcal{N}\\left(\\frac{\\sqrt{U}}{2},\\frac{1}{2}\\right)\\). Montrer que conditionnellement à \\(U\\), \\((2\\xi-1) Z\\) a même loi que \\(Y\\). Vérifier alors les calculs de la question précédente.\n\n\n\n\n\n\n\nAstuceIndications\n\n\n\n\nrappelle que pour \\(a&gt;0, \\lambda &gt;0\\), la densité d’une variable \\(G \\sim \\mathrm{Gamma}(a,\\lambda)\\) est donnée par\n\\[f_G(x) = \\frac{\\lambda^a x^{a-1}}{\\Gamma(a)} \\exp(-\\lambda x) \\mathbb{I}_{\\{x &gt;0 \\}}\\]\nOn fera attention à distinguer les domaines \\(D_1 = \\mathbb{R}_-^*\\times \\mathbb{R}\\) et \\(D_2 = \\mathbb{R}_+^* \\times \\mathbb{R}\\) pour pouvoir considérer les \\(\\mathcal{C}^1\\)-difféomorphismes \\(\\displaystyle{\\Psi_1 : \\begin{cases} \\!\\!&\\!\\! D_1 \\to \\mathbb{R}_+^* \\times \\mathbb{R} \\\\ \\!\\!&\\!\\! (x,y) \\to (x^2,y) \\end{cases}, \\ \\ \\Psi_2 :  \\begin{cases} \\!\\!&\\!\\! D_2 \\to \\mathbb{R}_+^* \\times \\mathbb{R} \\\\ \\!\\!&\\!\\! (x,y) \\to (x^2,y)\\end{cases}}\\).\nPour \\(\\alpha \\in \\mathbb{R}\\), les deux premiers moments de la variable \\(\\zeta \\sim \\mathcal{N}\\left(\\alpha \\frac{\\sqrt{u}}{2}, \\frac{1}{2}\\right)\\) sont\n\\[\\begin{align*}\n\\mathbb{E}[\\zeta]\n& = \\int_{\\mathbb{R}} \\frac{1}{\\sqrt{\\pi}} y \\exp\\left(-  \\left(y - \\alpha \\frac{\\sqrt{u}}{2}\\right)^2 \\right) dy \\\\\n& = \\alpha \\frac{\\sqrt{u}}{2},  \\\\\n\\mathbb{E}[\\zeta^2]\n& = \\mathbb{E}[\\zeta]^2+ \\mathrm{Var}[\\zeta] = \\int_{\\mathbb{R}} \\frac{1}{\\sqrt{\\pi}} y^2 \\exp\\left(-  \\left(y - \\alpha \\frac{\\sqrt{u}}{2}\\right)^2 \\right) dy \\\\\n& = \\alpha^2 \\frac{u}{4} + \\frac{1}{2}\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nOn a \\(U= X^2\\) avec \\(X \\sim \\mathcal{N}(0,2)\\). Pour \\(\\phi : \\mathbb{R} \\to \\mathbb{R}_+\\) borélienne, on obtient donc \\[\\begin{align*}\n\\mathbb{E}[\\phi(U)]\n& = \\int_{\\mathbb{R}} \\phi(x^2) \\frac{1}{2 \\sqrt{\\pi}} \\exp(-x^2) dx \\\\\n& = \\int_{\\mathbb{R}_-^*} \\phi(x^2) \\frac{1}{2 \\sqrt{\\pi}} \\exp\\left(-\\frac{x^2}{4}\\right) dx + \\int_{\\mathbb{R}_+^*} \\phi(x^2) \\frac{1}{2 \\sqrt{\\pi}} \\exp\\left(-\\frac{x^2}{4}\\right) dx\n\\end{align*}\\] En effectuant le changement de variables \\(u=x^2\\) dans chacune des deux intégrales ci-dessus on obtient\n\n\\[\\mathbb{E}[\\phi(U)] = \\int_{\\mathbb{R}_+^*} \\phi(u) \\frac{1}{2\\sqrt{\\pi}} \\exp\\left(-\\frac{u}{4}\\right) du,\\]\net on conclut que\n\\[f_U(u) = \\frac{1}{2\\sqrt{\\pi}} \\exp\\left(-\\frac{u}{4} \\right) \\mathbb{I}_{\\{u&gt;0\\}},\\]\nce qui est bien la densité d’une \\(\\mathrm{Gamma}(1/2,1/4)\\). 1. D’après le cours, un vecteur gaussien bi-dimensionnel suivant la loi \\(\\mathcal{N}(0,\\Sigma)\\) a une densité sur \\(\\mathbb{R}^2\\) ssi \\(\\mathrm{det}(\\Sigma) \\ne 0\\) et cette densité au point \\((x,y)\\) vaut\n\\[\\frac{1}{2\\pi \\sqrt{\\mathrm{det}(\\Sigma)}} \\exp\\left(- \\frac{1}{2} \\begin{pmatrix} x & y \\end{pmatrix} \\Sigma^{-1} \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\right).\\]\n\n\n\n\n\n\n\n\nNoteSolution (suite)\n\n\n\nIci \\(\\Sigma =  \\begin{pmatrix} 2 & 1 \\\\ 1 & 1 \\end{pmatrix}\\), donc \\(\\mathrm{det}(\\Sigma) = 1\\), \\(\\Sigma^{-1}= \\begin{pmatrix} 1 & -1 \\\\ -1 & 2 \\end{pmatrix}\\), et on obtient donc\n\\[g(x,y) = \\frac{1}{2 \\pi} \\exp\\left( -\\frac{x^2}{2} - xy - y^2\\right)\\]\n\nSoit \\(\\phi: \\mathbb{R}^2 \\to \\mathbb{R}_+\\) borélienne, on a d’après la question précédente\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(U,Y)]\n& =  \\int_{\\mathbb{R}^2} \\phi(x^2,y) \\frac{1}{2 \\pi} \\exp\\left( -\\frac{x^2}{2} - xy - y^2\\right)\n\\\\\n& =  \\int_{\\mathbb{R}_-^* \\times \\mathbb{R}} \\phi(x^2,y) \\frac{1}{2\\pi} \\exp\\left( -\\frac{x^2}{2} - xy - y^2\\right) dc dy \\\\\n& \\qquad + \\int_{\\mathbb{R}_+^* \\times \\mathbb{R}}  \\phi(x^2,y) \\frac{1}{2\\pi} \\exp\\left( -\\frac{x^2}{2} - xy - y^2\\right) dx dy\n\\end{align*}\\]\n\nL’application \\((x,y) \\to (u=x^2,y)\\) est un changement de variables de \\(\\mathbb{R}_-^* \\times \\mathbb{R} \\to \\mathbb{R}_+^* \\times \\mathbb{R}\\), avec \\(x = -\\sqrt{u}\\), et de \\(\\mathbb{R}_+^* \\times \\mathbb{R} \\to \\mathbb{R}_+^* \\times \\mathbb{R}\\) avec \\(x = \\sqrt{u}\\), dont le jacobien inverse est \\(\\frac{1}{2\\sqrt{u}}\\). On obtient donc :\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(U,Y)]\n    & = \\int_{\\mathbb{R}_+^* \\times \\mathbb{R}} \\frac{1}{4\\pi \\sqrt{u}} \\left(\\exp\\left( -\\frac{u}{2} + \\sqrt{u} y - y^2\\right) +  \\exp\\left( -\\frac{u}{2} - \\sqrt{u} y - y^2\\right) \\right)du dy\n\\end{align*}\\]\nce qui conduit bien au résultat souhaité.\nRemarque : Comme\n\\[\\int_{\\mathbb{R}} \\frac{1}{\\sqrt{\\pi}} \\exp\\left(-\\frac{u}{4} + \\sqrt{u} y -y^2\\right) dy = \\int_{\\mathbb{R}}  \\frac{1}{\\sqrt{\\pi}} \\exp\\left(-\\frac{u}{4} - \\sqrt{u} y -y^2\\right) dy = 1,\n\\]\npuisque la première intégrale est celle de la densité d’une variable \\(\\sim \\mathcal{N}(-\\sqrt{u}/2,1/2)\\), et la deuxième intégrale celle de la densité d’une variable \\(\\mathcal{N}(\\sqrt{u}/2,1/2)\\), on retrouve bien que\n\\[f_U(u) = \\int_{\\mathbb{R}} f(u,y)  dy = \\frac{2}{4\\sqrt{\\pi u}} \\exp\\left(-\\frac{u}{4}\\right) \\mathbb{I}_{\\{u &gt;0\\}},\\]\ncomme à la question 1.\n\n\n\n\n\n\n\n\nNoteSolution (suite)\n\n\n\n\nOn a donc\n\n\\[f_{Y \\mid U}(y \\mid u) =   \\frac{1}{2\\sqrt{\\pi}} \\left( \\exp\\left(- \\frac{u}{4} - y^2 - y \\sqrt{u} \\right) + \\exp\\left(-\\frac{u}{4}- y^2+y\\sqrt{u} \\right) \\right) \\mathbb{I}_{\\{u &gt;0\\}}.\\]\nRemarquons que\n\\[\\int_{\\mathbb{R}} \\frac{1}{\\sqrt{\\pi}} y \\exp\\left(- \\frac{u}{4} - y^2 + y \\sqrt{u} \\right) dy = \\int_{\\mathbb{R}} \\frac{1}{\\sqrt{\\pi}} \\exp\\left(-  \\left(y - \\frac{\\sqrt{u}}{2}\\right)^2 \\right) dy\\]\nest la moyenne d’une variable \\(\\sim \\mathcal{N}(-\\sqrt{u},1/2)\\), elle vaut donc \\(-\\frac{\\sqrt{u}}{2}\\).\nPar le même raisonnement, \\(\\int_{\\mathbb{R}} \\frac{1}{\\sqrt{\\pi}} y^2 \\exp\\left(- \\frac{u}{4} - y^2 + y \\sqrt{u} \\right) dy\\) est l’espérance du carré de cette même variable, et vaut donc \\(\\frac{1}{2}+\\frac{u}{4}\\).\nDe même, \\(\\int_{\\mathbb{R}} \\frac{1}{\\sqrt{\\pi}} y \\exp\\left(- \\frac{u}{4} - y^2 - y \\sqrt{u} \\right) dy\\) est la moyenne d’une variable \\(\\sim \\mathcal{N}(\\frac{\\sqrt{u}}{2},1/2)\\), et vaut donc \\(\\frac{\\sqrt{u}}{2}\\), tandis que \\(\\int_{\\mathbb{R}} \\frac{1}{\\sqrt{\\pi}} y^2 \\exp\\left(- \\frac{u}{4} - y^2 + y \\sqrt{u} \\right) dy\\) est l’espérance du carré de cette même variable, et vaut donc \\(\\frac{1}{2}+\\frac{u}{4}\\).\nOn a donc\n\\[\\int_{\\mathbb{R}} y f_{Y \\mid U}(y \\mid u) dy = -\\frac{1}{4}\\sqrt{u}+\\frac{1}{4}\\sqrt{u}=0, \\quad \\mbox{ donc } \\mathbb{E}[Y \\mid U]=0\\]\ntandis que\n\\[\\int_{\\mathbb{R}} y^2 f_{Y \\mid U}(y \\mid u) dy = \\frac{1}{4}(u+2) + \\frac{1}{4}(u+2) = u+2 \\quad \\mbox{ donc } \\mathbb{E}[Y^2 \\mid U] = \\frac{1}{2} +\\frac{U}{4}.\\]\nEnfin\n\\[\\mathrm{Var}[Y \\mid U] = \\mathbb{E}[Y^2 \\mid U] - \\mathbb{E}[Y \\mid U]^2= \\frac{1}{2}+\\frac{U}{4}.\\]\nBien s^ur \\(\\mathrm{Var}[\\mathbb{E}[Y \\mid U]]=0\\). Comme \\(\\mathbb{E}[U] = \\frac{1}{2} \\times \\left(\\frac{1}{4}\\right)^{-1} = 2\\), on a bien\n\\[\\mathbb{E}[\\mathrm{Var}[Y \\mid U]] = \\frac{1}{2} + \\frac{1}{2} = 1,\\]\net on a bien \\(\\mathrm{Var}(Y) = 1 = \\mathbb{E}[\\mathrm{Var}[Y \\mid U]]+\\mathrm{Var}[\\mathbb{E}[Y \\mid U]]\\).\n\n\n\n\n\n\n\n\nNoteSolution (suite)\n\n\n\n\nLa densité d’une variable \\(\\zeta \\sim \\mathcal{N}\\left(\\frac{\\sqrt{u}}{2}, \\frac{1}{2}\\right)\\) est donnée par\n\n\\[f_{\\zeta}(y) = \\frac{1}{\\sqrt{\\pi}} \\exp\\left(-  \\left(y -  \\frac{\\sqrt{u}}{2}\\right)^2 \\right), \\ y \\in \\mathbb{R}.\\]\nCelle de \\(-\\zeta \\sim \\mathcal{N}\\left(-\\frac{\\sqrt{u}}{2}, \\frac{1}{2}\\right)\\) est donc donnée par\n\\[f_{-\\zeta}(y) = \\frac{1}{\\sqrt{\\pi}} \\exp\\left(-  \\left(y +  \\frac{\\sqrt{u}}{2}\\right)^2 \\right), \\ y \\in \\mathbb{R}.\\]\nSi \\(\\xi \\sim \\mathrm{Ber}(1/2)\\), la variable \\((2\\xi-1) \\zeta\\) a donc densité \\(\\frac{1}{2} \\left(f_{\\zeta}(y) + \\frac{1}{2} f_{-\\zeta}(y)\\right), y \\in \\mathbb{R}\\), et on déduit que la densité conditionnelle de \\((2\\xi -1) Z\\) sachant \\(U\\) au point \\((y,u)\\) est donnée par \\(f_{Y \\mid U}(y \\mid u)\\). Ainsi \\((U,Y)\\) et \\((U, (2\\xi-1)Z)\\) ont même loi.\nOn retrouve bien :\n\\[\\mathbb{E}[Y \\mid U] = \\mathbb{E}[(2\\xi-1)Z \\mid U] = \\frac{1}{2} \\left( \\frac{\\sqrt{U}}{2} - \\frac{\\sqrt{U}}{2}\\right) =0, \\quad \\mathbb{E}(Y^2 \\mid U) = \\mathbb{E}[Z^2 \\mid U] = \\frac{U}{4}+\\frac{1}{2}\\]\n\n\n\nExercice 14 (Rattrapage passé)  \n\nSoit \\(X=(X_1,X_2,X_3) \\sim \\mathcal{N}(0,M)\\), où\n\\[M := \\begin{pmatrix} 2& 2 &-2  \\\\ 2& 5 & 1 \\\\ -2 & 1 & 5 \\end{pmatrix},\\]\n\nMontrer que \\(\\det(M)=0\\). Le vecteur \\(X\\) possède-t-il une densité dans \\(\\mathbb{R}^3\\)?\nTrouver \\(a \\in \\mathbb{R}\\) tel que \\(X_1\\) et \\(Y=X_2-a X_1\\) soient indépendantes. Calculer \\(\\mathrm{Var}(Y)\\) et en déduire la loi de \\((X_1,Y)\\).\nTrouver la loi conditionnelle de \\(X_2\\) sachant \\(X_1\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nOn trouve \\(\\ker(M) = \\mathrm{Vect} \\left\\{ \\begin{pmatrix} 2 \\\\ -1 \\\\ 1 \\end{pmatrix} \\right\\}\\) de dimension \\(1\\), donc \\(\\det(M)=0\\). Le vecteur \\(X\\) est donc p.s. à valeurs dans \\(\\mathrm{Ker}(M)^{\\perp} = \\{(x,y,z) \\in \\mathbb{R}^3 : 2x - y + z =0\\}\\), en particulier il ne possède pas de densité sur \\(\\mathbb{R}^3\\).\n\n\\((X_1,Y)\\) est un vecteur gaussien comme image par une application linéaire d’un vecteur gaussien, ses coordonnées sont donc indépendantes ssi \\(\\mathrm{Cov}(X_1,X_2-aX_1)=0\\) ssi \\(a=1\\).\nOn déduit que \\(X_2 = X_1 + Y\\), avec \\(Y = X_2-X_1\\) indépendant de \\(X_1\\), et de loi \\(\\mathcal{N}(0,3)\\) (on a utilisé que \\(\\mathrm{Var}(Y) = \\mathrm{Var}(X_1)+\\mathrm{Var}(X_2) - 2 \\mathrm{Cov}(X_1,X_2) = 3\\)).\n\nOn conclut que conditionnellement à \\(X_1\\), \\(X_2 \\sim \\mathcal{N}(X_1,3)\\).\n\n\n\nExercice 15 (Combinaison linéaire de gaussiennes)  \n\nOn considère \\(X_0 =0\\), et \\((X_n)_{n \\ge 1}\\) une suite de variables aléatoires réelles indépendantes, identiquement distribuées suivant la loi normale centrée réduite.\nOn introduit les variables\n\\[Y_i = \\frac{X_i-X_{i-1}}{i}, i \\ge 1.\\]\nPour \\(n \\ge 1\\), montrer que le vecteur \\((Y_1,...,Y_n)\\) est gaussien, puis calculer le vecteur moyenne et la matrice de covariances de \\((Y_1,...,Y_n)\\).\n\n\n\n\n\n\nNoteSolution\n\n\n\nFixons \\(n \\ge 1\\) et notons \\(\\mathbf{X}_n = (X_1,...,X_n), \\mathbf{Y}_n=(Y_1,...,Y_n)\\).\nLes variables \\(\\{X_i\\}_{i=1}^n\\) étant des gaussiennes centrées réduites indépendantes, on a déjà montré (par exemple à la première question du partiel) que \\((X_1,...,X_n)\\) est un vecteur gaussien (et d’ailleurs \\(\\mathbf{X}_n \\sim \\mathcal{N}(0,I_n)\\)).\nNotons alors\n\\[A_n := \\begin{pmatrix} 1 & 0 & 0 & 0 & \\dots & 0 & 0 \\\\\n                         -1/2 & 1/2 & 0 & 0& \\dots &0 & 0 \\\\\n                          0 & -1/3 & 1/3 & 0 & \\dots & 0 & 0 \\\\\n                         \\vdots &  & \\ddots & \\ddots & & & \\\\\n                         \\vdots &  &  & \\ddots & \\ddots & & \\\\\n                         \\vdots &  &  &        & \\ddots & \\ddots & \\\\  \n                          0 & 0 & 0& 0 & \\dots & -1/n & 1/n \\end{pmatrix},\\]\nde sorte que \\(\\mathbf{Y}_n = A_n \\mathbf{X}_n\\), et le vecteur \\(\\mathbf{Y}_n\\) est donc bien un vecteur gaussien en tant que transformation linéaire du vecteur gaussien \\(\\mathbf{X}_n\\).\n\n\n\n\n\n\n\n\nNoteSolution (suite)\n\n\n\nD’après un théorème du cours, on a alors que \\(\\mathbf{Y}_n \\sim \\mathcal{N}(A_n 0, A_n I_n A_n^T)\\), i.e. \\(\\mathbf{Y}_n \\sim \\mathcal{N}(0,A_n A_n^T)\\), où\n\\[\nA_n A_n^T = \\begin{pmatrix} 1 & -1/2 & 0 & 0 & \\dots & 0 & 0 \\\\\n                               -1/2 & 1/2 & -1/6 & 0 & \\dots & 0 & 0 \\\\\n                               0 & -1/6 & 2/9 & -1/12 & \\dots &0 & 0 \\\\\n                               \\vdots & & \\ddots & \\ddots & \\ddots & & \\\\\n                               \\vdots & &        & \\ddots & \\ddots & \\ddots & \\\\\n                               \\vdots & &        &        & \\ddots & \\ddots & \\\\\n                               0 & \\dots &  &  & 0 & -\\frac{1}{n(n-1)} & \\frac{2}{n^2}\\end{pmatrix}.\n\\]\n\n\nCalculer, pour \\(n \\ge 1\\), \\(\\mathbb{E}[Y_{n+1}\\mid Y_n]\\).\n\n\n\n\n\n\nNoteSolution\n\n\n\nPour \\(a \\in \\mathbb{R}\\) on peut toujours écrire \\(Y_{n+1} = Y_{n+1} + a Y_n - a Y_n\\).\nComme le vecteur \\(\\mathbf{Y}_{n+1}\\) est gaussien, la variable \\(Y_{n+1}+aY_n\\) est indépendante de \\(Y_n\\) si et seulement si \\(\\mathrm{cov}(Y_{n+1}+a Y_n, Y_n)=0.\\) Or \\[\\mathrm{cov}(Y_{n+1}+a Y_n, Y_n)= -\\frac{1}{n(n+1)} + \\frac{2a}{n^2},\\] qui s’annule pour \\(a = \\frac{n}{2(n+1)}\\).\nOn a alors, en utilisant cette indépendance et le fait que les variables \\(Y_n, Y_{n+1}\\) sont centrées :\n\\[\\begin{align*}\n\\mathbb{E}[Y_{n+1} \\mid \\mathcal{F}_n] & = \\mathbb{E}[( Y_{n+1} + \\frac{n}{2(n+1)} Y_n )\\mid \\mathcal{F}_n] - \\mathbb{E}[\\frac{n}{2(n+1)} Y_n \\mid \\mathcal{F}_n] \\\\\n& =  \\mathbb{E}[Y_{n+1} + \\frac{n}{2(n+1)} Y_n] - \\frac{n}{2(n+1)} Y_n \\\\\n& =  - \\frac{n}{2(n+1)} Y_n.\n\\end{align*}\\]\n\n\n\nExercice 16 (Loi jointe à densité)  \n\nSoient \\((X,Y)\\) dont la loi jointe a pour densité \\(f(x,y) = x(y-x) \\exp(-y), 0 \\le x \\le y &lt;\\infty\\). On introduit la notation \\(f_{X|Y}(x|y) := f(x,y)/f_Y(y)\\) lorsque le quotient est \\(&gt;0\\), \\(0\\) sinon.\n\nExprimer \\(f_{X|Y}(x|y)\\), puis \\(f_{Y|X}(y|x)\\).\nEn déduire les expressions de \\(\\mathbb{E}[X|Y], \\mathbb{E}[Y|X]\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nCalculons d’abord les densités marginales.\n\\[\\begin{align*}\nf_X(x) & =  \\mathbb{I}_{\\{x&gt;0\\}} \\int_x^{\\infty} x(y-x) \\exp(-y) dy \\\\\n& =  \\mathbb{I}_{\\{x &gt; 0\\}} x \\exp(-x)  \\int_0^{\\infty} u \\exp(-u) du = \\mathbb{I}_{\\{x &gt; 0\\}} x \\exp(-x)\n\\end{align*}\\]\nde sorte que \\(X \\sim \\mathrm{Gamma}(2,1)\\).\nPar ailleurs\n\\[\\begin{align*}\nf_Y(y)\n& =  \\mathbb{I}_{\\{y &gt;0\\}} \\exp(-y) \\int_0^y x(y-x) dx \\\\\n& =  \\mathbb{I}_{\\{y &gt;0\\}} \\exp(-y) \\left( \\frac{y^3}{2} - \\frac{y^3}{3} \\right) =  \\mathbb{I}_{\\{y &gt;0\\}} \\exp(-y) \\frac{y^3}{6}\n\\end{align*}\\]\nde sorte que \\(Y \\sim \\mathrm{Gamma}(4,1)\\).\nOn déduit\n\\[f_{X \\mid Y} (x \\mid y) = \\frac{6x(y-x)}{y^3} \\mathbb{I}_{\\{0 &lt; x &lt; y\\}},\\]\n\\[f_{Y \\mid X} (y \\mid x) = (y-x) \\exp(-(y-x)) \\mathbb{I}_{\\{0 &lt; x &lt; y\\}}.\\]\nRemarque : On peut assez facilement interpréter cette deuxième densité conditionnelle : sachant \\(X\\), \\(Y = X + U\\) où \\(U \\sim \\Gamma(2,1)\\) indépendante de \\(X\\).\n\n\n\n\n\n\n\n\n\nNoteSolution (suite)\n\n\n\n\nOn a\n\\[\\int_{\\mathbb{R}} x f_{X \\mid Y}(x \\mid y) dx = \\int_0^y \\frac{6x^2(y-x)}{y^3} dx = 2y - \\frac{3}{2}y= \\frac{y}{2},\\]\net on conclut que \\(\\mathbb{E}[X \\mid Y] = \\frac{Y}{2}\\).\nD’autre part\n\\[\\begin{align*}\n\\int_{\\mathbb{R}} y f_{Y \\mid X}(y \\mid x) dy & = \\int_{x}^{\\infty}  y (y-x) \\exp(-(y-x)) dy \\\\\n& =  \\int_0^{\\infty} (u+x) u \\exp(-u) du \\\\\n& =  \\left[ -(u+x)u \\exp(-u) \\right]_{0}^{\\infty} + \\int_0^{\\infty} (2u+x) \\exp(-u) du \\\\\n& =  \\left[ -(2u+x) \\exp(-u) \\right]_0^{\\infty} + \\int_0^{\\infty} 2 \\exp(-u) du = x + 2\n\\end{align*}\\]\net on conclut que \\(\\mathbb{E}[Y \\mid X] = X+2\\).\n\n\n\n\nExercice 17 (Exponentielles conditionnées)  \n\nSoient \\(Y,Z\\) deux v.a.r. indépendantes \\(\\sim \\mathrm{exp}(\\lambda)\\) où \\(\\lambda&gt;0\\). On pose \\(X= Y+Z\\). Quelle est la loi conditionnelle de \\(Y\\) sachant \\(X\\)? Que vaut \\(\\mathbb{E}[Y|X]\\)? En déduire l’expression de \\(\\mathbb{E}[Y|X]\\)\n\n\n\n\n\n\nNoteSolution\n\n\n\nRemarquons déjà que par le même raisonnement que dans l’exercice 5 on trouve \\(\\mathbb{E}[Y \\mid X] = \\frac{X}{2}\\).\nPour le reste on peut faire un raisonnement similaire aux exercices 11, 15. D’abord, pour \\(\\phi  : \\mathbb{R}^2 \\to \\mathbb{R}_+\\) borélienne,\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(Y, Y+Z)] & =\n  \\int_{\\mathbb{R}_+^2} \\phi(y,y+z) \\lambda^2 \\exp(-\\lambda (y+z)) dy dz \\\\\n& =  \\int_{\\mathbb{R}_+^2} \\mathbb{I}_{\\{y \\le x\\}} \\phi(y,x) \\lambda^2 \\exp(-\\lambda x) dy dx\n\\end{align*}\\]\nde sorte que \\(f_{(Y,X)}(y,x) = \\mathbb{I}_{\\{0&lt;y&lt;x\\}} \\lambda^2 \\exp(-\\lambda x), \\ (x,y) \\in \\mathbb{R}^2\\).\nComme \\(X \\sim \\mathrm{Gamma}(2,1)\\) on a\n\\(f_X(x) = \\lambda^2 \\exp(-\\lambda x) \\mathbb{I}_{\\{x &gt;0\\}}, x \\in \\mathbb{R}\\)\net donc\n\\[f_{Y \\mid X}(y \\mid x) = \\frac{1}{x}  \\mathbb{I}_{\\{0 &lt; y &lt; x \\}}\\]\nde sorte que la loi conditionnelle de \\(Y\\) sachant \\(X\\) est \\(\\mathrm{Unif}[0,X]\\). On conclut que \\(\\mathbb{E}[Y \\mid X] = \\frac{X}{2}.\\)\n\n\n\nExercice 18 (Gaussiennes corrélées)  \n\nSoient \\(X\\) et \\(Y\\) deux variables aléatoires indépendantes, toutes deux normales centrées réduites. On définit pour \\(\\sigma_1 &gt;0, \\sigma_2&gt;0, |\\rho|\\le 1\\),\n\\[U = \\sigma_1 X, \\quad V= \\sigma_2 \\rho X + \\sigma_2 \\sqrt{1- \\rho^2} Y.\\]\n\nQuelle est la loi de \\((U,V)\\)?\nQue vaut \\(\\mathbb{E}[UV]\\)?\nQue vaut \\(\\mathbb{E}[U \\mid V]? \\mathbb{E}[V \\mid U]? \\mathrm{Var}[U \\mid V]? \\mathrm{Var}[V \\mid U]?\\)\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nOn a \\(\\begin{pmatrix} U \\\\ V \\end{pmatrix} = \\begin{pmatrix} \\sigma_1 & 0 \\\\ \\sigma_2 \\rho & \\sigma_2 \\sqrt{1-\\rho^2} \\end{pmatrix} \\begin{pmatrix} X \\\\ Y \\end{pmatrix}\\), avec \\(\\begin{pmatrix} X \\\\ Y \\end{pmatrix} \\sim \\mathcal{N}(0,I_2)\\). Or \\(\\begin{pmatrix} \\sigma_1 & 0 \\\\ \\sigma_2 \\rho & \\sigma_2 \\sqrt{1-\\rho^2} \\end{pmatrix} I_2 \\begin{pmatrix} \\sigma_1 & 0 \\\\ \\sigma_2 \\rho & \\sigma_2 \\sqrt{1-\\rho^2} \\end{pmatrix}^T = \\begin{pmatrix} \\sigma_1^2 & \\sigma_1 \\sigma_2 \\rho \\\\ \\sigma_1 \\sigma_2 \\rho & \\sigma_2^2 \\end{pmatrix}\\), on déduit donc que \\(\\begin{pmatrix} U \\\\ V \\end{pmatrix}  \\sim \\mathcal{N}\\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix},  \\begin{pmatrix} \\sigma_1^2 & \\sigma_1 \\sigma_2 \\rho \\\\ \\sigma_1 \\sigma_2 \\rho & \\sigma_2^2 \\end{pmatrix}\\right).\\)\nOn a \\(\\mathbb{E}[UV]=\\mathrm{Cov}(UV) = \\sigma_1\\sigma_2 \\rho\\) d’après la question précédente.\nD’après les formules du cours, avec \\(\\theta = U, \\xi =V\\), \\(\\mu_{\\theta}=\\mu_{\\xi}=0, M_{\\theta \\xi} = M_{\\xi\\theta} = \\sigma_1\\sigma_2 \\rho, M_{\\theta \\theta} = \\sigma_1^2\\) et \\(MM_{\\xi \\xi} = \\sigma_2^2\\), on trouve que\n\n\\[\\mathbb{E}[U \\mid V] = M_{\\theta \\xi} M_{\\xi\\xi}^{-1} \\xi = \\frac{\\sigma_1 \\rho}{\\sigma_2} V, \\qquad \\mathrm{Var}[U \\mid V ] = M_{\\theta \\theta} - M_{\\theta \\xi} M_{\\xi \\xi } M_{\\xi \\theta} = \\sigma_1^2(1-\\rho^2)\\]\n\n\n\n\n\n\n\n\nNoteSolution (suite)\n\n\n\nDe manière symétrique on trouve que\n\\[\\mathbb{E}[V \\mid U] = \\frac{\\sigma_2 \\rho}{\\sigma_1} U, \\qquad \\mathrm{Var}[V \\mid U] = \\sigma_2^2 (1-\\rho^2).\\]\nRemarque : \\(\\mathrm{Cov}(\\alpha U + V, U) = \\alpha \\sigma_1^2 + \\rho \\sigma_1 \\sigma_2\\), on trouve donc pour \\(\\alpha = - \\frac{\\sigma_2 \\rho}{\\sigma_1}\\) que \\[ V = \\frac{\\sigma_2 \\rho}{\\sigma_1} U + \\left( - \\frac{\\sigma_2 \\rho}{\\sigma_1}U + V \\right),\\] la première partie de la somme étant bien s^ur \\(\\sigma(U)\\)-mesurable, alors que la deuxième en est indépendante, et suit la loi \\(\\mathcal{N}(0,\\sigma_2^2(1-\\rho^2))\\). On retrouve donc bien que conditionnellement à \\(U, V \\sim \\mathcal{N}\\left( \\frac{\\sigma_2 \\rho}{\\sigma_1} U, \\sigma_2^2(1-\\rho^2)\\right)\\).\n{} : \\(\\sigma_1^2\\) est la variance de \\(U\\), \\(\\sigma_2^2\\) celle de \\(V\\), et \\(\\rho\\) est le coefficient de corrélation de \\(U\\) et \\(V\\). L’énoncé de l’exercice fournit donc une mani ere de fabriquer un vecteur gaussien \\(2\\)-dimensionnel et non dégénéré quelconque à partir d’un vecteur gaussien centré réduit.\n\n\n\nExercice 19 (Gaussiennes corrélées (2))  \n\nSoit \\(Z = (X, Y )\\) un vecteur aléatoire gaussien à valeurs dans \\(\\mathbb{R}^2\\). On suppose que \\(E(X) = E(Y ) = 0\\), \\(\\mathrm{Var}(X) = \\mathrm{Var}(Y ) = 1\\) et que \\(\\mathrm{Cov}(X; Y ) = \\rho\\) avec \\(|\\rho|^2 \\ne 1\\). On pose \\(U = X -\\rho Y , V = \\sqrt{1-\\rho^2} Y\\).\n\nQuelles sont les lois de \\(U\\) et \\(V\\) ? Les v.a. \\(U\\) et \\(V\\) sont-elles indépendantes ?\nCalculer \\(\\mathbb{E}(U^2V^2), \\mathbb{E}(U V^3), \\mathbb{E}(V^4)\\). En déduire \\(\\mathbb{E}(X^2Y^2)\\).\nRetrouver ce dernier résultat par conditionnement.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nOn a \\(\\begin{pmatrix} U \\\\ V \\end{pmatrix} = \\begin{pmatrix} 1 & -\\rho \\\\ 0 & \\sqrt{1-\\rho^2} \\end{pmatrix} \\begin{pmatrix} X \\\\ Y \\end{pmatrix}\\), avec \\(\\begin{pmatrix} X \\\\ Y \\end{pmatrix} \\sim \\mathcal{N}\\left(\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1\\end{pmatrix} \\right)\\). Or \\(\\begin{pmatrix} 1 & -\\rho \\\\ 0 & \\sqrt{1-\\rho^2} \\end{pmatrix} \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1\\end{pmatrix}   \\begin{pmatrix} 1 & 0 \\\\ - \\rho & \\sqrt{1-\\rho^2} \\end{pmatrix} = \\begin{pmatrix} \\sigma_1^2 & \\sigma_1 \\sigma_2 \\rho \\\\ \\sigma_1 \\sigma_2 \\rho & \\sigma_2^2 \\end{pmatrix}\\), on déduit donc que \\(\\begin{pmatrix} U \\\\ V \\end{pmatrix}  \\sim \\mathcal{N}\\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix},  \\begin{pmatrix} 1-\\rho^2 & 0 \\\\ 0 & 1-\\rho^2 \\end{pmatrix}\\right),\\) autrement dit \\(U\\) et \\(V\\) sont i.i.d de loi \\(\\mathcal{N}(0,1-\\rho^2)\\).\nOn déduit \\[\\mathbb{E}[U^2V^2] = (1-\\rho^2)^2, \\quad \\mathbb{E}[UV^3] = 0, \\quad \\mathbb{E}[V^4] = 3(1-\\rho^2).\\] On a donc (\\(V\\) et \\(Y\\) ne diffèrent que par une constante multiplicative donc \\(U\\) est indépendant de \\(Y\\) \\[\\mathbb{E}[X^2 Y^2] = \\mathbb{E}[(U+\\rho Y)^2 Y^2] = \\mathbb{E}[U^2] \\mathbb{E}[Y^2] + 2 \\rho \\mathbb{E}[U] \\mathbb{E}[Y^3] + \\rho^2 \\mathbb{E}[Y^4] = (1-\\rho^2) +3\\rho^2 = 1+2\\rho^2.\\]\n\n\n\n\n\n\n\n\n\nNoteSolution (suite)\n\n\n\n\nOn a vu que \\(U=X-\\rho Y\\) est indépendant de \\(Y\\) (et suit une \\(\\mathcal{N}(0,1-\\rho^2)\\), donc sachant \\(Y\\), \\(X \\sim \\mathcal{N}(\\rho Y, 1-\\rho^2)\\). En particulier \\(\\mathbb{E}[X^2 \\mid Y] =  \\mathbb{E}[X \\mid Y]^2 + \\mathrm{Var}[X \\mid Y] = \\rho^2 Y^2 + 1-\\rho^2.\\) On a donc \\[\\mathbb{E}[X^2 Y^2] = \\mathbb{E}[\\mathbb{E}[X^2 Y^2 \\mid Y]] = \\mathbb{E}[\\rho^2 Y^4 + (1-\\rho^2)Y^2] = 3 \\rho^2 + (1-\\rho^2) = 1+2\\rho^2.\\]\n\n\n\n\nExercice 20 (Gaussiennes)  \n\nSoient \\(U, V, W\\) trois v.a.r. gaussiennes centrées réduites. On pose \\[Z =\\frac{U + VW}{\\sqrt{1+W^2}}.\\]\n\nQuelle est la loi conditionnelle de \\(Z\\) sachant \\(W\\)?\nEn déduire que \\(Z\\) et \\(W\\) sont indépendantes et donner la loi de \\(Z\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nPour \\(\\alpha \\in \\mathbb{R}\\), la loi de \\(\\frac{U+ \\alpha V}{\\sqrt{1+\\alpha^2}}\\) est gaussienne, centrée, et de variance \\[ \\frac{\\mathrm{Var}(U) + 2\\alpha \\mathrm{Cov}(U,V) + \\alpha^2 \\mathrm{Var}(V)}{1+\\alpha^2} = 1.\\] Donc, quelque soit \\(\\alpha \\in \\mathbb{R}\\), \\(\\frac{U+ \\alpha V}{\\sqrt{1+\\alpha^2}} \\sim \\mathcal{N}(0,1)\\). On déduit que sachant \\(W\\), \\(Z \\sim \\mathcal{N}(0,1)\\)\nLa loi conditionnelle de \\(Z\\) sachant \\(W\\) ne dépend pas de \\(W\\) (et c’est sa loi), on déduit que \\(Z\\) et \\(W\\) sont indépendantes. La loi de \\(Z\\) est \\(\\mathcal{N}(0,1)\\).\n\n\n\n\n\n\n\n\n\nNoteSolution (suite)\n\n\n\nRaisonnement alternatif :\nPar hypothèse, \\((U,V,W)\\) possède la densité jointe\n\\[f(u,v,w) = \\frac{1}{(2\\pi)^{3/2}} \\exp\\left(-\\frac{1}{2}(u^2+v^2+w^2) \\right), \\quad (u,v,w) \\in \\mathbb{R}^3.\\]\nCalculons la densité de \\((W,Z)\\). Soit \\(\\phi : \\mathbb{R}^2 \\to \\mathbb{R}_+\\) borélienne. On va utiliser le changement de variables \\(\\Psi : \\begin{cases} & \\mathbb{R}^3 \\to \\mathbb{R}^3 \\\\  & (u,v,w) \\to (z,s,t) \\end{cases}\\) avec \\(z=\\frac{u+vw}{\\sqrt{1+w^2}}, s=v, t=w\\). Il s’agit bien d’un \\(\\mathcal{C}^1\\)-difféomorphisme, d’inverse\n\\(u=z\\sqrt{1+t^2}-  st, v=s, w=t\\) et de jacobien inverse \\(\\sqrt{1+t^2}\\)\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(W,Z)] & =   \\frac{1}{(2\\pi)^{3/2}}\\int_{\\mathbb{R}^3}\\phi\\left(w,\\frac{u+vw}{\\sqrt{1+w^2}}\\right) \\exp\\left(-\\frac{1}{2}(u^2+v^2+w^2) \\right) du dv dw \\\\\n& =  \\frac{1}{(2\\pi)^{3/2}}\\int_{\\mathbb{R}^3}  \\phi(t,z)  \\sqrt{1+t^2} \\exp\\left(-\\frac{1}{2} \\left(z^2(1+s^2) + s^2 t^2 -2z t s \\sqrt{1+t^2}+ s^2 + t^2\\right)\\right) ds dt dz\n\\\\ & = \\int_{\\mathbb{R}^2} \\frac{1}{(2\\pi)} \\exp\\left( -\\frac{1}{2} (z^2+t^2)\\right) dt dz \\int_{\\mathbb{R}}\\frac{\\sqrt{1+t^2}}{\\sqrt{2\\pi}} \\exp\\left( -\\frac{1}{2} (s\\sqrt{1+t^2} -zt)^2\\right) ds\n\\end{align*}\\]\nComme \\(\\int_{\\mathbb{R}}\\frac{\\sqrt{1+t^2}}{\\sqrt{2\\pi}} \\exp\\left( -\\frac{1}{2} (s\\sqrt{1+t^2} -zt)^2\\right) ds = 1\\) (on reconna^t l’intégrale sur \\(\\mathbb{R}\\) de la densité d’une \\(\\mathcal{N}(\\frac{zt}{\\sqrt{1+t^2}}, \\frac{1}{\\sqrt{1+t^2}})\\), on obtient\n\\[\\mathbb{E}[\\phi(W,Z)]  = \\int_{\\mathbb{R}^2} \\frac{1}{(2\\pi)} \\exp\\left( -\\frac{1}{2} (s^2+t^2)\\right)\\]\net on conclut que \\((W,Z)\\) est un vecteur gaussien bi-dimensionnel, centré réduit, et on retrouve les résultats précédents.\n\n\n\nExercice 21 (Maxima d’exponentielles)  \n\nSoient \\(X_1\\) et \\(X_2\\) des v.a. indépendantes, de lois exponentielles de paramètres respectifs \\(\\lambda_1\\) et \\(\\lambda_2\\).\n\nCalculer \\(\\mathbb{E}[\\max(X_1,X_2) \\mid X_1]\\).\nCalculer \\(\\mathbb{E}[\\max(X_1;X_2)]\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\\(X_2 \\sim \\exp(\\lambda_2)\\) et possède donc la propriété d’absence de mémoire. Pour \\(a&gt;0\\) fixé on a donc\n\n\nOn en déduit que\n\\[\\mathbb{E}[X_1 \\mathbb{I}_{\\{X_1 \\ge X_2\\}} \\mid X_1] = X_1 (1-\\exp(-\\lambda_2 X_1)), \\quad \\mathbb{E}[X_2 \\mathbb{I}_{\\{X_2 \\ge X_1\\}} \\mid X_1] = \\left(X_1+\\frac{1}{\\lambda_2}\\right) \\exp(-\\lambda_2 X_1),\\]\net donc\n\\[\\mathbb{E}[\\max(X_1,X_2) \\mid X_1] = X_1 + \\frac{1}{\\lambda_2} \\exp(-\\lambda_2 X_1)\\]\n\nOn a pour \\(t \\ge 0\\), \\(\\mathbb{E}[\\exp(-t X_1)] = \\frac{\\lambda_1}{\\lambda_1+t}\\) et donc\n\n\\[\\mathbb{E}[\\max(X_1,X_2)] = \\mathbb{E}[\\mathbb{E}[\\max(X_1,X_2) \\mid X_1]] = \\frac{1}{\\lambda_1} + \\frac{1}{\\lambda_2} \\frac{\\lambda_1}{\\lambda_1+\\lambda_2} = \\frac{\\lambda_1^2 + \\lambda_1\\lambda_2 + \\lambda_2^2}{\\lambda_1\\lambda_2(\\lambda_1+\\lambda_2)}.\\]\nRemarque, vérification : Soit \\(Y = \\max(X_1,X_2)\\), on a \\(F_Y(t) = F_{X_1}(t)F_{X_2}(t) = (1-\\exp(-\\lambda_1 t)) (1-\\exp(-\\lambda_1 t))  \\mathbb{I}_{\\{t \\ge 0\\}}\\). On déduit\n\\[\\begin{align*}\nf_Y(t) & =  \\left( \\lambda_1 \\exp(-\\lambda_1 t) (1-\\exp(-\\lambda_2 t)) + \\lambda_2 \\exp(-\\lambda_2 t) (1-\\exp(-\\lambda_1 t)) \\right) \\mathbb{I}_{\\{t \\ge 0\\}}\n\\\\ & =  \\left(\\lambda_1 \\exp(-\\lambda_1 t) + \\lambda_2 \\exp(-\\lambda_2 t) - (\\lambda_1+\\lambda_2) \\exp(-(\\lambda_1+\\lambda_2)t) \\right) \\mathbb{I}_{\\{t \\ge 0\\}}.\n\\end{align*}\\]\nOn calcule alors facilement\n\\[\\mathbb{E}[Y] = \\int_{\\mathbb{R}} t f_Y(t) dt = \\frac{1}{\\lambda_1} + \\frac{1}{\\lambda_2}- \\frac{1}{\\lambda_1+\\lambda_2},\\]\net on retrouve le résultat précédent.\n\n\n\nExercice 22 (Densités jointes)  \n\nOn pose \\(h(x) = \\frac{1}{\\Gamma(a+1)} \\exp(-x)x^{a-1}\\) (\\(a &gt; 0\\) fixé) et \\(D = \\{0 &lt; y &lt; x\\}\\). Soit \\(f(x, y) = h(x)\\mathbf{1}_D(x, y)\\):\n\nMontrer que \\(f\\) est une densité de probabilité sur \\(\\mathbb{R}^2\\). On considère dans la suite un couple \\((X, Y)\\) de v.a.r. de densité \\(f\\).\nLes v.a. \\(X\\) et \\(Y/X\\) sont-elles indépendantes?\nQuelle est la loi conditionnelle de \\(Y\\) sachant \\(X\\) ?\nSoit \\(U\\) une v.a.r. indépendante du couple \\((X, Y)\\) telle que \\(\\mathbb{P}(U = 1) = p\\) et \\(\\mathbb{P}(U = 0) = 1 - p\\). On pose \\(Z = UX + (1 - U)Y\\). Quelle est l’espérance conditionnelle de \\(Z\\) sachant \\(X\\)?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nComme \\(h(x)\\) ne dépend pas de \\(y\\), on a pour \\(x \\in \\mathbb{R}\\), \\[\\int_{\\mathbb{R}} f(x,y) dy = x h(x) \\mathbb{I}_{\\{x &gt;0\\}} = \\frac{x^a}{\\Gamma(a+1)} \\exp(-x) \\mathbb{I}_{\\{x &gt;0\\}},\\] et on reconna^t là la densité d’une variable \\(\\Gamma(a,1)\\). On a donc \\[\\int_{\\mathbb{R}^2} f(x,y) dx dy = \\int_{\\mathbb{R}_+}  \\frac{x^a}{\\Gamma(a+1)} \\exp(-x) dx = 1.\\]\nNotons \\(T = \\frac{Y}{X}\\), on a pour \\(\\phi : \\mathbb{R}^2 \\to \\mathbb{R}_+\\) borélienne, en utilisant le changement de variables \\(\\Psi : \\begin{cases} & D \\to \\mathbb{R}_+^* \\times (0,1) \\\\ & (x,y) \\to \\left(x, t=\\frac{y}{x}\\right)\\end{cases}\\) de jacobien inverse \\(x\\),\n\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(X,T)] & =  \\int_{\\mathbb{R}^2} \\phi(x,\\frac{y}{x})  \\frac{x^{a-1}}{\\Gamma(a+1)} \\exp(-x) \\mathbb{I}_{\\{0&lt;y&lt;x\\}}dx dy \\\\\n& =  \\int_{\\mathbb{R}^2} \\phi(x,t)  \\frac{x^{a-1}}{\\Gamma(a+1)} \\exp(-x) \\mathbb{I}_{\\{x &gt;0\\}} \\mathbb{I}_{(0,1)}(t) dx dz\n\\end{align*}\\]\net on conclut que \\(X\\) et \\(T\\) sont indépendantes, de lois respectives \\(\\Gamma(a,1)\\), \\(\\mathrm{Unif}[0,1]\\).\n\n\n\n\n\n\n\n\nNoteSolution (suite)\n\n\n\n\nD’après ce qui précède, \\(Y = TX\\), avec \\(T\\) indépendante de \\(X\\), \\(\\sim \\mathrm{Unif}[0,1]\\). Remarquons que si \\(a&gt;0\\), \\(aT \\sim \\mathrm{Unif}[0,a]\\). On déduit que sachant \\(X\\), la loi conditionnelle de \\(Y\\) est \\(\\mathrm{Unif}[0,X]\\).\nOn a en utilisant que \\(\\mathbb{E}[XS \\mid X] = X \\mathbb{E}[S \\mid X]\\), puis l’indépendance de \\(X,U,Z\\),\n\n\\[\\begin{align*}\n\\mathbb{E}[Z \\mid X] & =  \\mathbb{E}[UX \\mid X] + \\mathbb{E}[(1-U)TX \\mid X]\\\\\n& =  X \\mathbb{E}[U] + X \\mathbb{E}[T(1-U)] = X \\mathbb{E}[U] + X \\mathbb{E}[T]\\mathbb{E}[1-U] = \\frac{3}{4} X.\n\\end{align*}\\]\n\n\n\nExercice 23  \n\nSoit \\((X_n, n \\in \\mathbb{N})\\) une suite de v.a.r.i.i.d. de densité \\(f\\) et fonction de répartition \\(F\\). Soient \\(N := \\min\\{ n \\ge 1 : X_n &gt;X_0\\}\\) et\n\\(M := \\min \\{n \\ge 1 : X_0 \\ge X_1 \\ge ... \\ge X_{n-1} &lt;X_n \\}.\\)\n\nTrouver \\(\\mathbb{P}(N=n)\\), puis montrer que la fonction de répartition de \\(X_N\\) est \\(F +(1-F) \\log(1-F)\\) (on pourra conditionner par les événements \\(\\{N=n\\}, n \\in \\mathbb{N}\\)).\nExprimer \\(\\mathbb{P}(M=m), m \\ge 1\\).\nOn suppose dans cette question que \\(f = \\mathbf{1}_{[0,1]}\\). Pour \\(x \\in (0,1)\\) on introduit \\(R^x := \\min\\{ n \\ge 1 : X_1+...+X_n &gt;x \\}\\). Montrer que \\(\\mathbb{E}[\\mathbf{1}_{\\{R^x&gt;n\\}} \\mid X_n] = \\Phi(X_n)\\) où \\(\\Phi(u) = \\mathbb{I}_{\\{u&lt;x\\}}  \\mathbb{P}(R^{x-u} &gt; n-1)\\). En déduire \\(H_n(x):= \\mathbb{P}(R^x&gt;n)\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nPuisque les \\((X_i, i \\ge 1)\\) sont à densité elles sont p.s. toutes distinctes, et puisqu’elles sont i.i.d elles sont échangeables. Autrement dit, pour tout \\(n \\in \\mathbb{N}^*\\), pour tout \\(\\sigma_n\\) permutation de \\(\\{0,\\dots,n\\}\\), \\((X_0,...,X_n)\\) a même loi que \\((X_{\\sigma_n(0)},\\dots,X_{\\sigma_n(n)})\\).\n\nIl s’ensuit que pour tout \\(n \\in \\mathbb{N}^*\\) l’application \\(\\tau_{n-1} : \\{0,...,n-1\\} \\to \\{0,...,n-1\\}\\) telle que $ X_{{n-1}(0)} &gt; X{{n-1}(1)} &gt; &gt; X{_{n-1}(n-1)}$ est uniforme dans les permutations de \\(\\{0, \\dots,n-1\\}\\). En particulier,\n\\[\n\\mathbb{P}(\\max(X_0,...,X_{n-1}) = X_0)   =   \\mathbb{P}(\\tau_n(0) =0 ) = \\frac{1}{n}\n\\]\n\\[\\begin{align*}\n\\mathbb{P}(N=n)\n    & =   \\mathbb{P}(\\max(X_0,...,X_{n-1}) = X_0, \\max(X_0,...,X_n)=X_n) \\\\\n    &=  \\mathbb{P}(\\tau_{n+1}(0) = n, \\tau_{n+1}(1)=0) \\\\\n    & =  \\frac{1}{n(n+1)}.\n\\end{align*}\\]\nPar ailleurs, le maximum de \\((n+1)\\) telles variables i.i.d a pour fonction de répartition \\(F^{n+1}\\). Or, sachant \\(\\{N=n\\}\\), \\(X_N\\) réalise ce maximum, on a donc \\[\\mathbb{P}(X_N &lt; a \\mid N=n)  = F(a)^{n+1} \\]\nIl découle que\n\\[\\mathbb{P}(X_N &lt; a) = \\sum_{n \\in \\mathbb{N}^*}  \\mathbb{P}(N=n)  \\mathbb{P}(X_N &lt; a \\mid N=n) = \\sum_{n \\in \\mathbb{N}^*} \\frac{F(a)^{n+1}}{n(n+1)}.\\]\nOr si \\(y &lt; 1\\),\n\\[y + (1-y) \\log(1-y) = y -(1-y)\\sum_{ n \\ge 1} \\frac{y^n}{n} = y -\\sum_{n \\ge 1} \\frac{y^n}{n}  + \\sum_{n \\ge 1} \\frac{y^{n+1}}{n} = \\sum_{n \\ge 1} \\frac{y^{n+1}}{n(n+1)},\\]\net on vérifie que l’égalité reste vraie si \\(y=1\\). On conclut, comme souhaité, que\n\\[\\sum_{n \\in \\mathbb{N}^*} \\frac{F(a)^{n+1}}{n(n+1)} = F(a) + (1-F(a)) \\log(1-F(a)).\\]\n\n\n\n\n\n\n\n\nNoteSolution (suite)\n\n\n\n\nOn a \\[\\mathbb{P}(M=m) =  \\mathbb{P}(\\tau_m = Id, \\tau_{m+1} \\ne Id) =  \\mathbb{P}(\\tau_m = Id)- \\mathbb{P}(\\tau_{m+1}=Id) = \\frac{1}{m!}- \\frac{1}{(m+1)!} = \\frac{m}{(m+1)!}\\]\nOn a \\(\\{R^x&gt;n\\} = \\{X_1+\\dots+X_n &lt; x\\} = \\{X_1+\\dots +X_{n-1} &lt; x-X_n\\} = \\{X_n &lt;x, R^{x-X_n}&gt;n-1\\}\\). On déduit que sachant \\(X_n\\),\n\n\\[R^x \\begin{cases} & \\le n \\mbox{ si } X_n \\ge x \\mbox{ ou si } X_1+\\dots + X_{n-1} \\ge x-X_n \\\\     &  &lt; n \\mbox{ si } X_n &lt;x, \\mbox{ et } R^{x-X_n} &gt; n-1 \\end{cases}.\\]\nce qui conduit à l’égalité souhaitée gr^ace à EF5.\nOn a donc\n\\[ H_n(x) = \\mathbb{E}[\\Phi(X_n)] = \\int_0^1  \\Phi(u) du =\\int_0^x H_{n-1}(x-u) du = \\int_0^x H_{n-1}(v) dv\\]\net donc \\(H_n\\) est la primitive de \\(H_{n-1}\\) nulle en \\(0\\). Comme \\(H_1(x) =  \\mathbb{P}(X_1 \\le x) = x\\), on conclut que \\(H_n(x) = \\frac{x^n}{n!}\\)\n\n\n\nExercice 24  \n\nSoient \\(X\\) et \\(Y\\) deux v.a.r. indépendantes de loi uniforme sur \\([0, 1]\\).\n\nQuelle est l’espérance conditionnelle de \\((Y - X)_+\\) sachant \\(X\\)?\nQuelle est la loi conditionnelle de \\((Y - X)_+\\) sachant \\(X\\)?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nComme \\((X,Y)\\) est uniforme sur \\([0,1]^2\\), si \\(a \\in [0,1]\\), on a\n\n\\[\\begin{align*}\n\\mathbb{E}[(Y-X)^+ \\mathbb{I}_{\\{X \\le a\\}}] & =  \\int_0^a \\int_0^1 (y-x)^+ dx dy \\\\\n& =  \\int_0^a \\left(\\int_x^1 (y-x) dy\\right) dx \\\\\n& = \\frac{1-(1-a)^3}{6}\n\\end{align*}\\]\nOn cherche donc \\(\\mathbb{E}[(Y-X)^+ \\mid X]\\) sous la forme \\(f(X)\\) avec une fonction \\(f\\) telle que\n\\[\\mathbb{E}[f(X) \\mathbb{I}_{\\{X \\le a\\}}] = \\int_0^a f(u) du = \\frac{1-(1-a)^3}{6},\\]\net donc \\(f(u) = \\frac{(1-u)^2}{2}\\), ce qui permet de conclure que\n\\[\\mathbb{E}[(Y-X)^+ \\mid X] = \\frac{(1-X)^2}{2}.\\]\n\nPour \\(a \\in [0,1]\\), \\((Y-a)^+ = 0 \\mathbb{I}_{\\{Y \\le a\\}} + (Y-a) \\mathbb{I}_{\\{Y &gt; a\\}}\\). De plus, sachant \\(\\{Y&gt;a\\}\\), la loi conditionnelle de \\(Y-a\\) est uniforme sur \\([0,1-a]\\). Autrement dit,\n\n\\[(Y-a)^+ =  \\xi Z\\]\noù \\(\\xi \\sim \\mathrm{Ber}(1-a)\\) indépendante de \\(Z \\sim \\mathrm{Unif}[0,1-a]\\), et si \\(\\phi: \\mathbb{R} \\to \\mathbb{R}_+\\) est borélienne, on a\n\\[\\mathbb{E}[\\phi(Y-a)^+] = a \\phi(0) + \\int_0^{1-a} \\phi(u) du.\\]\nConditionnellement  a \\(X\\), définissons \\(\\xi_X \\sim \\mathrm{Ber}(1-X)\\), indépendamment de \\(Z_X \\sim \\mathrm{Unif}[0,1-X]\\) . Alors d’après ce qui précède,\n\\[\\mathbb{E}[\\phi(Y-X)^+ \\mid X] = X \\phi(0) + \\int_0^{1-X} \\phi(u) du.\\]\nAutrement dit, sachant \\(X\\),\n\\[(Y-X)^+ = \\xi_X Z_X.\\]\n\n\n\nExercice 25  \n\nSoient \\(X_1, X_2, X_3\\) trois v. a. r. gaussiennes centrées réduites indépendantes. On pose \\(U = 2X_1 - X_2 - X_3, V = X_1 + X_2 + X_3, W = 3X_1 + X_2 - 4X_3\\).\n\nQuelles sont les lois de \\(U, V\\) et \\(W\\)? Quels sont les couples de v.a. indépendantes parmi les couples \\((U, V), (U,W), (V,W)\\)?\nMontrer qu’il existe \\(a \\in \\mathbb{R}\\) tel que \\(W = aU + Z\\) avec \\(U\\) et \\(Z\\) indépendantes. En déduire \\(\\mathbb{E}(W \\mid U)\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nOn a\n\n\\[\\begin{pmatrix} U \\\\ V \\\\ W \\end{pmatrix} = \\begin{pmatrix} 2 & -1 & -1 \\\\ 1 & 1 & 1 \\\\ 3 & 1 & -4 \\end{pmatrix} \\begin{pmatrix} X_1 \\\\ X_2 \\\\ X_3 \\end{pmatrix}, \\mbox{ et }  \\begin{pmatrix} 2 & -1 & -1 \\\\ 1 & 1 & 1 \\\\ 3 & 1 & -4 \\end{pmatrix}  \\begin{pmatrix} 2 & -1 & -1 \\\\ 1 & 1 & 1 \\\\ 3 & 1 & -4 \\end{pmatrix}^T = \\begin{pmatrix} 6 & 0 & 9 \\\\ 0 & 3 & 0 \\\\ 9 & 0 & 26 \\end{pmatrix},\\]\nde sorte que \\(\\begin{pmatrix} U \\\\ V \\\\ W \\end{pmatrix} \\sim \\mathcal{N}\\left(\\begin{pmatrix} 0 \\\\ 0 \\\\0 \\end{pmatrix}, \\begin{pmatrix} 6 & 0 & 9 \\\\ 0 & 3 & 0 \\\\ 9 & 0 & 26 \\end{pmatrix} \\right)\\).\nEn particulier \\(U\\) et \\(V\\) sont indépendants, tout comme \\(V\\) et \\(W\\), en revanche \\(U\\) et \\(W\\) ne le sont pas.\n\nPour que \\(W-aU\\) soit indépendant de \\(U\\) il faut et il suffit que \\(\\mathrm{Cov}(W-aU, U) = 9-6a = 0\\) et il faut donc choisir \\(a=\\frac{3}{2}\\). On peut en déduire que sachant \\(U\\), la loi conditionnelle de \\(W\\) est \\(\\mathcal{N}(\\frac{3}{2}U, \\frac{25}{2})\\), et en particuler que \\(\\mathbb{E}[W \\mid U]= \\frac{3}{2}U\\).\n\n\n\n\nExercice 26  \n\nSoient \\(X\\) et \\(Y\\) deux v. a. r. gaussiennes centrées réduites indépendantes. On pose \\(Z = X + Y\\) , \\(W = X - Y\\).\n\nMontrer que \\(Z\\) et \\(W\\) sont indépendantes. Quelle est la loi de \\(W\\)?\nEn déduire l’espérance conditionnelle et la loi conditionnelle de \\(X\\) sachant \\(Z\\).\nCalculer \\(\\mathbb{E}(XY \\mid Z)\\) et \\(\\mathbb{E}(XYZ \\mid Z)\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nOn a ici \\[  \\begin{pmatrix} Z \\\\ W \\end{pmatrix} = \\begin{pmatrix} 1 & 1  \\\\ 1 & -1  \\end{pmatrix} \\begin{pmatrix} U \\\\ V  \\end{pmatrix}, \\mbox{ et }  \\begin{pmatrix} 1 & 1  \\\\ 1 & -1    \\end{pmatrix}\\begin{pmatrix} 1 & 1  \\\\ 1 & -1    \\end{pmatrix} = \\begin{pmatrix}2 & 0 \\\\ 0 &2 \\end{pmatrix},\\] de sorte que \\(\\begin{pmatrix}Z\\\\ W \\end{pmatrix} \\sim \\mathcal{N}\\left(\\begin{pmatrix} 0 \\\\ 0  \\end{pmatrix}, \\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix} \\right)\\). En particulier \\(Z\\) et \\(W\\) sont indépendantes et \\(W \\sim \\mathcal{N}(0,2)\\).\nOn a \\(X = \\frac{Z+W}{2}\\) et donc d’apr es ce qui précède, sachant \\(Z\\), la loi condiitonnelle de \\(X\\) est \\(\\mathcal{N}\\left(\\frac{1}{2}Z, \\frac{1}{2}\\right)\\), et en particulier \\(\\mathbb{E}[X \\mid Z] = \\frac{1}{2}Z\\).\nOn a d’après ce qui précède, et les propriétés de l’espérance conditionnelle\n\n\\[\\begin{align*}\n\\mathbb{E}[XY \\mid Z] & =  \\mathbb{E}[\\frac{Z+W}{2} \\frac{Z-W}{2} \\mid Z] \\\\ & =  \\frac{Z^2}{4} - \\frac{Z \\mathbb{E}[W]}{2} + \\frac{\\mathbb{E}[W^2]}{4} \\\\\n& =  \\frac{Z^2}{4} + \\frac{1}{2}.\n\\end{align*}\\]\nOn déduit que \\[\\mathbb{E}[XYZ \\mid Z] = Z \\mathbb{E}[XY \\mid Z] = \\frac{Z^3}{4} + \\frac{Z}{2}.\\]"
  },
  {
    "objectID": "corriges/td3.html",
    "href": "corriges/td3.html",
    "title": "MA1AY010 Probabilités",
    "section": "",
    "text": "NoteTD III : Processus de branchement\n\n\n\n\n18 Septembre 2025-25 Septembre 2025\nMaster I ISIFAR\nProbabilités\n\n\n\nUn processus de Galton-Watson (processus de branchement homogène) est un processus stochastique en temps discret utilisé pour modéliser l’évolution d’une population où chaque individu se reproduit indépendamment des autres, suivant une même loi de probabilité donnée (appelée loi de reproduction).\nIl a été introduit au XIXᵉ siècle par Francis Galton et Henry Watson our étudier la probabilité d’extinction des noms de famille (nobles). On commence avec une génération initiale (génération \\(0\\)) formée d’un individu. Chaque individu de la génération \\(n\\) engendre un nombre aléatoire de descendants distribué selon la loi de reproduction. Les descendants forment la génération suivante (\\(n+1\\)). Les nombres de descendants des individus de la géneration \\(n\\) forment une famille indépendante. Le nombre d’individus dans la génération \\(n\\) est noté \\(Z_n\\).\nLa question centrale (et angoissante) est : la population s’éteint-elle presque sûrement (\\(Z_n \\to 0\\)) ou bien survit-elle avec une probabilité non nulle ?\nOn note \\(Q\\) la loi de reproduction (loi sur \\((\\mathbb{N}, 2^{\\mathbb{N}})\\). On note \\(\\mu\\) son espérance. On note \\(G_Q\\) la fonction génératrice des probabilités de la loi \\(Q\\).\nOn convient de \\(\\mathbb{N}^* = \\mathbb{N}\\setminus \\{0\\}\\).\n\nExercice 1 (Branchement (Modélisation))  \n\nProposer une formalisation, c’est à dire un univers \\(\\Omega\\), une tribu \\(\\mathcal{F}\\) de parties de \\(\\Omega\\), et une loi de probabilité \\(P\\) sur \\((\\Omega, \\mathcal{F})\\) sur lesquels on peut définir la collection de variables aléatoires \\(Z_0, Z_1, \\ldots, Z_n, \\ldots\\). Préciser la loi conditionnelle de \\(Z_{n+1}\\) sachant \\(\\{ Z_n =k\\},  k \\in \\mathbb{N}\\).\n\n\n\n\n\n\nNoteSolution\n\n\n\nPour chaque génération, on se donne une suite infinie d’entiers, soit un élément de \\(\\mathbb{N}^{\\mathbb{N}^*}\\). L’ensemble \\(\\mathbb{N}^{\\mathbb{N}^*}\\) n’est pas dénombrable.\nPour représenter la suite des générations, on se donne un élément de \\(\\left(\\mathbb{N}^{\\mathbb{N}^*}\\right)^{\\mathbb{N}}\\). Cet ensemble est en bijection avec \\(\\mathbb{N}^{\\mathbb{N}^* \\times \\mathbb{N}}\\). On convient de \\(\\Omega = \\mathbb{N}^{\\mathbb{N}^* \\times \\mathbb{N}}\\). Pour \\(\\omega \\in \\Omega\\), \\(X^n_j(\\omega)\\) est le nombre d’enfants de l’individus \\(j \\in \\mathbb{N}^*\\) dans la génération \\(n \\in \\mathbb{N}\\).\nComme \\(\\Omega\\) n’est pas dénombrable, on ne choisit pas l’ensemble de ses parties comme tribu.\nPour la génération \\(n \\in \\mathbb{N}\\), pour \\(k \\in \\mathbb{N}^*\\), \\(\\mathcal{G}^n_k\\) est la tribu (de parties de \\(\\mathbb{N}^{\\mathbb{N}^*}\\)) engendrée par \\(X^n_{1}, X^n_2, \\ldots, X^n_k\\), et \\(\\mathcal{G}^n\\) est la tribu engendrée par \\(\\left(\\mathcal{G}^n_k\\right)_{k \\in \\mathbb{N}^*}\\). La tribu \\(\\mathcal{G}^n= \\sigma\\left(\\left(\\mathcal{G}^n_k\\right)_{k \\in \\mathbb{N}^*} \\right)\\) est la tribu engendrée par les événements cylindriques décrivant la génération \\(n\\).\nPour chaque \\(n\\), \\(\\mathcal{F}_n = \\sigma \\left( \\cup_{m\\leq n}  \\mathcal{G}^m \\right)\\), la tribu engendrée par les \\((X^m_j)_{m \\leq n, j \\in \\mathbb{N}^*}\\).\nEnfin \\(\\mathcal{F} = \\sigma\\left( \\cup_n \\mathcal{F}_n \\right)\\).\nPour chaque \\(n\\), on munit la \\(n^{\\text{ieme}}\\) génération de la loi produit infinie qui étend les lois produits finies définies sur \\(\\mathcal{G}^n_k\\): \\[P\\left\\{ \\bigwedge_{j \\leq k} X^n_j = x^n_j\\right\\}  = \\prod_{j \\leq k } Q\\left\\{X^n_j = x^n_j \\right\\}\\] pour \\((x^n_j)_{1\\leq j\\leq k} \\in \\mathbb{N}^k\\)\nOn munit \\((\\Omega, \\mathcal{F})\\) de la loi produit infinie qui étend les les lois produits définies sur les \\(n\\) premières générations.\nLa taille des générations \\((Z_n)_{n \\in \\mathbb{N}}\\) est définie récurrence. On a \\(Z_0=1\\) et \\[Z_{n+1} = \\sum_{j=1}^{Z_n} X^n_j\\]\nPour tout \\(n\\), \\(Z_n\\) est \\(\\mathcal{F}_{n-1}\\)-mesurable (vérification par récurrence sur \\(n\\))\nPour tout \\(n\\), \\(\\mathcal{F}_{n-1}\\) est indépendante de \\(\\mathcal{G}^n\\)\n\n\n\nExercice 2 (Branchement, Espérance conditionnelle)  \n\nCalculer l’espérance conditionnelle de \\(Z_{n+1}\\) sachant \\(\\sigma(Z_n)\\)\n\n\n\n\n\n\nNoteSolution\n\n\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[ Z_{n+1} \\mid \\mathcal{F}_{n-1} \\right]\n    & =   \\mathbb{E}\\left[ \\sum_{j=1}^{Z_n} X^n_j \\mid \\mathcal{F}_{n-1} \\right] \\\\\n    & =   \\mathbb{E}\\left[ \\sum_{j=1}^{\\infty} \\mathbb{I}_{j \\leq Z_n} X^n_j \\mid \\mathcal{F}_{n-1} \\right] \\\\\n    & =   \\sum_{j=1}^{\\infty} \\mathbb{I}_{j \\leq Z_n} \\mathbb{E}\\left[  X^n_j \\mid \\mathcal{F}_{n-1} \\right] \\\\\n    & =   \\sum_{j=1}^{\\infty} \\mathbb{I}_{j \\leq Z_n} \\mathbb{E}\\left[  X^n_j \\right] \\\\\n    & =   \\sum_{j=1}^{Z_n} \\mu \\\\\n    & =   \\mu \\times Z_n\n\\end{align*}\\] On peut même conclure:\n\\[\\mathbb{E}\\left[ Z_{n+1} \\mid Z_n  \\right]  = \\mu \\times Z_n\\]\n\n\n\nExercice 3 (Branchement. Espérance taille des générations)  \n\nCalculer \\(\\mathbb{E} Z_n\\) en fonction de \\(\\mu\\) et \\(n\\).\n\n\n\n\n\n\nNoteSolution\n\n\n\n\\[\\mathbb{E} Z_{n+1} = \\mathbb{E}\\left[\\mathbb{E}\\left[ Z_{n+1} \\mid Z_n  \\right] \\right]= \\mu \\times \\mathbb{E} Z_n\\] D’où: \\[\\mathbb{E} Z_n = \\mu^n\\]\n\n\n\n\n\n\n\n\n\nImportantConvention\n\n\n\nSelon la valeur de \\(\\mu\\) (espérance de la loi de reproduction), on distingue trois cas :\n\n\\(\\mu&lt;1\\), cas sous-critique\n\\(\\mu=1\\), cas critique\n\\(\\mu&gt;1\\), cas sur-critique\n\nDans tous les cas, on note \\(p_E\\) la probabilité d’extinction (probabilité de l’événement \\(\\cup_{n \\in \\mathbb{N}} \\{ Z_n =0\\}\\)).\n\n\n\nExercice 4 (Branchement. Cas sous-critique)  \n\nCacluler \\(p_E\\), la probabilité d’extinction dans le cas sous-critique.\n\n\n\n\n\n\nNoteSolution\n\n\n\n\\[E = \\cup_{n} \\{ Z_n =0 \\} = \\cup_n \\cap_{m \\geq n} \\{ Z_m = 0\\}\\]\n\\[\\Omega \\setminus E  = \\cap_n \\{ Z_n &gt; 0\\}\\] Pour chaque \\(k\\) \\[P \\left\\{ \\cap_n \\{ Z_n &gt; 0\\} \\right\\}  \\leq P \\{ Z_k &gt; 0\\}\\]\nComme la suite des événements \\(\\{ Z_n &gt; 0\\}\\) est décroissante,\n\\[P \\left\\{ \\cap_n \\{ Z_n &gt; 0\\} \\right\\}  \\leq   \\lim_n P \\{ Z_n &gt; 0\\}\\]. Comme \\[P \\{ Z_n &gt; 0\\} \\leq \\mathbb{E} Z_n\\] on conclut dans le cas sous-critique \\(\\mu &lt;1\\), que \\[P \\left\\{ \\cap_n \\{ Z_n &gt; 0\\} \\right\\}=0\\] Soit \\[P\\{ E\\} =1\\] (extinction presque sûre).\n\n\n\nExercice 5 (Branchement. Cas sûr-critique)  \n\nMontrer que dans tous les cas, \\(p_E\\) est solution de l’équation \\(G_Q(x)=x \\, .\\)\n\n\n\n\n\n\nNoteSolution\n\n\n\n\\[E = \\cup_{k=1}^\\infty \\{Z_1=k\\} \\cap_{j=1}^k {E_j}\\]\n\\[\\begin{align*}\np_{E}\n  & = \\mathbb{E} \\left[ \\mathbb{E}[  \\mathbb{I}_E  \\mid Z_1]\\right] \\\\\n  & = \\mathbb{E} \\left[ p_E^{Z_1}\\right] \\\\\n  & = G_Q(p_E)\n\\end{align*}\\] car \\(Z_1 \\sim Q\\).\nLa probabilité d’extinction satisfait l’équation: \\[p_E = G_Q(p_E)\\]\nLa Figure 1 illustre le problème posé par l’équation \\(p_E=G_Q(p_E)\\) lorsque la loi de reproduction \\(Q\\) est la loi de Poisson de paramètre \\(\\mu=1.5\\). Les deux intersections entre la courbe en trait plein et la droite pointillée représentent les deux solutions de l’équation \\(p_E = G_Q(p_E)\\).\n\n\n\n\n\n\n\n\nFigure 1: Fonction génératrice de la loi de Poisson de paramètre \\(\\mu=1.5\\): \\(s \\mapsto \\exp(s(\\mu-1))\\). La droite pointillée représente l’équation \\(y=x\\). La droite en tiret l’équation \\(y= 1 + \\mu(x-1)\\)\n\n\n\n\n\n\n\n\n\nExercice 6 (Branchement. Cas sur-critique I)  \n\nÉtudier les solutions de l’équation \\(x=G_Q(x)\\).\n\n\n\n\n\n\nNoteSolution\n\n\n\nComme série entière de rayon de convergence \\(\\geq 1\\), dérivable en \\(1\\), \\(G_Q\\) possède les propriétés suivantes:\n\n\\(G_Q\\) est croissante et convexe sur \\([0,1]\\), indéfiniment dérivable sur \\(]0,1[\\)\nLa dérivée de \\(G_Q\\) en \\(x=1\\) est égale à l’espérance de la loi de reproduction \\(Q\\), soit \\(\\mu\\).\n\nLa fonction \\(x \\mapsto G_Q(x) -x\\) est convexe, de dérivée \\(G'_Q-1\\) sur \\([0,1]\\).\nSi \\(\\mu&lt; 1\\), la dérivée croît jusqu’à \\(0\\) atteint en \\(x=1\\). \\(G_Q(x)-x\\) décroit donc de \\(Q(0)\\) à \\(0\\) entre \\(0\\) et \\(1\\). L’équation \\(x=G_Q(x)\\) ne possède qu’une seule racine (triviale), \\(x=1\\). On retrouve le fait que \\(p_E\\) soit égal à \\(1\\) dans le cas sous-critique.\nSi \\(\\mu &gt; 1\\), \\(G'_Q(0) -1= Q(\\{1\\})-1 &lt; 0\\) et \\(G'_Q(1)-1 = \\mu-1&gt;0\\), \\(G_Q' -1\\) s’annule en un \\(\\theta \\in ]0,1[\\), est négative sur \\([0,\\theta]\\), positive sur \\([\\theta,1]\\). La fonction \\(G_Q(x)-x\\) décroit de \\(0\\) à \\(\\theta\\), croit de \\(\\theta\\) à \\(1\\). Elle est positive en \\(0\\) et nulle en \\(1\\), elle s’annule donc une seule fois en tre \\(0\\) et \\(\\theta\\). Dans le cas sur-critique, l’équation \\(x=G_Q(x)\\) admet une racine non-triviale entre \\(0\\) et \\(1\\).\nPour déterminer \\(p_E\\) dans le cas sur-critique, il faut déterminer la racine de l’équation \\(x=G_Q(x)\\) qui est égale à \\(p_E\\).\n\n\n\nExercice 7 (Branchement. Cas sur-critique II)  \n\nDéterminer \\(p_E\\) dans le cas sur-critique.\n\n\n\n\n\n\nNoteSolution\n\n\n\nPour déterminer \\(p_E\\) dans le cas sur-critique, nous allons étudier la suite \\(\\left(P\\{ Z_n=0\\}\\right)_n\\). C’est une suite croissante, majorée par \\(1\\). Elle possède une limite dans \\([0,1]\\), et \\(p_E = \\lim_{n} \\uparrow P\\{Z_n =0\\}\\).\nOn note \\(P\\{Z_1=0\\} = Q\\{0\\}\\) ou encore \\(P\\{Z_1=0\\} = G_Q(0)\\).\nLa relation entre \\(P\\{Z_n=0\\}\\) et \\(G_Q\\) est (relativement) simple et pas inattendue.\nSi on note \\(G_{Z_n}\\) la fonction génératrice de la loi de \\(Z_n\\),\non a d’abord \\(G_{Z_1}=G_Q\\).\n\\[\\begin{align*}\n\\mathbb{E} \\left[s^{Z_{n+1}} \\mid \\sigma(Z_n) \\right]\n& = \\mathbb{E} \\left[s^{\\sum_{i=1}^{Z_n}  X^n_i} \\mid \\sigma(Z_n) \\right] \\\\\n& = \\mathbb{E} \\left[\\prod_{i=1}^{Z_n} s^{X^n_i} \\mid \\sigma(Z_n) \\right] \\\\\n& = \\prod_{i=1}^{Z_n} \\mathbb{E} \\left[ s^{X^n_i} \\mid \\sigma(Z_n) \\right] \\\\\n& = \\prod_{i=1}^{Z_n} G_Q(s) \\\\\n& = (G_Q(s))^{Z_n} \\, .\n\\end{align*}\\]\nOn en déduit: \\[G_{Z_{n+1}}(s) = \\mathbb{E} \\left[(G_Q(s))^{Z_n}\\right] = G_{Z_n} (G_Q(s))\\]\nD’où (par récurrence sur \\(n\\)) : \\[G_{Z_n} = \\underbrace{G_Q \\circ \\ldots \\circ G_Q}_{n \\text{ fois}}\\]\nComme \\[P\\{ Z_n = 0\\} = G_{Z_n}(0)\\] on a \\[P\\{ Z_n = 0\\} =  \\underbrace{G_Q \\circ \\ldots \\circ G_Q}_{n \\text{ fois}}(0)\\] la suite \\(P\\{ Z_n = 0\\}\\), vérifie la récurrence \\(u_{n+1} = G_Q(u_n)\\) avec \\(u_1=Q\\{0\\}\\).\nNotons \\(\\tau\\) la solution non-triviale de \\(x=G_Q(x)\\).\nSi \\(u \\in [0, \\tau[\\), alors \\[u &lt; G_Q(u) &lt; \\tau\\]\nCette observation permet de déduire que la suite \\((u_n)_n\\) définie par \\(u_{n+1} = G_Q(u_n)\\) et \\(u_1=Q\\{0\\}\\) et croissante et majorée par \\(\\tau\\). Elle admet une limite qui est un point fixe de \\(G_Q\\), c’est donc \\(\\tau\\).\nOn peut donc conclure que dans le cas sur-critique, la probabilité d’extinction est la solution non-triviale de l’équation \\(x=G_Q(x)\\)."
  },
  {
    "objectID": "weeks/week-5.html",
    "href": "weeks/week-5.html",
    "title": "Week 5",
    "section": "",
    "text": "Important\n\n\n\nLa semaine V (6- 9 octobre 2025) est consacrée aux modes de convergence en calcul des probabilités :\n\nConvergence presque sûre, en probabilité, en loi\nLoi(s) des grands nombres\nThéorème central limite\n\nCC 1 : déplacé du mardi 7 septembre au jeudi 9 septembre (durée 1h30 de 15h à 16h30).",
    "crumbs": [
      "Agenda",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-5.html#préparervoirrevoir",
    "href": "weeks/week-5.html#préparervoirrevoir",
    "title": "Week 5",
    "section": "Préparer/Voir/Revoir",
    "text": "Préparer/Voir/Revoir\n\n\nLoi faible des grands nombres\nThéorème central limite\n\n\n\n\nConvergence en probabilité/presque sûre et loi des grands nombres\nConvergence en loi et TCL",
    "crumbs": [
      "Agenda",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-5.html#exercices",
    "href": "weeks/week-5.html#exercices",
    "title": "Week 5",
    "section": "Exercices",
    "text": "Exercices\n\nFeuille TD V\nCorrections TD II\nCorrections TD IV\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Agenda",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-3.html",
    "href": "weeks/week-3.html",
    "title": "Week 3",
    "section": "",
    "text": "Important\n\n\n\nLa semaine III (22-26 septembre 2025) est consacrée aux calculs de moments, et au conditionnement.",
    "crumbs": [
      "Agenda",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#préparervoirrevoir",
    "href": "weeks/week-3.html#préparervoirrevoir",
    "title": "Week 3",
    "section": "Préparer/Voir/Revoir",
    "text": "Préparer/Voir/Revoir\n\n\nEspérances, moments, Espaces \\(L^p\\) (2025-09-22)\nConditionnement (2025-09-23 et 2025-09-25)\n\n\n\n\nVoir Notes, Espérances, Moments, Espaces \\(L^p\\), …\nVoir Notes, Conditionnement",
    "crumbs": [
      "Agenda",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#exercices",
    "href": "weeks/week-3.html#exercices",
    "title": "Week 3",
    "section": "Exercices",
    "text": "Exercices\n\nFeuille TD II\nFeuille TD II supplément\nQuestionnaire I\nFeuille TD III Branchement\n\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Agenda",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-1.html",
    "href": "weeks/week-1.html",
    "title": "Semaine 1",
    "section": "",
    "text": "Important\n\n\n\nLa semaine I (8-12 septembre 2025) est consacrée aux révisions de Licence. On (re)-présentera beaucoup de définitions, quelques outils essentiels, et on tentera d’utiliser ces définitions et outils sur la liste d’exercices des feuilles de TD.",
    "crumbs": [
      "Agenda",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#préparerrevoir",
    "href": "weeks/week-1.html#préparerrevoir",
    "title": "Semaine 1",
    "section": "Préparer/Revoir",
    "text": "Préparer/Revoir\n\nEspace probabilisable\n\nTribu/\\(\\sigma\\)-algèbre\nTribu borélienne\nTribu de Lebesgue\nLemme des classes monotones\n\nLoi de probabilité, mesure positive, \\(\\sigma\\)-additivité\nFonctions mesurables et variables aléatoires\nEspérance, variance, moments\nThéorèmes de convergence (monotone, Fatou, dominée)\nIndépendance (Évenements, Tribus)",
    "crumbs": [
      "Agenda",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#exercices",
    "href": "weeks/week-1.html#exercices",
    "title": "Semaine 1",
    "section": "Exercices",
    "text": "Exercices\nFeuille TD I\nCorrigés :\n\n1 (transformation affine),\n2 (minima, maxima),\n10 (transformées de Laplace),\n11 (densité de la loi du carré),\n17 (somme de Poisson indépendantes, conditionnement discret),\n18 (sommes aléatoires de variables indépendantes).",
    "crumbs": [
      "Agenda",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#exercices-supplémentaires",
    "href": "weeks/week-1.html#exercices-supplémentaires",
    "title": "Semaine 1",
    "section": "Exercices supplémentaires",
    "text": "Exercices supplémentaires\nFeuille TD I suppléments\n\n\nRetour à l’agenda ⏎",
    "crumbs": [
      "Agenda",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-2.html",
    "href": "weeks/week-2.html",
    "title": "Semaine 2",
    "section": "",
    "text": "Important\n\n\n\nLa semaine II (15-19 septembre 2025) est encore consacrée aux révisions de Licence.",
    "crumbs": [
      "Agenda",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#préparerrevoir",
    "href": "weeks/week-2.html#préparerrevoir",
    "title": "Semaine 2",
    "section": "Préparer/Revoir",
    "text": "Préparer/Revoir\n\n\nEspaces produits, tribus produits (2025-09-15)\nLois produits (2025-09-15)\nThéorème de Fubini (2025-09-15)\nCalculs de densités image, formules de changement de variable (2025-09-16)\nEspérances, moments, Espaces \\(L^p\\) (2025-09-16)\nConditionnement discret (2025-09-18)\nProcessus de branchement aléatoire (2025-09-18)\nEnregistrement du cours du 18 septembre\n\n\n\n\nVoir Notes, Espaces produits, etc\nVoir Notes, calculs de densité image\nVoir Notes, Fonctions génératrices\nVoir Notes, Conditionnement discret\nVoir Notes, Espérances, Moments, Espaces \\(L^p\\), …",
    "crumbs": [
      "Agenda",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#exercices",
    "href": "weeks/week-2.html#exercices",
    "title": "Semaine 2",
    "section": "Exercices",
    "text": "Exercices\nFeuille TD I Feuille TD I suppléments\n\nTD I.18 (sommes aléatoires de variables indépendantes) (fin).\nSupplément TD I Exercice 5 (Anniversaires)\nTD I.7\nTD I….\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Agenda",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-4.html",
    "href": "weeks/week-4.html",
    "title": "Week 4",
    "section": "",
    "text": "Important\n\n\n\nLa semaine IV (29 septembre- 2 octobre 2025) est consacrée aux caractérisations des lois de probabilités (Transformées de Laplace et de Fourier).",
    "crumbs": [
      "Agenda",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#préparervoirrevoir",
    "href": "weeks/week-4.html#préparervoirrevoir",
    "title": "Week 4",
    "section": "Préparer/Voir/Revoir",
    "text": "Préparer/Voir/Revoir\n\n\nTransformée de Laplace\nTransformée de Fourier\n\n\n\n\nVoir Transformée de Laplace …\nVoir Transformée de Fourier …",
    "crumbs": [
      "Agenda",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#exercices",
    "href": "weeks/week-4.html#exercices",
    "title": "Week 4",
    "section": "Exercices",
    "text": "Exercices\n\nFeuille TD IV\nCorrections TD I\nCorrections TDIII\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Agenda",
      "Week 4"
    ]
  },
  {
    "objectID": "course-faq.html",
    "href": "course-faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Pas nécessairement.",
    "crumbs": [
      "Informations",
      "FAQ"
    ]
  },
  {
    "objectID": "course-faq.html#les-corrigés-des-exercices-sont-ils-disponibles-en-ligne",
    "href": "course-faq.html#les-corrigés-des-exercices-sont-ils-disponibles-en-ligne",
    "title": "FAQ",
    "section": "",
    "text": "Pas nécessairement.",
    "crumbs": [
      "Informations",
      "FAQ"
    ]
  },
  {
    "objectID": "course-faq.html#comment-travailler-avec-les-cahiers-de-simulation",
    "href": "course-faq.html#comment-travailler-avec-les-cahiers-de-simulation",
    "title": "FAQ",
    "section": "Comment travailler avec les cahiers de simulation ?",
    "text": "Comment travailler avec les cahiers de simulation ?\n\nEn R\n\nTélécharger et installer R 4.x.y: https://cran.r-project.org/\nTélécharger et installer RStudio: https://dailies.rstudio.com/\nInstaller Quarto CLI: https://quarto.org/docs/getting-started/installation.html\nInstaller Git: https://happygitwithr.com/install-git.html\nInstaller tout package utile avec install.packages(\"___\") (voir aussi le package pak)\n\n\n\nEn Python",
    "crumbs": [
      "Informations",
      "FAQ"
    ]
  },
  {
    "objectID": "course-links.html",
    "href": "course-links.html",
    "title": "Quelques liens",
    "section": "",
    "text": "Entrepot GitHub du cours\n🔗 sur GitHub\n\n\nEntrepot GitHub des Notes de cours\n🔗 sur GitHub\n\n\nPortail Moodle du cours\n🔗 sur Moodle",
    "crumbs": [
      "Informations",
      "Liens"
    ]
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Cliquer ici pour télécharger une copie PDF du syllabus.",
    "crumbs": [
      "Informations",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#informations",
    "href": "course-syllabus.html#informations",
    "title": "Syllabus",
    "section": "Informations",
    "text": "Informations\n\n\n\n\n\n\n\n\n\n\nJour\nHoraire\nSalle\n\n\n\n\nCours/Exercices\nLundi\n13:30 - 16:30\nSophie Germain 1015\n\n\nExercices\nMardi\n8:30 - 10:30\nHalle aux Farines 227C\n\n\nCours/Exercices\nJeudi\n13:30 - 16:30\nSophie Germain 1015",
    "crumbs": [
      "Informations",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#objectifs",
    "href": "course-syllabus.html#objectifs",
    "title": "Syllabus",
    "section": "Objectifs",
    "text": "Objectifs\nA la fin du premier semestre, vous saurez…\n\nDéfinir, caractériser une loi de probabilité sur un espace simple.\nÉtablir et utiliser les différents types de convergence de variables aléatoires.\nManipuler les espérances conditionnelles.\nManipuler les vecteurs gaussiens (en dimension finie).",
    "crumbs": [
      "Informations",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#bibliographie",
    "href": "course-syllabus.html#bibliographie",
    "title": "Syllabus",
    "section": "Bibliographie",
    "text": "Bibliographie\nBerger, Q., Caravenna, F., & Dai Pra, P. (2021). Probabilità (Vol. 9). Springer.\n\nEn italien Via ENT Paris Cité\nEn français Via ENT Paris Cité\n\nGrimmett, G., & Stirzaker, D. (2020). One thousand exercises in probability. Oxford University Press.",
    "crumbs": [
      "Informations",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#cours-et-exercices",
    "href": "course-syllabus.html#cours-et-exercices",
    "title": "Syllabus",
    "section": "Cours et exercices",
    "text": "Cours et exercices\n\nNotes de cours (~ Poly)\nExercices",
    "crumbs": [
      "Informations",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#validation",
    "href": "course-syllabus.html#validation",
    "title": "Syllabus",
    "section": "Validation",
    "text": "Validation\n\nContrôle continu\n4 épreuves (Examen terminal inclus)\n\n\nExamens\nL’examen terminal de session I dure trois heures, il porte sur l’ensemble du cours",
    "crumbs": [
      "Informations",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#intégrité",
    "href": "course-syllabus.html#intégrité",
    "title": "Syllabus",
    "section": "Intégrité",
    "text": "Intégrité\nTL;DR: Ne trichez pas!",
    "crumbs": [
      "Informations",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#sauvez-les-dates",
    "href": "course-syllabus.html#sauvez-les-dates",
    "title": "Syllabus",
    "section": "Sauvez les dates!",
    "text": "Sauvez les dates!\n\nContrôle Continu 1 Jeudi 9 octobre 2025 Mardi 7 octobre 2025 (Programme : semaines I à III)\nContrôle Continu 2 Jeudi 16 octobre 2025 (Programme : semaines I à V)\nContrôle Continu 3 Novembre\nExamen entre le Lundi 8 et le Vendredi 19 décembre\nExamen session II en juin 2026\n\nCliquer ici for l’emploi du temps.",
    "crumbs": [
      "Informations",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "MA1AY010 : Enseignants",
    "section": "",
    "text": "S. Boucheron est professeur dans l’Équipe de Statistique du Laboratoire de Probabilités, Statistiques et Modélisation (Université Paris Cité, Sorbonne Université, CNRS).\n\n\n\n\n\n\n\nConsultations\nBureau\n\n\n\n\nLundi 11:00 am - 12:00 pm\nSophie Germain 5013\n\n\n\nS. Abbes est professeur à l’IRIF (Université Paris Cité, CNRS).",
    "crumbs": [
      "Informations",
      "Enseignants"
    ]
  },
  {
    "objectID": "course-team.html#instructeurs",
    "href": "course-team.html#instructeurs",
    "title": "MA1AY010 : Enseignants",
    "section": "",
    "text": "S. Boucheron est professeur dans l’Équipe de Statistique du Laboratoire de Probabilités, Statistiques et Modélisation (Université Paris Cité, Sorbonne Université, CNRS).\n\n\n\n\n\n\n\nConsultations\nBureau\n\n\n\n\nLundi 11:00 am - 12:00 pm\nSophie Germain 5013\n\n\n\nS. Abbes est professeur à l’IRIF (Université Paris Cité, CNRS).",
    "crumbs": [
      "Informations",
      "Enseignants"
    ]
  },
  {
    "objectID": "exercices-listings.html",
    "href": "exercices-listings.html",
    "title": "TDs",
    "section": "",
    "text": "Note\n\n\n\nLes séances d’exercice sont organisées autour des feuilles de TD. Sentez vous libres d’attaquer les exercices à l’avance.\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Titre\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Tags\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitre\n\n\n\nTags\n\n\n\n\n\n\n\n\nSep 8, 2025\n\n\nTD I : Rappels de Licence\n\n\nVariables aléatoires réelles\n\n\n\n\n\n\nSep 11, 2025\n\n\nTD I supplément : Rappels de Licence\n\n\nVariables aléatoires réelles, Branchement\n\n\n\n\n\n\nSep 22, 2025\n\n\nTD II : Conditionnement\n\n\nConditionnement, Branchement\n\n\n\n\n\n\nSep 22, 2025\n\n\nTD II : Conditionnement (supplément)\n\n\nConditionnement, Branchement\n\n\n\n\n\n\nSep 22, 2025\n\n\nQuestionnaire I\n\n\n \n\n\n\n\n\n\nSep 22, 2025\n\n\nTD III : Branchement\n\n\nConditionnement, Branchement\n\n\n\n\n\n\nSep 29, 2025\n\n\nTD IV : Conditionnement\n\n\nConditionnement\n\n\n\n\n\n\nOct 6, 2025\n\n\nTD V : Convergences/Caractérisations\n\n\nConvergences, Caractérisations, LGN, TCL\n\n\n\n\n\n\nOct 13, 2025\n\n\nTD VI : Gaussiennes\n\n\nGaussiennes\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nTipSuggestion",
    "crumbs": [
      "TDs"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA15Y010 Probabilités et Extrêmes",
    "section": "",
    "text": "Cette page résume le déroulement du semestre\n\n\n\n\n\n\n\n\n\n\n\n\n\nSemaine\nDate\nThème(s)\nPréparer\nCours\nExercices\nÉvaluation\n\n\n\n\n1\n8 Sep 2025\nRévisions Licence\n\n\nTD 1\n\n\n\n\n9 Sep\nRévisions Licence\n\n\nTD 1\n\n\n\n\n11 Sep\nRévisions Licence: théoremes de convergence (intégration)\n\n\nTD 1\n\n\n\n2\n15 Sep\nRévisions Licence: Espaces roduits\n\n\n\n\n\n\n\n16 Sep\nChangement de variables. Calcul Gamma/Beta\n\n\n\n\n\n\n\n18 Sep\nFonctions génératrices/Conditionnement discret\n\n\n\n\n\n\n3\n22 Sep\nMoments, Espaces \\(L^p\\)\n\n\nTD 3\n\n\n\n\n23 Sep\nEspérance conditionnelle\n\n\n\n\n\n\n\n25 Sep\nConditionnement\n\n\nTD 2 TD 2 supplément\n\n\n\n4\n29 Sep\nCaractérisations\n\n\nTD4\n\n\n\n\n30 Sep\nCaractérisations\n\n\nCorrections TD1\n\n\n\n\n2 Oct\nCaractérisations\n\n\nCorrections TD3\n\n\n\n5\n6 Oct\nConvergences\n\n\nTD 5\n\n\n\n\n7 Oct\nConvergences\n\n\n\n\n\n\n\n9 Oct\nConvergences et CC 1\n\n\n\n\n\n\n6\n13 Oct\nConvergences\n\n\n\n\n\n\n\n14 Oct\nVecteurs gaussiens\n\n\nTD 6\n\n\n\n\n16 Oct\nVecteurs gaussiens + CC 2",
    "crumbs": [
      "Informations",
      "Agenda"
    ]
  },
  {
    "objectID": "exercices/td4.html",
    "href": "exercices/td4.html",
    "title": "MA1AY010 Probabilités",
    "section": "",
    "text": "NoteTD IV : Espérance conditionnelle/Catactérisations\n\n\n\n\n29 Septembre 2025-2 octobre 2025\nMaster I ISIFAR\nProbabilités\n\n\n\n\n\n\n\n\n\nNoteConventions\n\n\n\nDans les 3 exercices qui suivent, \\(X_1, \\ldots, X_n, ...\\) constituent une famille indépendante de variables aléatoires identiquement distribuées à valeur dans \\(\\{-1,1\\}\\).\nL’univers des possibles est \\(\\Omega = \\{-1,1\\}^{\\mathbb{N}}\\). Les \\(X_i\\) sont les projections canoniques.\nOn note \\(\\mathcal{F}_n\\) la tribu engendrée par les \\(n\\) premières coordonnées :\n\\[\n\\mathcal{F}_n =  \\sigma(X_1, \\ldots, X_n) \\,\n\\]\nL’univers est muni de la tribu des cylindres \\(\\mathcal{F} = \\sigma\\left(\\bigcup_n \\mathcal{F}_n\\right)\\).\nOn note \\(\\Delta\\) une constante à valeur dans \\((0,1)\\) (la dérive de la marche aléatoire).\nOn note \\(\\mathbb{P}\\) la loi produit infini, telle que pour tout \\(x \\in \\{-1, 1\\}^n\\)\n\\[\n\\mathbb{P}\\left\\{\\bigwedge_{i=1}^n X_i = x_i\\right\\} = \\prod_{i=1}^n \\frac{1}{2}\\left(1 + x_i \\Delta\\right)\n\\]\nOn étudie la marche alétoire sur \\(\\mathbb{Z}\\) de dérive \\(\\Delta\\).\nOn note \\(S_n = \\sum_{i=1}^n X_i\\).\nL’indice \\(n\\) représente le temps, \\(S_n\\) la position à l’instant \\(n\\).\n\n\n\nExercice 1 (Marches aléatoires biaisées i)  \n\n\nQuelle est la loi de \\(S_n\\) ?\n\\(S_n\\) est-elle \\(\\mathcal{F}_n, \\mathcal{F}_{n-1}, \\mathcal{F}_{n+1}\\) mesurable ?\nQuelle est l’espérance de \\(S_n\\) ?\nQuelle est la variance de \\(S_n\\) ?\n\n\n\n\n\n\n\nNoteOn admettra l’inégalité de Hoeffding:\n\n\n\nSi \\(Y_1, \\ldots, Y_n\\) sont des variables aléatoires indépendantes telles que \\(a_i \\leq Y_i \\leq b_i\\) (les \\(Y_i\\) sont bornées), alors\n\\[\nP \\left\\{  Z - \\mathbb{E} Z \\geq t  \\right\\} \\leq \\mathrm{e}^{- 2 \\frac{t^2}{\\sum_{i=1}^n (b_i-a_i)^2}}\n\\]\navec \\(Z = \\sum_{i=1}^n Y_i\\).\n\n\n\nExercice 2 (Marches aléatoires biaisées ii)  \n\nPour \\(0 \\leq \\tau \\leq  n \\Delta\\),\n\nMajorer \\(\\mathbb{P}\\{ S_n \\leq \\tau \\}\\) à l’aide de l’inégalité de Chebyshev\nMajorer \\(\\mathbb{P}\\{ S_n \\leq \\tau \\}\\) à l’aide de l’inégalité de Hoeffding\nL’ensemble \\[\nE = \\left\\{ \\omega :  \\forall n,  S_n(\\omega) &lt; \\tau \\right\\}\n\\] appartient-il à la tribu \\(\\mathcal{F}_m\\) pour un \\(m\\) donné ? est-il un événement de \\(\\mathcal{F}\\) ?\nSi \\(E\\) est un événement, quelle est sa probabilité ?\n\n\n\n\n\n\n\nNoteConvention\n\n\n\nOn suppose \\(\\tau \\in \\mathbb{N} ∖  \\{0\\}\\).\nOn note \\(T = \\inf \\left\\{ n : S_n \\geq \\tau \\right\\}\\). Si \\(\\forall n, \\quad S_n(\\omega)&lt; \\tau\\), alors \\(T(\\omega) = \\infty\\).\nOn note \\(S_T\\), la fonction définie par \\[\nS_T(\\omega) = \\sum_{n=1}^\\infty \\mathbb{I}_{T(\\omega)=n} S_n(\\omega)\\qquad \\text{si } T(\\omega) &lt; \\infty\n\\] et \\(S_T(\\omega) =0\\) si \\(T(\\omega) =\\infty\\).\n\n\n\nExercice 3 (Marches aléatoires biaisées iii)  \n\n\nPourquoi peut-on considérer que \\(T\\) est une variable aléatoire (à valeur dans \\(\\mathbb{N} \\cup \\{\\infty\\}\\)) ?\nQuelle est la probabilité que \\(T = \\infty\\) ?\nL’événement \\(\\{ T \\leq n \\}\\) est-il \\(\\mathcal{F}_{n-1}, \\mathcal{F}_n, \\mathcal{F}_{n+1}\\) mesurable ?\nPourquoi peut-on considérer que \\(S_T\\) est une variable aléatoire ?\nQuelle est l’espérance de \\(S_T\\) ?\nMontrer que \\(\\mathbb{E} S_T = \\Delta \\mathbb{E} T\\) En déduire \\(\\mathbb{E} T\\).\n\n\nExercice 12 (Binomiale négative) Les variables \\(X_1, :::\nX_2, \\ldots, X_n, \\ldots\\) sont des variables de Bernoulli de probabilité de succès \\(p \\in (0,1)\\), indépendantes. On définit \\(T_1 = \\min \\{i : X_i=1\\}\\) (temps du premier succès), \\(T_2 = \\min \\{i : i &gt; T_1, X_i=1\\}\\) (temps du premier succès après \\(T_1\\)), et récursivement \\(T_{n+1} = \\min \\{ i : i &gt; T_n, X_i =1\\}\\) (temps du \\(n+1\\)eme succès).\nOn admet l’existence d’un espace de probabilité \\((\\Omega, \\mathcal{F}, P)\\) où \\(\\Omega = \\{0,1\\}^{\\mathbb{N}}\\), \\(\\mathcal{F}\\) est une tribu pour laquelle les \\(X_i\\) sont mesurables, et \\(P\\) tel que \\(X_1, \\ldots, X_n, \\ldots\\) est une famille indépendante.\n\n\\(T_1\\) et plus généralement \\(T_n\\) sont-elles des variables aléatoires?\nCalculer \\(P \\{ T_1 &gt; k  \\}\\) pour \\(k \\in \\mathbb{N}\\).\nCalculer \\(P \\{ T_1 = k  \\}\\) pour \\(k \\in \\mathbb{N}\\)\nCalculer \\(\\mathbb{E}T_1\\).\nCalculer \\(P \\{ T_1 = k  \\wedge T_2 = k+j\\}\\) pour \\(k, j \\in \\mathbb{N}\\)\nCalculer \\(P \\{ T_2 = k  \\}\\) pour \\(k \\in \\mathbb{N}\\)\nCalculer \\(\\mathbb{E}T_2\\)\nCalculer \\(\\mathbb{E} T_n\\)\n\n\nExercice 4 (Allocations aléatoires)  \n\nOn dispose de \\(n\\) urnes numérotées de \\(1\\) à \\(n\\) et de \\(n\\) boules. Les boules sont réparties de manière uniforme dans les urnes (chaque boule se comporte de manière indépendante des autres et a probabilité \\(1/n\\) de tomber dans chaque urne). On note \\(U_i\\) la variable aléatoire désignant le nombre de boules qui tombent dans l’urne \\(i\\). Dans la suite \\(\\alpha &gt;1\\) est un réel.\n\nDéterminer la loi de \\(U_i\\).\nMontrer que l’on a : \\[\\mathbb{P}( \\max_{1 \\leq i \\leq n} U_i &gt; \\alpha \\ln n) \\leq n \\mathbb{P}( U_1 &gt; \\alpha \\ln n).\\]\nCalculer \\(\\mathbb{E}(\\exp(U_1))\\).\nMontrer que pour tout \\(\\beta &gt; -n\\), on a \\((1+\\beta/n)^n \\leq \\exp(\\beta)\\).\nMontrer que \\(\\mathbb{P}(U_1 &gt; \\alpha \\ln n) \\leq \\frac{\\exp(\\exp(\\alpha)-1)}{n^\\alpha}\\).\nEn déduire que si \\(\\alpha &gt;1\\), on a \\[\\mathbb{P}( \\max_{1 \\leq i \\leq n} U_i &gt; \\alpha \\ln n) \\rightarrow_{n \\rightarrow \\infty} 0.\\]\n\n\nExercice 5 (Restitution Organisée de Connaissances)  \n\n\nSoient \\(A, B, C\\) trois événements dans un espace probabilisé. A-t-on toujours: \\(A \\perp\\!\\!\\!\\perp B \\text{ et } B \\perp\\!\\!\\!\\perp C \\Rightarrow A \\perp\\!\\!\\!\\perp C\\)?\nSoient \\(P\\) et \\(Q\\) deux lois de probabilités sur \\((\\Omega, \\mathcal{F})\\), on définit l’ensemble \\(\\mathcal{M} = \\big\\{ A : A \\in \\mathcal{F}, P(A)=Q(A)\\}\\). Répondre par vrai/faux/je ne sais pas aux questions suivantes:\n\n\\(\\mathcal{M}\\) est-il toujours une classe monotone ?\n\\(\\mathcal{M}\\) est-il toujours une \\(\\sigma\\)-algèbre ?\n\\(\\mathcal{M}\\) est-il toujours une \\(\\pi\\)-classe ?\n\nSoient \\(G\\) et \\(F\\) sont deux fonctions génératrices de probabilité. Répondre par vrai/faux aux questions suivantes:\n\nEst-il vrai que \\(G \\times F\\) est toujours une fonction génératrice ?\nEst-il vrai que \\(G + F\\) est toujours une fonction génératrice de probabilité ?\nEst-il vrai que \\(\\lambda G + (1-\\lambda) F\\) avec \\(\\lambda \\in [0,1]\\) est toujours une fonction génératrice de probabilité ?\n\nSi \\(\\widehat{F}\\) est la fonction caractéristique de la loi de \\(X\\), et si \\(\\epsilon \\perp\\!\\!\\!\\perp X\\), avec \\(P\\{\\epsilon=1\\}= P\\{\\epsilon=-1\\}=1/2\\), quelle est la fonction caractéristique de la loi de \\(\\epsilon X\\)?\n\n\nExercice 6 (Distributions biaisées par la taille)  \n\nSi \\(X\\) est une variable aléatoire positive intégrable, la version biaisée par la taille de \\(X\\) est la variable aléatoire \\(X^*\\) dont la loi \\(Q\\) est absolument continue par rapport à celle de \\(X\\) (notée \\(P\\)) et dont la densité (par rapport à celle de \\(X\\)) est proportionnelle à \\(X\\): \\[\n\\frac{\\mathrm{d}Q}{\\mathrm{d}P}(x) = \\frac{x}{\\mathbb{E}X} \\, .\n\\]\n\nCaractériser \\(X^*\\) lorsque \\(X\\) est une Bernoulli.\nCaractériser \\(X^*\\) lorsque \\(X\\) est binomiale.\nCaractériser \\(X^*\\) lorsque \\(X\\) est Poisson.\nCaractériser \\(X^*\\) lorsque \\(X\\) est Gamma.\nSi \\(X\\) est à valeurs entières, exprimer la fonction génératrice de \\(X^*\\) en fonction de celle de \\(X\\).\nExprimer la transformée de Laplace de \\(X^*\\) en fonction de celle de \\(X\\).\nSi \\(U\\) est une transformée de Laplace, dérivable à droite en \\(0\\), \\(U'/U'(0)\\) est-elle la transformée de Laplace d’une loi sur \\([0, \\infty)\\)?\n\n\nExercice 7 (Amies des gaussiennes)  \n\n\n\n\n\n\n\nNoteRappel\n\n\n\nLa loi normale centrée réduite \\(\\mathcal{N}(0,1)\\) admet pour densité \\(x\\mapsto \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)\\),\n\n\n\nSi \\(X \\sim \\mathcal{N}(0,1)\\), donner une densité de la loi de \\(Y=\\exp(X)\\) (Loi log-normale). Calculer l’espérance et la variance de \\(Y\\).\nMême question si \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\).\nSi \\(X, Y \\sim \\mathcal{N}(0,1)\\), avec \\(X \\perp\\!\\!\\!\\perp Y\\), donner une densité de la loi de \\(Z=Y/X\\) (Loi de Student à 1 degré de liberté)\nSi \\(X, Y \\sim \\mathcal{N}(0,1)\\), avec \\(X \\perp\\!\\!\\!\\perp Y\\), donner une densité de la loi de \\(W = Y/ \\sqrt{X^2}\\).\nSi \\(X \\sim \\mathcal{N}(0,1)\\) et \\(\\epsilon\\) vaut \\(\\pm 1\\) avec probabilité \\(1/2\\) (variable de Rademacher) avec \\(X \\perp\\!\\!\\!\\perp \\epsilon\\), donner une densité de la loi de \\(Y = \\epsilon X\\). \\(Y\\) et \\(X\\) sont-elles indépendantes ?\n\n\nExercice 8  \n\n\nExercice 9 (Principe de réflexion)  \n\nPrincipe de réflexion\nDans cet exercice, \\(X_1, X_2, \\ldots\\) sont des variables de Rademacher indépendantes (\\(P\\{X_i = \\pm 1\\} = \\frac{1}{2}\\)), \\(S_n =\\sum_{i=1}^n X_i, S_0=0\\) et \\(M_n = \\max_{k \\leq n} S_n\\).\nMontrer que, pour \\(a&gt; 0\\),\n\\[P\\left\\{ M_n &gt; a \\right\\}\\leq 2 P\\left\\{ S_n &gt; a \\right\\}\\]\n\n\n\n\n\n\nNoteStatistique des rangs/Statistiques d’ordre\n\n\n\nLes statistiques d’ordre \\(X_{1:n}\\leq X_{2:n}\\leq X_{n:n}\\) d’un \\(n\\)-échantillon \\(X_1,\\ldots,X_n\\) d’observations indépendantes identiquement distribuées sont formées par le réarrangement croissant (convention) de l’échantillon.\nQuand \\(n\\) est clair d’après le contexte on peut les noter \\(X_{(1)} \\leq \\ldots \\leq X_{(n)}\\).\n\n\n\nExercice 11 (Statistiques d’ordre)  \n\nVérifier que la l::: oi jointe des statistiques d’ordre est absolument continue par rapport à la loi de l’échantillon.\nOn suppose que \\(X\\) est une variable aléatoire réelle, absolument continue, de densité continue. Montrer que l’échantillon est presque sûrement formé de valeurs deux à deux distinctes.\nDonner la densité de la loi jointe des statistiques d’ordre.\nSi la loi des \\(X_i\\) définie par sa fonction de répartition \\(F\\), admet une densité \\(f\\), quelle est la densité de la loi de \\(X_{k:n}\\) pour \\(1\\leq k\\leq n\\) ?\n\nMontrer que conditionnellement à \\(X_{k:n}=x\\), la suite\n\\[(X_{i:n}-X_{k:n})_{i=k+1,\\ldots, n}\\]\nest distribuée comme les statistiques d’ordre d’un \\(n-k\\) échantillon de la loi d’excès au dessus de \\(x\\) (fonction de survie \\(\\overline{F}(x+\\cdot)/\\overline{F}(x))\\) avec la convention \\(\\overline{F}=1-F\\)).\n\n(Représentation de Rényi)\n\nExercice 10 (Statistiques d’ordre d’un échantillon exponentiel) Cet exercice reprend::: les conventions de l’exercice précédent. On s’intéresse maintenant aux statistiques d’ordre d’un échantillon exponentiel.\n\nSi \\(X_1,\\ldots,X_n\\) est un échantillon i.i.d. de la loi exponentielle d’espérance \\(1\\) (densité \\(\\mathbb{I}_{x&gt;0} \\mathrm{e}^{-x}\\)), et \\(X_{n:n}\\geq X_{n-1:n}\\geq  \\ldots \\geq X_{1:n}\\) les statistiques d’ordre associées, montrer que:\navec la convention \\(X_{0:n}=0\\), les écarts \\((X_{i:n}-X_{i-1:n})_{1\\leq i\\leq n}\\) (spacings) forment une collection de variables aléatoires indépendantes ;\n\\(X_{i:n}-X_{i-1:n}\\) est distribuée selon une loi exponentielle d’espérance \\(\\tfrac{1}{i}\\) .\nMaintenant \\((k_n)_n\\) est une suite croissante d’entiers qui tend vers l’infini, telle que \\(k_n/n\\) tend vers une limite finie (éventuellement nulle). Montrer que\n\\[\\frac{X_{k_n:n} -\\mathbb{E} X_{k_n:n} }{\\sqrt{\\operatorname{var}(X_{k_n:n} )}}\\]\nconverge en loi vers une Gaussienne centrée réduite."
  },
  {
    "objectID": "exercices/td5-bis.html",
    "href": "exercices/td5-bis.html",
    "title": "MA1AY010 Probabilités",
    "section": "",
    "text": "NoteTD V : Caractérisations/Convergences\n\n\n\n\nOctobre 2025\nMaster I ISIFAR\nProbabilités\n\n\n\n\nExercice 1  \n\nOn définit la densité \\(f_a\\) par \\(f_a(x) = \\frac{a}{\\pi(x^2 + a^2)}\\) pour \\(a&gt;0\\). Si \\(X \\sim f_a, Y \\sim f_b, X \\perp\\!\\!\\!\\perp Y\\) quelle est la densité de la loi de \\(X+Y\\)?\nSuggestion: passez par les fonctions caractéristiques.\n\nExercice 2 (Marche symétrique arrêtée)  \n\nOn considère une suite de variables aléatoires \\((X_n)_{n \\geq 1}\\) sur \\((\\Omega,\\mathcal{A},\\mathbb{P})\\) de loi \\(\\mathbb{P}(X_1=1)=1/2=1-\\mathbb{P}(X_1=-1)\\). On se donne un entier \\(x\\) entre \\(0\\) et \\(n\\) et on pose pour \\(t\\geq 1\\), \\(S_t= x + \\sum_{k=1}^t X_k\\), et \\(\\tau_x=\\inf\\{t \\geq 0, S_t=0 \\text{ ou } S_t=n\\}\\). Le but de l’exercice est de calculer \\(u_x=\\mathbb{E}(\\tau_x)\\).\n\n\\(\\tau_x\\) est-elle une variable aléatoire?\nCalculer \\(u_0\\) et \\(u_n\\).\nÉcrire une équation reliant \\(u_k\\), \\(u_{k+1}\\) et \\(u_{k-1}\\).\nPoser \\(v_k=u_k-u_{k-1}\\) et en déduire \\(u_k\\), puis \\(\\max_{k \\in [0,n]} u_k\\).\nEcrire un programme permettant de simuler des réalisations aléatoires de \\(\\tau_x\\).\nEstimer numériquement les \\(u_k\\) à partir des simulations de la question précédente.\nReprésenter graphiquement les estimations de \\(u_k\\) en fonction de \\(k\\).\n\n\nExercice 3 (Théorème d’approximation de Weierstrass)  \n\nSoit \\(f : [0, 1] → \\mathbb{R}\\) une fonction continue.\nSoit \\(S_n\\) une variable aléatoire distribuée selon une loi binomiale de paramètres \\(n\\) et \\(x \\in (0,1)\\).\nMontrer que :\n\\[\n\\lim_n \\sup_{0\\leq x \\leq 1} \\left| f(x) - \\sum_{k=0}^n \\binom{n}{k} x^k  (1-x)^{n-k} f\\left(\\frac{k}{n}\\right)\\right| = 0\n\\]\nSuggestion: Utiliser \\[\n\\mathbb{E}(Z) = \\mathbb{E}(Z \\mathbb{I}_A ) + \\mathbb{E}(Z \\mathbb{I}_{A^c} )\n\\] avec \\(Z = f (x) − f (S_n/n )\\) et \\[\nA = \\{|S_n/n − x| &gt; δ\\},\n\\]\n\n\n\n\n\n\nLe théorème d’approximation de Weierstrass s’énonce: toute fonction continue sur \\([0, 1]\\) peut être approchée uniformément par des polynômes.\n\n\n\n\n\n\n\n\n\nSi \\(f\\) est une fonction croissante sur \\([a,b]\\), on définit son inverse généralisée \\(f^\\leftarrow\\) par\n\\[f^{\\leftarrow}(y) = \\inf \\{ x : x \\in [a,b], \\,  f(x) \\geq y\\} \\qquad \\text{ pour } y\\in [ f(a),f(b)]\\,.\\]\n\n\n\n\nExercice 4 (Fonctions quantiles)  \n\n\nMontrer que si \\((f_n)_n\\) est une suite de fonctions croissantes sur \\([a,b]\\subset \\mathbb{R}\\) qui converge simplement vers \\(f\\) une autre fonction croissante sur \\([a,b]\\) en tout point de continuité de \\(f\\), alors la suite \\((f_n^\\leftarrow(y))\\) converge simplement vers \\(f^\\leftarrow(y)\\) en tout \\(y\\in [f(a),f(b)]\\) où \\(f^\\leftarrow\\) est continue.\nSoit \\(F\\) une fonction de répartition, on définit la fonction quantile associée \\(F^\\leftarrow\\) par\n\\[F^{\\leftarrow}(p) = \\inf \\{ x :  F(x) \\geq p\\} \\qquad \\text{ pour } p\\in ]0,1[ \\, .\\]\nVérifier que si deux fonctions de répartitions ont même fonction quantiles alors elles sont égales.\nDans cette définition \\(F\\) est une fonction de répartition, \\(F^\\leftarrow\\) la fonction quantile associée.\nMontrer que pour tout \\(x\\in \\mathbb{R}, p\\in ]0,1[\\),\n\n\n\n\\(F^\\leftarrow (p)\\leq x \\Leftrightarrow  p\\leq F(x).\\)\n\\(F\\circ F^\\leftarrow(p) \\geq p\\) avec égalité si et seulement si il existe \\(x\\) tel que \\(F(x)=p.\\)\\ Si \\(F\\circ F^{\\leftarrow}(p)&gt;p\\) alors \\(F^{\\leftarrow}\\) est discontinue en \\(p\\).\n\\(F^\\leftarrow\\circ F(x)\\leq  x\\).\\ Si \\(F^\\leftarrow\\circ F(x)&lt;  x\\), alors il existe \\(\\epsilon&gt;0\\) tel que \\(F(x-\\epsilon)=F(x)\\).\n\\((F\\circ G)^\\leftarrow = G^\\leftarrow \\circ F^\\leftarrow\\)\n\n\nExercice 5 (Statistiques d’ordre)  \n\nSi \\(Y_{1:n}\\leq Y_{2:n}\\leq\\ldots\\leq Y_{n:n}\\) forme les statistiques d’ordre d’un \\(n\\)-échantillon de la loi exponentielle d’espérance \\(1\\), et si \\(X_{1:n}\\leq X_{2:n}\\leq\\ldots\\leq X_{n:n}\\) désigne les statistiques d’ordre d’un \\(n\\)-échantillon d’une loi de fonction de répartition \\(F\\) qui admet une densité partout positive, montrer que\n\\[\\left( X_{1:n}, X_{2:n},\\ldots, X_{n:n} \\right) \\sim \\left(  F^\\leftarrow (1-\\exp(-Y_{1:n})),\\ldots, F^\\leftarrow(1-\\exp(-Y_{n:n}))\\right)\\, .\\]\nLa fonction quantile empirique \\(F_n^\\leftarrow\\) est la fonction quantile associée à la fonction de répartition empirique \\(F_n\\). Si les points de l’échantillon sont deux à deux distincts\n\\[F_n^\\leftarrow\\left(p\\right) = X_{k:n} \\, \\qquad \\text{ pour }  \\frac{k-1}{n}&lt; p\\leq \\frac{k}{n} \\, .\\]\nSoit \\(F\\) une fonction de répartition qui est dérivable en \\(F^{\\leftarrow}(p)\\) de dérivée non nulle notée \\(f(p)\\) pour une valeur \\(p\\in ]0,1[\\). Montrer que\n\\[\\sqrt{n} \\left( F_n^\\leftarrow(p) -F^\\leftarrow(p)\\right) + \\sqrt{n}\\frac{1}{f(p)}\\left(F_n(F^\\leftarrow(p)) -p  \\right) = o_P(1) \\, .\n\\]\nQuelle est la loi limite de \\(\\sqrt{n} \\left( F_n^\\leftarrow(p) -F^\\leftarrow(p)\\right)\\) ?\n\nExercice 6 (Convergences (relations))  \n\nSi \\(X_1, \\ldots, X_n, \\ldots\\) sont définies sur le même espace probabilisé, et convergent en distribution vers \\(Z\\) (définie sur le même espace), peut-on affirmer que :\n\n\\(X_1, \\ldots, X_n, \\ldots\\) convergent en probabilité vers \\(Z\\) ?\n\\(X_1, \\ldots, X_n, \\ldots\\) convergent presque sûrement vers \\(Z\\) ?\n\n\nExercice 7 (Convergence en distribution vers une constante)  \n\nSi \\(X_1, \\ldots, X_n, \\ldots\\) sont définies sur le même espace probabilisé, et convergent en distribution vers \\(c\\) (une variable aléatoire constante/dégénérée) égale à \\(c\\) avec probabilité 1), montrer que \\(X_1, \\ldots, X_n, \\ldots\\) converge en probabilité vers \\(c\\).\n\nExercice 8 (Lemme de Slutsky)  \n\nSi les suites de variables aléatoires \\(X_1, \\ldots, X_n, \\ldots\\), \\(Y_1, \\ldots, Y_n, \\ldots\\) sont définies sur le même espace probabilisé, et convergent respectivement en distribution vers \\(X\\) et vers \\(c\\) (constante), montrer que\n\n\\(X_nY_n \\rightsquigarrow cX\\)\n\\(X_n/Y_n \\rightsquigarrow X/c\\) si \\(c\\neq 0\\)\n\\(g(X_n, Y_n) \\rightsquigarrow g(X,c)\\) si \\(g : \\mathbb{R}^2 \\to \\mathbb{R}\\) est continue\n\n\n\nExercice 9 (representation-mediane-uniforme) Dans cet exercice \\(Y_1, Y_2, \\ldots, Y_{n+1}\\) sont i.i.d. exponentielles. On suppose \\(n\\) impair montrer que\n\\[\\frac{\\sum_{i=1}^{\\lfloor(n+1)/2\\rfloor} Y_i}{\\sum_{i=1}^{n+1} Y_i}\\]\nest distribuée comme la médiane empirique d’un \\(n\\)-échantillon de la loi uniforme sur \\([0,1]\\).\n\n\nExercice 10 (tcl-mediane-uniforme)  \n\nDans cet exercice \\(Y_1, Y_2, \\ldots, Y_{n+1}\\) sont i.i.d. exponentielles.\n\nMontrer que\n\\[\\frac{\\sum_{i=1}^{k} Y_i}{\\sum_{i=1}^{n+1} Y_i}\\]\nest distribué comme la \\(k^{\\text{eme}}\\) statistique d’ordre d’un \\(n\\) échantillon de ;la loi uniforme sur \\([0,1]\\)\nPour \\(n\\) pair, \\(k=n/2\\),\n\\[\\sqrt{n}\\left(\\frac{\\sum_{i=1}^{k} Y_i}{\\sum_{i=1}^{n+1} Y_i}- \\frac{1}{2} \\right)\\]\nconverge en distribution vers une Gaussienne centrée. Précisez la la variance de la loi limite.\nSuggestion : utilisez le lemme de Slutsky (Exercice 8).\n\n\nExercice 11 (Lemme de Scheffé)  \n\n\nVérifier que la convergence simple des densités vers une densité implique la convergence en distribution.\nLa réciproque est-elle vérifiée? A-t-elle un sens bien défini?\n\n\nExercice 12 (Convergence Poisson/Gaussienne)  \n\nSi \\(X_n \\sim \\text{Poisson}(n)\\), montrer que \\(\\frac{X_n -n}{\\sqrt{n}} \\rightsquigarrow \\mathcal{N}(0,1)\\)\n\nExercice 13 (Convergence Gamma/Gaussienne)  \n\nSi \\(X_n \\sim \\text{Gamma}(n,\\lambda)\\), montrer que \\(\\frac{X_n - n/\\lambda}{\\sqrt{n}} \\rightsquigarrow \\mathcal{N}(0,1/\\lambda)\\)\n\nExercice 14 (Convergence Maxima d’échantillon uniforme)  \n\nSi \\(X_1, \\ldots, X_n, \\ldots \\sim_{\\text{i.i.d.}} \\text{Unif}[0,1]\\), \\(M_n = \\max(X_1, \\ldots, X_n)\\), pouvez-vous trouvez \\((a_n, b_n)_n\\) avec \\(a_n &gt;0\\), tels que \\(\\left((M_n -b_n)/a_n\\right)_n\\) convergent en loi vers une variable aléatoire non-dégénérée?\n\nExercice 15 (Loi faible des grands nombres)  \n\n\nVérifier la loi faible des grands nombres si on suppose que les sommants \\(X_i\\) ont un moment d’ordre \\(4\\)."
  },
  {
    "objectID": "exercices/td2.html",
    "href": "exercices/td2.html",
    "title": "MA1AY010 Probabilités",
    "section": "",
    "text": "NoteTD II : Espérances et lois conditionnelles\n\n\n\n22 Septembre 2025-26 Septembre 2025\n\nMaster I Isifar\nProbabilités\n\n\n\n\nExercice 1 (Espérance conditionnelle/tribu atomique)  \n\n\n\n\n\n\n\nNoteCours\n\n\n\nEspérance conditionnelle par rapport à une tribu engendrée par une partition dénombrable.\n\n\n\nSoit \\((A_n, n \\in \\mathbb{N}^*)\\) une partition de \\(\\Omega\\) et \\(\\mathcal{F}= \\sigma(A_n, n \\ge 1)\\) la tribu engendrée par les \\(A_n, n \\ge 1\\). Rappelons qu’une v.a.r. \\(Y\\) est \\(\\mathcal{F}\\)-mesurable si et seulement si il existe une suite de réls \\((a_n)\\) telle que \\(Y= \\sum_{n \\ge 1} a_n \\mathbf{1}_{A_n}\\). Exprimer \\(\\mathbb{E}[X \\mid \\mathcal{F}]\\).\nSoient \\(X,Y\\) deux variables i.i.d. \\(\\sim\\) Ber\\((p)\\). On considère \\(\\mathcal{G} = \\sigma(\\{X+Y=0\\})\\). Calculer \\(\\mathbb{E}[X \\mid \\mathcal{G}], \\mathbb{E}[Y\\mid \\mathcal{G}]\\). Les variables obtenues sont-elles toujours indépendantes?\n\n\nExercice 2 (Conditionnement continu)  \n\nSoient \\((X,Y)\\) un couple de v.a. réelles intégrables de densité jointe \\(f\\), \\(g : \\mathbb{R}^2 \\to \\mathbb{R}\\) borélienne telle que \\(g(X,Y) \\in \\mathbb{L}^1\\).\nRappeler l’expression de \\(\\phi, \\psi\\) telles que \\[\\mathbb{E}[g(X,Y)\\mid Y] = \\phi(Y), \\quad \\mathbb{E}[g(X,Y)|X] = \\psi(X).\\]\n\nOn considère \\((X,Y)\\) de densité jointe \\(f(x,y)= \\frac{1}{x} \\mathbf{1}_{\\{0 \\le y \\le x \\le 1\\}}.\\) Quelle est la loi de \\(X\\)? Calculer la distribution conditionnelle \\(f_{Y \\mid X}\\) de \\(Y\\) sachant \\(X\\). Calculer \\(\\mathbb{P}(X^2 +Y^2 \\le 1 |X)\\), puis en déduire \\(\\mathbb{P}(X^2+Y^2 \\le 1)\\).\nPour simplifier l’expression obtenue on pourra utiliser que \\(x \\to \\sqrt{1-x^2} - \\tanh^{-1}(\\sqrt{1-x^2}) = \\sqrt{1-x^2}-\\frac{1}{2} \\ln(1+\\sqrt{1-x^2}) + \\frac{1}{2} \\ln(1-\\sqrt{1-x^2})\\) est une primitive de \\(x \\to \\frac{\\sqrt{1-x^2}}{x}\\).\nDans le cas général, montrer que \\(\\mathbb{E}[\\mathbb{E}[Y|X]] = \\mathbb{E}[Y]\\). Que vaut \\(\\mathbb{E}[Y]\\) dans l’exemple de la question précédente?\nMontrer, dans le cas général, que \\[\\mathbb{E}[\\mathbb{E}[Y|X] g(X)] = \\mathbb{E}[Yg(X)],\\] pour toute fonction \\(g\\) telle que les deux espérances sont définies. Que vaut \\(\\mathbb{E}[Y g(X) \\mid X]\\)?\n\n\nExercice 3 (Partiel passé)  \n\n\n\n\n\n\n\nNotePartiel passé\n\n\n\n\n\n\nSoient \\(0 \\le r \\le p \\le 1\\) tels que \\(1-2p+r \\ge 0\\).\nSoient \\(X_1, X_2\\) tels que\n\\[\\begin{eqnarray*}\n&&  \\mathbb{P}(X_1=1, X_2=1)=r, \\quad   \\mathbb{P}(X_1=0, X_2=1)=p-r, \\\\\n&& \\mathbb{P}(X_1=1, X_2=0)=p-r, \\quad  \\mathbb{P}(X_1=0, X_2=0)=1-2p+r.\n\\end{eqnarray*}\\]\n\nQuelle est la loi de \\(X_1\\)? celle de \\(X_2\\)?\nCalculer \\(Y = \\mathbb{E}[X_1\\mid X_2]\\) et vérifier que\n\\[Y= \\begin{cases} & \\frac{p-r}{1-p} \\mbox{ avec probabilité } 1-p\\\\ & \\frac{r}{p} \\mbox{ avec probabilité } p.\\end{cases}\\]\nRappelons que par définition \\(\\text{Var}[X_1 \\mid X_2] = \\mathbb{E}[X_1^2\\mid X_2] - \\mathbb{E}[X_1\\mid X_2]^2\\). Montrer que\n\\[\\mathrm{Var}[X_1 \\mid X_2] = \\left( \\frac{p-r}{1-p} - \\left(\\frac{p-r}{1-p}\\right)^2\n\\right) \\mathbf{1}_{\\{X_2=0\\}} + \\left( \\frac{r}{p} - \\left(\\frac{r}{p}\\right)^2 \\right)\n\\mathbf{1}_{\\{X_2=1\\}}.\\]\nQue vaut \\(\\mathrm{Var}(\\mathbb{E}[X_1\\mid X_2])\\)? \\(\\mathbb{E}[\\mathrm{Var}[X_1\\mid X_2]]\\)? Vérifier qu’on a bien\n\\[\\mathrm{Var}(X_1) = \\mathrm{Var}(\\mathbb{E}[X_1\\mid X_2]) + \\mathbb{E}[\\mathrm{Var}[X_1\\mid X_2]].\\]\n\n\nExercice 4 (Conditionnement)  \n\nSoit \\((X_n)\\) une suite de v.a. .i.i.d intégrables, et \\(S_n = \\sum_{i=1}^n X_i\\).\n\nQue valent \\(\\mathbb{E}[X_1\\mid X_2], \\mathbb{E}[S_n \\mid X_1], \\mathbb{E}[S_n \\mid S_{n-1}]?\\)\nMontrer que si les paires de variables \\((X,Z)\\), \\((Y,Z)\\) ont la même loi jointe, alors pour toute fonction réelle positive (ou satisfaisant une condition d’intégrabilité), \\(\\mathbb{E}[f(X)\\mid Z] = \\mathbb{E}[f(Y)\\mid Z]\\). En déduire \\(\\mathbb{E}[X_1 \\mid S_n]\\).\n\n\nExercice 5 (Examen passé)  \n\n\n\n\n\n\n\nNote(Examen passé)\n\n\n\n\n\n\nSoit \\((X_n, n \\ge 0)\\) une suite de variables i.i.d, avec \\(X_1 \\sim \\text{Ber}(1/2)\\). On pose \\(S_n = \\sum_{i=1}^n (X_i -1/2)\\), \\(\\mathcal{F}_n= \\sigma(X_1,...,X_n)\\).\nCalculer \\(\\mathbb{E}[S_n \\mid \\mathcal{F}_5]\\) en fonction de \\(n\\). Quelle est la loi de cette variable aléatoire?\n\nExercice 6 (Partiel passé)  \n\nSoient \\(\\{\\mathbf{e}_i, i \\in \\mathbb{N} \\}\\) des variables i.i.d exponentielles de paramètre \\(1\\). Pour \\(n \\in \\mathbb{N}^*\\) on note \\(S_n := \\sum_{i=1}^n \\mathbf{e}_i\\).\n\nOn note \\(f_n\\) la fonction de densité de la variable \\(S_n\\). Montrer que pour tout \\(t \\ge 0\\) \\[ f_n(t) = \\frac{t^{n-1}}{(n-1)!} \\exp(-t).\\]\nPour \\(t &gt;0, n \\in \\mathbb{N}^*\\), que vaut \\(\\mathbb{P}(S_n \\le t)\\)?\nOn fixe \\(t&gt;0\\) et on suppose \\(X_t \\sim \\mathrm{Poisson}(t)\\). Que vaut \\(\\mathbb{P}(X_t \\ge n)\\), pour \\(n \\in \\mathbb{N}^*\\)?\nSur la demi-droite \\(\\mathbb{R}_+\\) on place les points \\(S_1, S_2, S_3,...\\). On note \\(N_t\\) le nombre de ces points qui tombent dans l’intervalle \\([0,t]\\). Exprimer l’événement \\(\\{N_t \\ge n\\} = \\{S_n \\le t\\}\\). Déterminer la loi de \\(N_t\\) à l’aide des questions préc'dentes.\n\nMontrer que, conditionnellement à \\(\\{N_t=1\\}\\), la loi de \\(\\mathbf{e}_1\\) est uniforme sur \\([0,t]\\).\nConditionnellement à \\(\\{N_t=2\\}\\), quelle est la loi du vecteur \\((\\mathbf{e}_1; \\mathbf{e}_2)\\)?\n\n\nExercice 7 (CC2 2023)  \n\nOn considère \\[ X \\sim \\mathcal{N} \\left( \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 2 & 1 & -1 & -1 \\\\ 1 & 2 & -1 & 0 \\\\ -1 & -1 & 3 & -1 \\\\ -1 & 0 & -1 & 5 \\end{pmatrix}\\right).\\]\n\nCalculer \\(\\mathbb{E}[X_3 \\mid X_4]\\), et déterminer la loi conditionnelle de \\(X_3\\) sachant \\(X_4\\).\n\nOn pose \\(A = \\begin{pmatrix} 2  & 1 \\\\ 1 & 2 \\end{pmatrix}\\), \\(B= \\begin{pmatrix} -1 & -1 \\\\ -1 & 0 \\end{pmatrix}\\). Calculer \\(BA^{-1}\\), puis vérifier que \\[B A^{-1} B^T = \\begin{pmatrix} \\frac{2}{3} & \\frac{1}{3} \\vspace{0.1cm} \\\\ \\frac{1}{3} & \\frac{2}{3}\\end{pmatrix}.\\]\nDéterminer \\(\\mathbb{E}\\left[\\begin{pmatrix} X_3 \\\\ X_4 \\end{pmatrix} \\ \\bigg| \\ \\begin{pmatrix} X_1 \\\\ X_2 \\end{pmatrix}\\right]\\). et la loi conditionnelle de \\(\\begin{pmatrix} X_3 \\\\ X_4 \\end{pmatrix}\\) sachant \\(\\begin{pmatrix} X_1 \\\\ X_2 \\end{pmatrix}\\).\n\n\nExercice 8 (Partiel passé)  \n\nSoit \\((X_1,X_2,X_3) \\sim \\mathcal{N}(\\mu, M)\\) où \\[\\mu = \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}, \\quad M = \\begin{pmatrix} 2 & 1 & 0 \\\\ 1 & 2 & 1 \\\\ 0 & 1 & 2 \\end{pmatrix}.\\]\n\nQuelle est la loi du couple \\((X_1,X_2)\\)?\nDéterminer \\(\\alpha\\) un réel tel que $Y = X_1 + X_2 $ est indépendante de \\(X_1\\). Que vaut \\(\\mathbb{E}[Y]\\)? \\(\\text{Var}(Y)\\)?\nEn déduire \\(\\mathbb{E}[X_2 \\mid X_1]\\). Quelle est la loi conditionnelle de \\(X_2\\) sachant \\(X_1\\)?\nDéterminer un réel \\(\\beta\\) tels que \\(Z=\\beta X_1 + X_3\\) est indépendante de \\(X_1\\). En déduire \\[\\mathbb{E}[X_3 \\mid X_1], \\quad \\mathbb{E}[X_3^2 \\mid X_1].\\]\nCalculer \\(\\mathbb{E}\\left[X_1^2X_2 + X_3^2 X_1 \\mid X_1\\right]\\).\n\n\nExercice 9 (Examen passé)  \n\nSoit \\((X_1,X_2,X_3) \\sim \\mathcal{N}(\\mu,M)\\), où \\[ \\mu = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\qquad M=  \\begin{pmatrix} 1 & 1/2 & 2 \\\\ 1/2 & 1 & 1 \\\\ 2 & 1 & 3 \\end{pmatrix}.\\] Calculer \\(\\mathbb{E}[X_1+2X_2 \\mid X_3].\\) Quelle est la loi conditionnelle de \\(X_1+2X_2\\) sachant \\(X_3\\)?\n\nExercice 10 (CC2 2023)  \n\nOn suppose dans cet exercice que \\((X,Y)\\) est un couple de variables aléatoires tel que pour toute \\(\\phi : \\mathbb{R}^2\\to \\mathbb{R}_+\\) borélienne, \\[\\mathbb{E}[\\phi(X,Y)] = \\sum_{n \\ge 1} \\frac{2}{3^{n}\\sqrt{2\\pi n}}  \\int_{\\mathbb{R}} \\phi(n,y) \\exp\\left(-\\frac{y^2}{2n}\\right) dy.\\]\n\nMontrer que \\(X \\sim \\mathrm{Geom}(2/3)\\).\nVérifier que pour une fonction \\(f : \\mathbb{R} \\to \\mathbb{C}\\) telle que \\(f(Y) \\in \\mathbb{L}^1\\), on a\n\\[\\mathbb{E}[f(Y) \\mid X] = \\sum_{n \\ge 1} \\left(\\int_{\\mathbb{R}} \\frac{1}{\\sqrt{ 2\\pi n}} f(y) \\exp\\left(-\\frac{y^2}{2n} \\right) dy \\right) \\mathbb{I}_{\\{X=n\\}}\\] %1. En déduire que pour tout \\(k \\in \\mathbb{N}\\), %\\[\\mathbb{E}[Y^k \\mid X] = \\frac{k!}{X^{2k}}\\]\nCalculer \\(\\mathbb{E}[\\exp(itY) \\mid X]\\), \\(t \\in \\mathbb{R}\\), quelle est la loi conditionnelle de \\(Y\\) sachant \\(X\\) ?\nDéduire que si \\(t\\in \\mathbb{R}\\) \\[\\mathbb{E}[\\exp(itY)] = \\frac{2 \\exp\\left(-\\frac{t^2}{2}\\right)}{3-\\exp\\left(-\\frac{t^2}{2}\\right)}.\\]\n\n\nExercice 11 (Partiel passé)  \n\n\nPartie I\nOn considère le couple \\((X,Z)\\) de densité jointe \\[ f(x,z) := (z-x)\\exp(-z) \\mathbf{1}_{\\{z \\ge x \\ge 0\\}}.\\]\n\nCalculer la loi de \\(X\\), puis celle de \\(Z\\).\nEn déduire que\n\\[f_{X \\mid Z}(x \\mid z) = \\frac{2(z-x)}{z^2} \\mathbf{1}_{\\{0 \\le x \\le z, z &gt;0\\}}.\\]\nCalculer \\(\\mathbb{E}[X \\mid Z]\\), puis \\(\\mathrm{Var}[X\\mid Z]\\).\nCalculer \\(f_{Z \\mid X}(z \\mid x)\\), puis démontrer que \\(\\mathbb{E}[Z \\mid X] = X + 2\\).\nQuelle est la loi du couple \\((X, Z-X)\\)? En déduire la loi de \\(Z-X\\).\n\n\n\nPartie II\n\nSoit \\(z &gt;0\\). On suppose que \\(U_1^z \\sim \\mathrm{Unif}[0,z]\\), \\(U_2^z \\sim \\mathrm{Unif}[0,z]\\) et que \\(U_1^z\\) est indépendante de \\(U_2^z\\). Calculer la densité de \\(\\min(U_1^z, U_2^z)\\).\nOn suppose à présent que conditionnellement à \\(Z\\), \\(U_1^Z \\sim \\mathrm{Unif}[0,Z]\\), \\(U_2^Z \\sim \\mathrm{Unif}[0,Z]\\) et que \\(U_1^Z\\) est (toujours conditionnellement à \\(Z\\)) indépendante de \\(U_2^Z\\). Montrer que, conditionnellement à \\(Z\\), \\(\\mathrm{min}(U_1^Z, U_2^Z)\\) a la même loi que X.\nSoient \\(X_1,X_2,X_3\\) trois variables indépendantes, toutes trois distribuées suivant la distribution exponentielle de paramètre \\(1\\). On note \\(S= X_1+X_2+X_3\\). Déterminer la loi de \\((X_1,S)\\). Que vaut \\(\\mathbb{E}[X_1\\mid S]\\)? \\(\\mathbb{E}[S \\mid X_1]\\)? Montrer finalement que conditionnellement à \\(S\\), le couple \\((X_1,X_1+X_2)\\) a la même loi que \\(\\left(\\mathrm{min}(U_1^S, U_2^S), \\mathrm{max}(U_1^S, U_2^S)\\right)\\).\n\n\nExercice 12 (Partiel passé)  \n\nPour \\((x,y) \\in \\mathbb{R}^2\\) on définit\n\\[f(x,y) := \\frac{4y}{x^3} \\mathbf{1}_{\\{0&lt;x&lt;1, 0&lt;y &lt;x^2\\}}.\\]\nVérifier que \\(f\\) est bien une densité de probabilité, puis calculer les densités marginales \\(f_X\\), \\(f_Y\\).\nCalculer \\(f_{Y\\mid X}(y \\mid x)\\) et en déduire que\n\\[\\mathbb{E}[Y \\mid X] = \\frac{2}{3} X^2.\\]\nMontrer que\n\\[f_{X \\mid Y}(x\\mid y) = \\frac{2y}{1-y} \\frac{1}{x^3} \\mathbf{1}_{\\{0&lt;x&lt;1, 0&lt;y &lt;x^2\\}},\\]\npuis calculer \\(\\mathbb{E}[X \\mid Y]\\).\n\nExercice 13 (CC2 2023)  \n\nDans cet exercice on suppose que\n\\[\n\\begin{pmatrix} X \\\\ Y\\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 2 & 1 \\\\ 1 & 1 \\end{pmatrix} \\right),\n\\]\net on pose \\(U= X^2\\).\n\nVérifier que \\(U \\sim \\mathrm{Gamma}(1/2,1/4)\\).\nMontrer que \\((X,Y)\\) possède une densité jointe \\(g\\) que l’on déterminera.\n\nMontrer que \\((U,Y)\\) possède la densité jointe \\[ f(u,y) = \\frac{1}{{ 4} \\pi \\sqrt{u}} \\left( \\exp\\left(- \\frac{u}{2} - y^2 + y \\sqrt{u} \\right) + \\exp\\left(-\\frac{u}{2}- y^2 - y\\sqrt{u} \\right) \\right) \\mathbb{I}_{\\{u &gt;0\\}}. \\]\nCalculer \\(f_{Y \\mid U}(y \\mid u)\\). En déduire \\(\\mathbb{E}[Y \\mid U], \\mathbb{E}[Y^2 \\mid U]\\) et \\(\\mathrm{Var}(Y \\mid U)\\). Vérifier qu’on a bien \\[\\mathrm{Var}[Y] = \\mathbb{E}[\\mathrm{Var}[Y \\mid U]] + \\mathrm{Var}[\\mathbb{E}[Y \\mid U]]\\, .\\]\nOn suppose que conditionnellement à \\(U\\), \\(\\xi\\) et \\(Z\\) sont indépendantes avec \\(\\xi \\sim \\mathrm{Ber}(1/2)\\) et \\(Z \\sim \\mathcal{N}\\left(\\frac{\\sqrt{U}}{2},\\frac{1}{2}\\right)\\). Montrer que conditionnellement à \\(U\\), \\((2\\xi-1) Z\\) a même loi que \\(Y\\). Vérifier alors les calculs de la question précédente.\n\n\n\n\n\n\n\nAstuceIndications\n\n\n\n\nrappelle que pour \\(a&gt;0, \\lambda &gt;0\\), la densité d’une variable \\(G \\sim \\mathrm{Gamma}(a,\\lambda)\\) est donnée par\n\\[f_G(x) = \\frac{\\lambda^a x^{a-1}}{\\Gamma(a)} \\exp(-\\lambda x) \\mathbb{I}_{\\{x &gt;0 \\}}\\]\nOn fera attention à distinguer les domaines \\(D_1 = \\mathbb{R}_-^*\\times \\mathbb{R}\\) et \\(D_2 = \\mathbb{R}_+^* \\times \\mathbb{R}\\) pour pouvoir considérer les \\(\\mathcal{C}^1\\)-difféomorphismes \\(\\displaystyle{\\Psi_1 : \\begin{cases} \\!\\!&\\!\\! D_1 \\to \\mathbb{R}_+^* \\times \\mathbb{R} \\\\ \\!\\!&\\!\\! (x,y) \\to (x^2,y) \\end{cases}, \\ \\ \\Psi_2 :  \\begin{cases} \\!\\!&\\!\\! D_2 \\to \\mathbb{R}_+^* \\times \\mathbb{R} \\\\ \\!\\!&\\!\\! (x,y) \\to (x^2,y)\\end{cases}}\\).\nPour \\(\\alpha \\in \\mathbb{R}\\), les deux premiers moments de la variable \\(\\zeta \\sim \\mathcal{N}\\left(\\alpha \\frac{\\sqrt{u}}{2}, \\frac{1}{2}\\right)\\) sont\n\\[\\begin{align*}\n\\mathbb{E}[\\zeta]\n& = \\int_{\\mathbb{R}} \\frac{1}{\\sqrt{\\pi}} y \\exp\\left(-  \\left(y - \\alpha \\frac{\\sqrt{u}}{2}\\right)^2 \\right) dy \\\\\n& = \\alpha \\frac{\\sqrt{u}}{2},  \\\\\n\\mathbb{E}[\\zeta^2]\n& = \\mathbb{E}[\\zeta]^2+ \\mathrm{Var}[\\zeta] = \\int_{\\mathbb{R}} \\frac{1}{\\sqrt{\\pi}} y^2 \\exp\\left(-  \\left(y - \\alpha \\frac{\\sqrt{u}}{2}\\right)^2 \\right) dy \\\\\n& = \\alpha^2 \\frac{u}{4} + \\frac{1}{2}\n\\end{align*}\\]\n\n\n\n\nExercice 14 (Rattrapage passé)  \n\nSoit \\(X=(X_1,X_2,X_3) \\sim \\mathcal{N}(0,M)\\), où\n\\[M := \\begin{pmatrix} 2& 2 &-2  \\\\ 2& 5 & 1 \\\\ -2 & 1 & 5 \\end{pmatrix},\\]\n\nMontrer que \\(\\det(M)=0\\). Le vecteur \\(X\\) possède-t-il une densité dans \\(\\mathbb{R}^3\\)?\nTrouver \\(a \\in \\mathbb{R}\\) tel que \\(X_1\\) et \\(Y=X_2-a X_1\\) soient indépendantes. Calculer \\(\\mathrm{Var}(Y)\\) et en déduire la loi de \\((X_1,Y)\\).\nTrouver la loi conditionnelle de \\(X_2\\) sachant \\(X_1\\).\n\n\nExercice 15 (Combinaison linéaire de gaussiennes)  \n\nOn considère \\(X_0 =0\\), et \\((X_n)_{n \\ge 1}\\) une suite de variables aléatoires réelles indépendantes, identiquement distribuées suivant la loi normale centrée réduite.\nOn introduit les variables\n\\[Y_i = \\frac{X_i-X_{i-1}}{i}, i \\ge 1.\\]\nPour \\(n \\ge 1\\), montrer que le vecteur \\((Y_1,...,Y_n)\\) est gaussien, puis calculer le vecteur moyenne et la matrice de covariances de \\((Y_1,...,Y_n)\\).\nCalculer, pour \\(n \\ge 1\\), \\(\\mathbb{E}[Y_{n+1}\\mid Y_n]\\).\n\nExercice 16 (Loi jointe à densité)  \n\nSoient \\((X,Y)\\) dont la loi jointe a pour densité \\(f(x,y) = x(y-x) \\exp(-y), 0 \\le x \\le y &lt;\\infty\\). On introduit la notation \\(f_{X|Y}(x|y) := f(x,y)/f_Y(y)\\) lorsque le quotient est \\(&gt;0\\), \\(0\\) sinon.\n\nExprimer \\(f_{X|Y}(x|y)\\), puis \\(f_{Y|X}(y|x)\\).\nEn déduire les expressions de \\(\\mathbb{E}[X|Y], \\mathbb{E}[Y|X]\\).\n\n\nExercice 17 (Exponentielles conditionnées)  \n\nSoient \\(Y,Z\\) deux v.a.r. indépendantes \\(\\sim \\mathrm{exp}(\\lambda)\\) où \\(\\lambda&gt;0\\). On pose \\(X= Y+Z\\). Quelle est la loi conditionnelle de \\(Y\\) sachant \\(X\\)? Que vaut \\(\\mathbb{E}[Y|X]\\)? En déduire l’expression de \\(\\mathbb{E}[Y|X]\\)\n\nExercice 18 (Gaussiennes corrélées)  \n\nSoient \\(X\\) et \\(Y\\) deux variables aléatoires indépendantes, toutes deux normales centrées réduites. On définit pour \\(\\sigma_1 &gt;0, \\sigma_2&gt;0, |\\rho|\\le 1\\),\n\\[U = \\sigma_1 X, \\quad V= \\sigma_2 \\rho X + \\sigma_2 \\sqrt{1- \\rho^2} Y.\\]\n\nQuelle est la loi de \\((U,V)\\)?\nQue vaut \\(\\mathbb{E}[UV]\\)?\nQue vaut \\(\\mathbb{E}[U \\mid V]? \\mathbb{E}[V \\mid U]? \\mathrm{Var}[U \\mid V]? \\mathrm{Var}[V \\mid U]?\\)\n\n\nExercice 19 (Gaussiennes corrélées (2))  \n\nSoit \\(Z = (X, Y )\\) un vecteur aléatoire gaussien à valeurs dans \\(\\mathbb{R}^2\\). On suppose que \\(E(X) = E(Y ) = 0\\), \\(\\mathrm{Var}(X) = \\mathrm{Var}(Y ) = 1\\) et que \\(\\mathrm{Cov}(X; Y ) = \\rho\\) avec \\(|\\rho|^2 \\ne 1\\). On pose \\(U = X -\\rho Y , V = \\sqrt{1-\\rho^2} Y\\).\n\nQuelles sont les lois de \\(U\\) et \\(V\\) ? Les v.a. \\(U\\) et \\(V\\) sont-elles indépendantes ?\nCalculer \\(\\mathbb{E}(U^2V^2), \\mathbb{E}(U V^3), \\mathbb{E}(V^4)\\). En déduire \\(\\mathbb{E}(X^2Y^2)\\).\nRetrouver ce dernier résultat par conditionnement.\n\n\nExercice 20 (Gaussiennes)  \n\nSoient \\(U, V, W\\) trois v.a.r. gaussiennes centrées réduites. On pose \\[Z =\\frac{U + VW}{\\sqrt{1+W^2}}.\\]\n\nQuelle est la loi conditionnelle de \\(Z\\) sachant \\(W\\)?\nEn déduire que \\(Z\\) et \\(W\\) sont indépendantes et donner la loi de \\(Z\\).\n\n\nExercice 21 (Maxima d’exponentielles)  \n\nSoient \\(X_1\\) et \\(X_2\\) des v.a. indépendantes, de lois exponentielles de paramètres respectifs \\(\\lambda_1\\) et \\(\\lambda_2\\).\n\nCalculer \\(\\mathbb{E}[\\max(X_1,X_2) \\mid X_1]\\).\nCalculer \\(\\mathbb{E}[\\max(X_1;X_2)]\\).\n\n\nExercice 22 (Densités jointes)  \n\nOn pose \\(h(x) = \\frac{1}{\\Gamma(a+1)} \\exp(-x)x^{a-1}\\) (\\(a &gt; 0\\) fixé) et \\(D = \\{0 &lt; y &lt; x\\}\\). Soit \\(f(x, y) = h(x)\\mathbf{1}_D(x, y)\\):\n\nMontrer que \\(f\\) est une densité de probabilité sur \\(\\mathbb{R}^2\\). On considère dans la suite un couple \\((X, Y)\\) de v.a.r. de densité \\(f\\).\nLes v.a. \\(X\\) et \\(Y/X\\) sont-elles indépendantes?\nQuelle est la loi conditionnelle de \\(Y\\) sachant \\(X\\) ?\nSoit \\(U\\) une v.a.r. indépendante du couple \\((X, Y)\\) telle que \\(\\mathbb{P}(U = 1) = p\\) et \\(\\mathbb{P}(U = 0) = 1 - p\\). On pose \\(Z = UX + (1 - U)Y\\). Quelle est l’espérance conditionnelle de \\(Z\\) sachant \\(X\\)?\n\n\nExercice 23  \n\nSoit \\((X_n, n \\in \\mathbb{N})\\) une suite de v.a.r.i.i.d. de densité \\(f\\) et fonction de répartition \\(F\\). Soient \\(N := \\min\\{ n \\ge 1 : X_n &gt;X_0\\}\\) et\n\\(M := \\min \\{n \\ge 1 : X_0 \\ge X_1 \\ge ... \\ge X_{n-1} &lt;X_n \\}.\\)\n\nTrouver \\(\\mathbb{P}(N=n)\\), puis montrer que la fonction de répartition de \\(X_N\\) est \\(F +(1-F) \\log(1-F)\\) (on pourra conditionner par les événements \\(\\{N=n\\}, n \\in \\mathbb{N}\\)).\nExprimer \\(\\mathbb{P}(M=m), m \\ge 1\\).\nOn suppose dans cette question que \\(f = \\mathbf{1}_{[0,1]}\\). Pour \\(x \\in (0,1)\\) on introduit \\(R^x := \\min\\{ n \\ge 1 : X_1+...+X_n &gt;x \\}\\). Montrer que \\(\\mathbb{E}[\\mathbf{1}_{\\{R^x&gt;n\\}} \\mid X_n] = \\Phi(X_n)\\) où \\(\\Phi(u) = \\mathbb{I}_{\\{u&lt;x\\}}  \\mathbb{P}(R^{x-u} &gt; n-1)\\). En déduire \\(H_n(x):= \\mathbb{P}(R^x&gt;n)\\).\n\n\nExercice 24  \n\nSoient \\(X\\) et \\(Y\\) deux v.a.r. indépendantes de loi uniforme sur \\([0, 1]\\).\n\nQuelle est l’espérance conditionnelle de \\((Y - X)_+\\) sachant \\(X\\)?\nQuelle est la loi conditionnelle de \\((Y - X)_+\\) sachant \\(X\\)?\n\n\nExercice 25  \n\nSoient \\(X_1, X_2, X_3\\) trois v. a. r. gaussiennes centrées réduites indépendantes. On pose \\(U = 2X_1 - X_2 - X_3, V = X_1 + X_2 + X_3, W = 3X_1 + X_2 - 4X_3\\).\n\nQuelles sont les lois de \\(U, V\\) et \\(W\\)? Quels sont les couples de v.a. indépendantes parmi les couples \\((U, V), (U,W), (V,W)\\)?\nMontrer qu’il existe \\(a \\in \\mathbb{R}\\) tel que \\(W = aU + Z\\) avec \\(U\\) et \\(Z\\) indépendantes. En déduire \\(\\mathbb{E}(W \\mid U)\\).\n\n\nExercice 26  \n\nSoient \\(X\\) et \\(Y\\) deux v. a. r. gaussiennes centrées réduites indépendantes. On pose \\(Z = X + Y\\) , \\(W = X - Y\\).\n\nMontrer que \\(Z\\) et \\(W\\) sont indépendantes. Quelle est la loi de \\(W\\)?\nEn déduire l’espérance conditionnelle et la loi conditionnelle de \\(X\\) sachant \\(Z\\).\nCalculer \\(\\mathbb{E}(XY \\mid Z)\\) et \\(\\mathbb{E}(XYZ \\mid Z)\\)."
  },
  {
    "objectID": "exercices/td1.html",
    "href": "exercices/td1.html",
    "title": "Variables aléatoires réelles",
    "section": "",
    "text": "NoteTD I : Révisions de Licence\n\n\n\n\n8 Septembre 2025-12 Septembre 2025\nMaster I Isifar\nProbabilités"
  },
  {
    "objectID": "exercices/td1.html#fonctions-de-répartition",
    "href": "exercices/td1.html#fonctions-de-répartition",
    "title": "Variables aléatoires réelles",
    "section": "Fonctions de répartition",
    "text": "Fonctions de répartition\n\nExercice 1 (Transformation affine)  \n\nSoit \\(X\\) une variable aléatoire réelle, \\(F_X\\) sa fonction de répartition, \\(a, b\\) deux réels fixés, et \\(Y := aX+b\\)\n\nOn suppose dans cette question que \\(a=1\\). Comment déduire \\(F_Y\\) de \\(F_X\\)?\nSi (la loi de) \\(X\\) admet une densité, en est-il de même de \\(Y\\)? Si oui, exprimer dans ce cas \\(f_Y\\) à l’aide de \\(f_X\\).\nOn suppose dans cette question que \\(b=0\\) et \\(a&gt;0\\). Comment déduire \\(F_Y\\) de \\(F_X\\)?\nSi (la loi de ) \\(X\\) admet une densité, à quelle condition sur \\(a\\) en est-il de même de (la loi de) \\(Y\\)? Exprimer dans ce cas \\(f_Y\\) à l’aide de \\(f_X\\).\nRépondre aux mêmes questions lorsque \\(b=0\\) et \\(a=-1\\)?\nRépondre enfin aux mêmes questions lorsque \\(a\\) et \\(b\\) sont quelconques.\n\n\nExercice 2 (Minimum, maximum de variables indépendantes)  \n\nSoient \\(X_i, i \\ge 1\\), des variables indépendantes. Pour \\(k \\ge 2\\), on note \\(Y_k = \\min (X_1,...,X_k)\\), \\(Z_k = \\max(X_1,...,X_k).\\)\n\nDans cette question on s’intéresse à \\(k=2\\).\nComment déduire \\(F_{Y_2}\\) de \\(F_{X_1}, F_{X_2}\\)? Même question pour \\(F_{Z_2}\\).\nGénéraliser à \\(k\\) quelconque.\nQuelle est la loi de \\(Z_k\\) lorsque les \\(\\{X_i, i \\ge 1\\}\\) sont i.i.d., \\(\\sim \\mathrm{Unif}[0,1]\\)?\nQuelle est la loi de \\(Y_k\\) lorsque les \\(\\{X_i, i \\ge 1\\}\\) sont des variables exponentielles indépendantes, avec \\(X_i \\sim \\text{Exp}(\\lambda_i)\\) (où pour tout \\(i\\), \\(\\lambda_i &gt;0\\))?\n\n\nExercice 3  \n\nOn suppose que \\(X \\sim \\mathcal{N}(0,1)\\). Que valent\n\\[\\mathbb{P}(X \\le 1), \\quad \\mathbb{P}(-1.23 \\le X \\le 0.43), \\quad \\mathbb{P}(X&gt;0.32)?\\]\n\nExercice 4  \n\nOn suppose que l’écart à la taille moyenne \\(T=15.5\\) des individus d’une population suit une loi normale centrée réduite.\nDans quel intervalle centré en \\(T\\) se situent les tailles de \\(99\\%\\) des individus de la population?\n\nExercice 5  \n\nEtant donnée X une variable aléatoire gaussienne de paramètres \\(\\mu\\) et \\(\\sigma^2\\), donner la probabilité que \\(|X - \\mu|\\) dépasse \\(k\\sigma\\) pour \\(k = 1, 2, 3\\).\nSuggestion: On commencera par montrer que \\(\\sigma^{-1}(X-\\mu)\\) suit une loi normale centrée réduite.\nReprendre les questions de l’exercice précédent lorsque \\(\\mu=2, \\sigma=2\\).\nReprendre les questions de l’exercice précédent lorsque \\(\\mu=0, \\sigma=1/2\\)."
  },
  {
    "objectID": "exercices/td1.html#densités",
    "href": "exercices/td1.html#densités",
    "title": "Variables aléatoires réelles",
    "section": "Densités",
    "text": "Densités\n\nExercice 6  \n\nDans les cas suivants, trouver la valeur de \\(C\\) pour que \\(f\\) soit une densité de probabilité.\n\n\\(f(x) = C \\frac{1}{\\sqrt{x (1-x)}}, 0 &lt;x &lt;1,\\)\n\\(f(x) = C \\exp(-x-\\exp(-x)), x \\in \\mathbb{R},\\)\n\\(f(x) = C \\frac{1}{1+x^2}\\).\n\n\nExercice 7  \n\n(Mélange)\nOn suppose que \\(X\\) et \\(Y\\) sont deux variables de densités respectives \\(f_X, f_Y\\), et que \\(\\alpha \\in [0,1]\\). Montrer que \\(g : = \\alpha f_X + (1-\\alpha) f_Y\\) est également une densité de probabilité.\nTrouver une variable aléatoire dont \\(g\\) est la densité.\n\nExercice 8  \n\nSoit \\(X\\) de densité \\(f\\), et \\((\\alpha, \\beta) \\in \\overline{\\mathbb{R}}^2\\) sont supposés tels que \\[\\mathbb{P}(\\alpha &lt; X &lt; \\beta) =1\\] On suppose que \\(g\\) est un \\(C^{1}\\)-difféomorphisme croissant de \\((\\alpha, \\beta)\\) sur \\((g(\\alpha),g(\\beta))\\).\n\nMontrer que \\(g(X)\\) a pour densité \\(\\frac{f(g^{-1}(x))}{g'(g^{-1}(x))} \\mathbf{1}_{\\{x \\in (g(\\alpha), g(\\beta))\\}}\\).\nQuelle est la densité de la variable \\(aX+b\\), où \\(a&gt;0\\) et \\(b\\in \\mathbb{R}\\) sont fixés?\n\nSoit \\(Y \\sim \\mathcal{N}(0,1)\\). Quelle est la densité de \\(Z = \\exp(Y)\\)?"
  },
  {
    "objectID": "exercices/td1.html#lois-usuelles-calculs-de-loi",
    "href": "exercices/td1.html#lois-usuelles-calculs-de-loi",
    "title": "Variables aléatoires réelles",
    "section": "Lois usuelles, calculs de loi",
    "text": "Lois usuelles, calculs de loi\n\nExercice 9  \n\n(Fonctions de répartition et fonctions caractéristiques de lois usuelles)\n\n\n\n\n\n\nAttention : dans le cas d’une variable continue, quand on calcule \\(\\Phi_X\\) on %doit intégrer sur \\(\\mathbb{R}\\) une fonction complexe. Trois méthodes sont envisageables.\nParfois on peut intégrer séparément partie réelle et partie imaginaire.\nParfois il est utile de se servir de la formule de Cauchy. En particulier, cette formule assure que si \\(f\\) est une fonction holomorphe, si \\(\\mathcal{C}\\) est un contour fermé “raisonnable” (en particulier tout cercle ou polygone régulier), et enfin si \\(\\overset{\\circ}{\\mathcal{C}}\\) désigne l’ensemble des points se trouvant à l’intérieur de ce contour, alors\n\\[\\forall a \\in \\overset{\\circ}{\\mathcal{C}}, \\quad f(a) = \\frac{1}{2\\pi i} \\oint_{\\mathcal{C}}  \\frac{f(z)}{z-a} \\mathrm{d}z.\\]\nAttention : cette formule montre bien que l’on ne peut pas traiter l’intégrale d’une fonction complexe en faisant “comme si” \\(i\\) était réél (!) La méthode des résidus est en outre une conséquence directe de la formule de Cauchy.\nEnfin, on peut utiliser le prolongement analytique (voir l’exemple de la fonction \\(\\Gamma\\)).\n\n\n\n\nExprimer le plus simplement \\(F_X\\) dans les cas suivants (on pourra se contenter de tracer l’allure du graphe de la fonction de répartition lorsque celle-ci ne possède pas d’expression simple).\n\n\\(n \\in \\mathbb{N}^*, p \\in [0,1], X \\sim \\text{Bin}(n,p)\\),\n\\(\\lambda&gt;0, X \\sim \\text{Poisson}(\\lambda)\\),\n\\(a&gt;0, X\\sim \\text{Unif}[-a,a]\\),\n\\(\\lambda&gt;0, X \\sim \\mathbf{exp}(\\lambda)\\),\n\\(\\lambda&gt;0, s \\in \\mathbb{N}^*\\), \\(X \\sim \\Gamma(\\lambda, s)\\) (on rappelle que la densité \\(f_X\\) de \\(X \\sim \\Gamma(\\lambda,s)\\) est telle que \\(f_X(x) =\\frac{1}{\\Gamma(s)} \\lambda^s x^{s-1}\\exp(-\\lambda x) \\mathbf{1}_{[0,\\infty[}(x), x \\in \\mathbb{R}\\)),\n\\(X \\sim \\mathcal{N}(0,1)\\),\n\\(\\mu \\in \\mathbb{R}, \\sigma&gt;0, X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\).\n\n\\(a&gt;0, X \\sim \\mathrm{Cauchy}(a)\\) (on rappelle que la densité \\(f_X\\) de la loi de Cauchy de paramètre \\(a\\) est telle que \\(f_X(x) = \\frac{a}{\\pi(x^2+a^2)}, x\\in \\mathbb{R}\\)).\n\n(*) \\(X \\sim \\mathrm{stable}(1/2)\\) (cette loi a pour densité \\(\\sqrt{2\\pi x^{-3}} \\exp(-1/2x)\\mathbf{1}_{[0,\\infty[}(x).\\))\n\nLesquelles parmi ces variables possèdent une densité?\n\nExprimer le plus simplement \\(\\Phi_X\\) pour les \\(7\\) premières variables de la première question ci-dessus. En déduire, ou trouver par un calcul direct, \\(E[X],\\) et \\(\\mathrm{Var}[X]\\).\n\n\nExercice 10  \n\nPour des valeurs de \\(t\\) que l’on précisera, calculer la transformée de Laplace \\(L(t) := \\mathbb{E}[\\exp(-t X)]\\) et la fonction génératrice des moments \\(G(u) := \\mathbb{E}[u^X]\\) de la variable \\(X\\) dans les cas suivants.\n\n\\(X \\sim \\mathrm{Ber}(p)\\), où \\(p \\in [0,1]\\),\n\\(X \\sim \\mathrm{Bin}(n,p)\\), où \\(n \\in \\mathbb{N}^*, p \\in [0,1]\\),\n\\(X \\sim \\mathrm{Geom}(p)\\), où \\(p \\in [0,1]\\),\n\\(X \\sim \\mathrm{Poisson}(\\lambda)\\), où \\(\\lambda&gt;0\\),\n\n\\(X=Y_1+...+Y_n\\), où les \\(Y_i, 1 \\le i \\le n\\) sont des variables indépendantes, et \\(Y_i \\sim \\mathrm{Poisson}(\\lambda_i)\\), avec \\(\\lambda_i &gt;0\\).\n\n\nExercice 11  \n\nSoit \\(X\\) une v.a.r. de densité \\(f\\). Quelle est la densité de \\(X^2\\)? Qu’obtient-on dans le cas où \\(X \\sim \\mathcal{N}(0,1)\\)?\n\nExercice 12  \n\nSoit \\(X \\sim \\exp(1)\\). Calculer la densité des variables suivantes :\n\n\\(Y = aX+b\\), où \\(a&gt;0\\) et \\(b \\in \\mathbb{R}\\). Qu’observe-t-on dans le cas où \\(b=0\\)?\n\n\\(Z = X^2\\).\n\\(U = \\exp(-X)\\).\n\n\nExercice 13  \n\nTrouver la loi de \\(\\arcsin(X)\\) lorsque\n\n\\(X \\sim \\mathrm{Unif}[0,1]\\),\n\\(X \\sim \\mathrm{Unif}[-1,1]\\).\n\n\nExercice 14  \n\nOn souhaite peindre un mur (infini!) en utilisant un arroseur automatique qui effectue des demi-révolutions successives. Pour simplifier le modèle, on représentera le mur par une droite verticale \\(\\Delta\\), et l’arroseur par une source ponctuelle \\(O\\) située à \\(1\\) mètre du mur, et émettant en tout instant \\(t\\) de façon parfaitement rectiligne dans la direction \\(\\overset{\\rightarrow}{u}(t)\\). On note \\(M\\) la projection orthogonale de \\(O\\) sur \\(\\Delta\\) et \\(\\theta(t)\\) l’angle entre \\(O \\overset{\\rightarrow}{u}(t)\\) et \\(\\overset{\\rightarrow}{OM}\\). L’intersection de \\(O\\overset{\\rightarrow}{u}\\) avec \\(\\Delta\\) est notée \\(H(\\theta)\\).\nOn suppose en outre que lors d’une demi-révolution, \\(\\theta(t)\\) parcourt exactement l’intervalle \\((-\\pi/2,\\pi/2)\\).\nOn fait l’hypothèse que la demi-révolution s’effectue à vitesse angulaire constante, et on se demande quelle sera la répartition de l’épaisseur de la couche de peinture le long de \\(\\Delta\\) après un nombre entier de demi-révolutions.\n\nJustifier qu’une particule de peinture choisie uniformément au hasard parmi toutes les particules est envoyée suivant un angle \\(\\theta \\sim \\mathrm{Unif}(-\\pi/2,\\pi/2)\\). Une telle particule se pose alors en \\(H(\\theta)\\), On note \\(h(\\theta)\\) l’ordonnée de \\(H(\\theta)\\) (c’est également la distance algébrique entre \\(O\\) et \\(H\\)).\nExprimer \\(h(\\theta)\\) en fonction de \\(\\theta\\). Quelle est la loi de \\(h(\\theta)\\)? Que pouvez-vous en déduire sur la distribution de l’épaisseur de la couche de peinture le long du mur?\nA posteriori, quelle critique peut-on formuler sur le modèle?\n\n\nExercice 15  \n\nSoit \\(X \\sim \\mathrm{Cauchy}\\) (de paramètre \\(1\\)).\nQuelle est la loi de\n\n\\(Y:=\\frac{1}{X}\\)?\n\\(Z:= \\frac{1}{1+X^2}\\)?\n\n\nExercice 16  \n\nSoit \\(Z \\sim \\mathcal{N}(0,1)\\). Montrer que pour tout \\(x&gt;0\\),\n\\[\\left(x^{-1}-x^{-3}\\right) \\exp(-x^2/2) \\le \\sqrt{2\\pi} \\mathbb{P}(Z&gt;x) \\le x^{-1} \\exp(-x^2/2).\\]\nIndication : on pourra penser à utiliser le changement de variable \\(y=x+z\\) pour obtenir l’inégalité de droite, et on commencera par calculer la dérivée de \\(\\left(x^{-1}-x^{-3}\\right) \\exp(-x^2/2)\\) pour obtenir celle de gauche.\n\nExercice 17 (Calcul d’une loi conditionnelle discrète)  \n\nSoient \\(X_1,...,X_n\\) des variables de Poisson, indépendantes, de paramètres respectifs \\(\\lambda_1,...,\\lambda_n\\).\n\nDéterminer la loi de \\(Y:=\\sum_{k=1}^n X_k\\).\nPour \\(r \\in \\mathbb{N}\\), que vaut la loi conditionnelle de \\((X_1,...,X_n)\\) sachant \\(Y=r\\)?"
  },
  {
    "objectID": "exercices/td1.html#exemples-divers",
    "href": "exercices/td1.html#exemples-divers",
    "title": "Variables aléatoires réelles",
    "section": "Exemples divers",
    "text": "Exemples divers\n\nExercice 18  \n\nOn suppose que le nombre \\(X\\) d’oeufs pondus par un insecte suit une loi de Poisson de paramètre \\(\\lambda&gt;0\\) et que la probabilité qu’un oeuf meurt sans éclore est, indépendamment des autres oeufs, égale à \\(1-p\\), où \\(p \\in ]0,1[\\).\n\nDémontrer que le nombre \\(Y\\) d’oeufs qui arrivent à éclosion suit une loi de Poisson de paramètre \\(\\lambda p\\).\nQuelle est la loi jointe de \\((Y,Z)\\), où \\(Z= X-Y\\) est le nombre d’oeufs morts avant éclosion?\n\n\nExercice 19  \n\n(Somme d’exponentielles et \\(\\chi^2\\))\nSoit \\(\\lambda&gt;0\\) et \\((Y_i)_{1 \\le i \\le n}\\) des variables i.i.d, \\(\\sim \\exp(\\lambda)\\).\n\nCalculer \\(\\mathbb{E}[\\exp(-tY_1)]\\) pour \\(t \\ge 0\\).\n\nCalculer \\(\\mathbb{E}\\left[\\exp(-t\\sum_{i=1}^n) Y_i \\right]\\) pour \\(t \\ge 0\\).\nMontrer que la densité de la variable \\(X= \\sum_{i=1}^n Y_i\\) est proportionnelle à \\(x^{n-1} \\exp(-\\lambda x) \\mathbf{1}_{x \\ge 0}\\). En déduire la valeur de cette densité.\nSoient \\(X_n, n \\ge 1\\) des variables i.i.d, \\(\\sim \\mathcal{N}(0,1)\\). On pose \\(\\chi^2(n):=\\sum_{i=1}^n X_i^2\\), \\(Z:=X_1X_2 +X_3X_4\\). \\ Calculer \\(\\Phi_{\\chi^2(n)}, \\Phi_Z\\). Pouvez-vous deviner la distribution de \\(Z\\) (on pourra utiliser un résultat d’un exercice précédent)?\n\n\nExercice 20  \n\nSoit \\(Z = (X, Y )\\), une variable aléatoire à valeurs dans \\(\\mathbb{R}^2\\). On suppose que \\(Z\\) admet une densité \\(f\\) définie par\n\\[f(x, y) = \\frac{4}{\\pi \\sigma^2} \\exp\\left(-\\frac{x^2+y^2}{\\sigma^2}\\right) \\mathbf{1}_{\\{x \\ge |y|\\}},\\]\noù \\(\\sigma&gt;0\\).\n\nVérifier que \\(f\\) est bien une densité de probabilité.\nCalculer les lois de \\(X\\) et de \\(Y\\) . Les variables aléatoires \\(X\\) et \\(Y\\) sont-elles indépendantes ?\nCalculer la loi de \\((X - Y, X + Y )\\) et montrer que \\(X - Y\\) et \\(X + Y\\) sont indépendantes.\n\n\nExercice 21  \n\n\nSoit \\(X_1,...,X_n\\) des variables i.i.d, \\(\\sim \\mathcal{N}(0,1)\\), et \\(Z=\\sum_{i=1}^n \\alpha_i X_i\\) (où les \\(\\alpha_i, i=1,...,n\\) sont de rééls fixés). Quelle est la loi de \\(Z\\)?\nSoit \\((X_1,...,X_n) \\sim \\mathcal{N}(0,A)\\). Quelle est la loi de \\(Z=\\sum_{i=1}^n \\alpha_i X_i\\)?\nSoit \\((X_1,...,X_n) \\sim \\mathcal{N}(0,A)\\). Quelle est la loi de \\((Z_1,Z_2)\\), où, pour des rééls \\(\\alpha_{i,1}, \\alpha_{i,2}, i=1,...,n\\) fixés, \\[  Z_1=\\sum_{i=1}^n \\alpha_{i,1} X_i,  Z_2=\\sum_{i=1}^n \\alpha_{i,2} X_i.\\]\nGénéraliser la question précédente en exprimant la loi de \\[ (Z_1,...,Z_n) =  (X_1,...,X_n) P,\\] où \\(P\\) est une matrice \\(n \\times n\\).\nSoient \\(X,Y\\) deux variables indépendantes, \\(\\sim \\mathrm{Unif}[0,1]\\). Quelle est la loi de \\(S=X+Y\\)?\nSoient \\(X, Y\\) des variables indépendantes de loi respectives \\(\\Gamma(a,c), \\Gamma(b,c)\\), où \\(a,b,c&gt;0\\). On pose \\(S=X+Y,\nT= \\frac{X}{X+Y}\\) Quelle est la loi du couple \\((S,T)\\)?\nSoient \\(X,Y\\) deux variables indépendantes, \\(\\sim \\mathrm{Unif}[0,1]\\). On pose \\(U = \\sqrt{-2\\log(X)} \\cdot \\cos(2\\pi Y), V = \\sqrt{-2\\log(X)} \\cdot \\sin(2\\pi Y)\\). Quelle est la loi du couple \\((X,Y)\\)?\nSoient \\(X,Y\\) deux variables indépendantes, \\(\\sim \\mathcal{N}(0,1)\\). On pose \\(T = \\frac{Y}{X}\\). Quelle est la loi de \\(T\\)?\nSoient \\((X,Y)\\) un couple de variables indépendantes, \\(\\sim \\mathcal{N}(0,1)\\). On pose \\(U=X, V= X^2 + Y^2\\). Quelle est la loi de \\((U,V)\\)?\nSoit \\(X\\) de densité \\(\\exp(-x) \\mathbf{1}_{\\mathbb{R}_+}(x)\\). On pose \\(U = [X]\\) et \\(V= X-[X]\\), la partie entière, resp. la partie décimale de \\(X\\). Quelle est la loi de \\((U,V)\\)?\n\n\nExercice 22  \n\nSoient \\(X_1, X_2\\) deux variables indépendantes et identiquement distribuées suivant la loi \\(\\mathrm{Unif}\\{1,2,3\\}\\).\nOn note \\(U = \\min\\{X_1,X_2\\}\\), \\(V= \\max\\{X_1,X_2\\}\\) et enfin \\(S= U+V\\).\n\nDéterminer la loi jointe de \\((U, V)\\) et \\((V, S)\\).\nEn déduire les lois de \\(U, V\\), et \\(S\\). Calculer les lois de \\(UV\\) et \\(VS\\).\nCalculer les covariances et les coefficients de corrélation de \\((U, V)\\) et \\((V, S)\\)."
  },
  {
    "objectID": "exercices/td1.html#le-cadre-gaussien",
    "href": "exercices/td1.html#le-cadre-gaussien",
    "title": "Variables aléatoires réelles",
    "section": "Le cadre gaussien",
    "text": "Le cadre gaussien\n\nExercice 23  \n\nOn considère deux variables indépendantes \\(Y \\sim \\mathcal{N}(0,1)\\) et\n\\[\\varepsilon= \\begin{cases} & 1 \\mbox{ avec probabilité } p \\\\ & -1 \\mbox{ avec probabilité } 1-p, \\end{cases}\\]\noù \\(p \\in (0,1)\\).\n\nQuelle est la loi de \\(Z= \\varepsilon Y\\)\nQuelle est la loi de \\(Y+Z\\)?\nLe vecteur \\((Y,Z)\\) est-il un vecteur gaussien?\n\n\nExercice 24  \n\nSoit \\((X,Y,Z)\\) le vecteur aléatoire gaussien d’espérance \\((1,1,0)\\) et de matrice de covariance\n\\[K = \\begin{pmatrix} 2 & 1 & 1 \\\\ 1 & 2 & 2 \\\\ 1 & 2 & 2  \\end{pmatrix}.\\]\n\nEcrire la fonction caractéristique de \\((X,Y,Z)\\).\n\nTrouver la loi de \\(2X+Y+Z\\), de \\(4X-2Y+Z\\), enfin de \\(Y-Z\\).\nLe vecteur \\((X,Y)\\) admet-il une densité dans \\(\\mathbb{R}^2\\). Si oui, laquelle?\nPour \\(a \\in \\mathbb{R}\\) on définit \\(u_a : \\mathbb{R}^3 \\to \\mathbb{R}^3\\) de matrice\n\\[A = \\begin{pmatrix} 1/\\sqrt{2} & 0 & 0 \\\\ a & -2a & 0 \\\\ 0 & 1 & -1\\end{pmatrix}.\\]\nDéterminer la loi de \\(u_a(X-1,Y-1,Z)\\) en fonction de \\(a\\).\nPour quelle valeur de \\(a\\) les deux premières coordonnées de \\(u_a(X-1,Y-1,Z)\\) suivent-ils une loi normale centrée réduite sur \\(\\mathbb{R}^2\\)?\n\n\nExercice 25  \n\nSoit \\(\\rho\\in ]-1,1[\\) et \\((X,Y)\\) un vecteur gaussien centré de matrice de covariances \\(M = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}\\). On notera \\(\\sigma = \\sqrt{1-\\rho^2}\\).\n\nCalculer det(\\(M\\)), \\(M^{-1}\\), puis exprimer la densité \\(f_{(X,Y)}\\) du vecteur \\((X,Y)\\).\nMontrer que\n\\[g_x(y) := \\frac{f_{(X,Y)}(x,y)}{f_X(x)} = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp \\left( - \\frac{1}{2(1-\\rho^2)} \\left(y-\\rho x\\right)^2\\right).\\]\nMontrer que pour tout \\(x \\in \\mathbb{R}\\), \\(y \\to g_x(y)\\) définit une densité.\nSi on note \\(Y_x\\) une variable de densité \\(g_x\\), que pouvez-vous dire sur la loi de \\(Y_x\\)?\n\nTrouver \\(\\alpha\\), \\(\\beta\\) deux réels tels que \\((X, \\alpha X + \\beta Y)\\) suit la loi normale centrée réduite.\n\nRemarquer que l’on peut écrire \\(Y= -\\frac{\\alpha}{\\beta} X +\\frac{1}{\\beta}(\\alpha X + \\beta Y)\\). Sauriez-vous dire pourquoi cette écriture est intéressante?\n\nExercice 26  \n\nMontrer que le vecteur aléatoire de dimension \\(3\\) de moyenne \\(m = (7,0,1)\\) et de matrice de covariances\n\\[K = \\begin{pmatrix} 10 & -1 & 4 \\\\ -1 & 1 & -1 \\\\ 4 & -1 & 2 \\end{pmatrix}\\]\nappartient presque sûrement à un hyperplan affine de \\(\\mathbb{R}^3\\) que l’on déterminera."
  },
  {
    "objectID": "exercices/quizz_a.html",
    "href": "exercices/quizz_a.html",
    "title": "MA1AY010 Probabilités",
    "section": "",
    "text": "NoteQuestionnaire I\n\n\n\n\n8 Septembre 2025-12 Septembre 2025\nMaster I Isifar\nProbabilités\n\n\n\n\nExercice 6  \n\nEst-il vrai qu’une fonction croissante sur \\(\\mathbb{R}\\) est toujours mesurable ?\nSi \\(X,Y\\) sont indépendantes, de lois à densité sur \\(\\mathbb{R}\\), est-il vrai que \\(\\mathbb{P} \\{ X= Y \\}=0\\)?\n\n\nExercice 5 Soit \\(X\\) une variable aléatoire réelle, dont la loi admet une densité \\(f\\) par rapport à la mesure de Lebesgue. Soit \\(a\\) un réel positif et \\(b\\) un réel quelconque.\n\nLa loi de la variable aléatoire \\(aX +b\\) admet-elle une densité ?\nSi oui, exprimer cette densité en fonction de \\(f, a, b\\).\n\n\nExercice 4 Sur un espace de probabilité \\((\\Omega, \\mathcal{F}, P)\\), on dispose d’une famille infinie de variables aléatoires indépendantes \\(X_1, X_2, \\ldots, X_n, \\dots\\) toutes distribuées selon la loi exponentielle standard (densité \\(\\mathrm{e}^{-x}\\) sur \\([0, \\infty)\\)).\nOn définit \\(Y_n = \\sum_{i=1}^n \\left( X_i/i - \\frac{1}{i}\\right)\\) pour tout \\(n\\).\n\nCalculer l’espérance et la variance de \\(Y_n\\).\nLa suite \\((Y_n)_n\\) converge-t-elle dans \\(\\mathcal{L}_2\\) ?\nSi oui, pouvez-vous caractériser la loi de la limite?\n\n::: {#exr- name=““} (Inégalité d’association)\nSoit \\(X\\) une variable aléatoire réelle sur un espace de probabilité \\((\\Omega, \\mathcal{F}, P)\\) et \\(f,g\\) deux fonctions croissantes positives sur \\(\\mathbb{R}\\).\nMontrer que \\[\n\\mathbb{E} \\left[ f(X) g(X)\\right] \\geq \\mathbb{E}\\left[ f(X)\\right]  \\times \\mathbb{E} \\left[ g(X) \\right] \\, .\n\\]\n\nExercice 3 Soit \\(X_1, \\ldots, X_n\\) une suite de variables aléatoires i.i.d. selon une loi exponentielle. On note \\((X_{i,n})_{i\\leq n}\\) le réarrangement croissant de \\(X_1, \\ldots, X_n\\), (les statisques d’ordre)\n\nQuelle est la loi de \\(X_{1,n}\\), le minimum ?\nQuelle est la loi de \\(X_{2,n} - X_{1, n}\\) ?\nQuelle est la densité jointe de la loi de \\((X_{i,n})_{i\\leq n}\\) ?\nQuelle est la densité jointe de la loi de \\((X_{i,n} - X_{i-1,n})_{i\\leq n}\\) avec la convention \\(X_{0,n}=0\\)?\n\n\nExercice 2 Soit \\(X\\) une variable aléatoire distribuée selon une loi binomiale de paramètres \\(n\\) et \\(p \\in (0,1)\\). Montrer que \\[\n\\frac{1}{\\mathbb{E}X+1} &lt; \\mathbb{E}\\frac{1}{X+1} \\leq \\frac{1}{\\mathbb{E}X + p} \\, .\n\\] Montrer que si \\(X\\) est distribuée selon une loi de Poisson \\[\n\\mathbb{E}\\frac{1}{X+1} = \\frac{1 - \\mathrm{e}^{-\\mathbb{E}X}}{\\mathbb{E}X} \\, .\n\\] On note \\(E\\) l’événement \\(\\{ X &gt;0 \\}\\) montrer que \\[\n\\mathbb{E}_{P (\\cdot|E)}\\left[ \\frac{1}{X}\\right] \\geq \\mathbb{E}\\frac{1}{X+1} \\, .\n\\]\n\nExercice 1 Le jeu oppose un présentateur à un candidat (le joueur). Ce joueur est placé devant trois portes fermées. Derrière l’une d’elles se trouve une voiture et derrière chacune des deux autres se trouve une chèvre. Il doit tout d’abord désigner une porte. Puis le présentateur doit ouvrir une porte qui n’est ni celle choisie par le candidat, ni celle cachant la voiture (le présentateur sait quelle est la bonne porte dès le début). Le candidat a alors le droit d’ouvrir la porte qu’il a choisie initialement, ou d’ouvrir la troisième porte.\nOn suppose que l’organisateur du jeu place la voiture uniformément au hasard derrière l’une des trois portes avant la partie.\nLes questions qui se posent au candidat sont :\n\nQue doit-il faire ?\nQuelles sont ses chances de gagner la voiture en agissant au mieux ?"
  },
  {
    "objectID": "exercices/td1-supplement.html",
    "href": "exercices/td1-supplement.html",
    "title": "MA1AY010 Probabilités",
    "section": "",
    "text": "NoteTD I : Révisions de Licence (exercices supplémentaires)\n\n\n\n\n8 Septembre 2025-12 Septembre 2025\nMaster I Isifar\nProbabilités"
  },
  {
    "objectID": "exercices/td1-supplement.html#pour-aller-plus-loin",
    "href": "exercices/td1-supplement.html#pour-aller-plus-loin",
    "title": "MA1AY010 Probabilités",
    "section": "Pour aller plus loin",
    "text": "Pour aller plus loin\n\nExercice 1 (un (contre)-exemple à garder en tête)  \n\nOn note \\(C\\) l’ensemble de Cantor, qui est obtenu, récursivement, en enlevant \\(I_1^{(1)} : = (1/3,2/3)\\) à l’intervalle \\([0,1]\\), puis en enlevant les tiers intermédiaires \\(I_2^{(1)} = (1/9,2/9), I_2^{(2)}= (7/9,8/9)\\) des deux intervalles restants, et ainsi de suite, de sorte qu’à l’etape \\(n\\) on enlève \\(2^{n-1}\\) intervalles tous de taille \\(1/3^n\\), notés \\(I_n^{(j)},j=1...2^{n-1}\\).\nOn définit la fonction croissante :\n\\[F(x) := \\begin{cases}  0 & \\text{ si } x \\le 0 \\\\    1 & \\text{ si } x \\ge 1 \\\\ \\frac{2j-1}{2^n} \\text{ si } x \\in I_n^{(j)}. \\end{cases}\\]\n\nLa fonction \\(F\\) est-elle continue?\nDans quel ensemble la variable \\(X\\), de fonction de répartition \\(F\\) —définie ci-dessus— prend elle ses valeurs? Que vaut \\(\\mathbb{E}[X]\\)? Que vaut \\(\\mathbb{P}(|X-\\mathbb{E}[X]|&lt;1/8)\\)? Pour \\(x \\in C\\), que vaut \\(\\mathbb{P}(X=x)\\)?\nJustifier que la variable \\(X\\) est uniforme sur \\(C\\). Admet-elle une densité sur \\([0,1]\\)?\n\nQuelle est la loi de \\(Y=F(X)\\)?\nPourrait-on ainsi définir une variable aléatoire uniforme sur \\(\\mathbb{Q}\\cap [0,1]\\)?\nSoient \\(\\{X_j, j \\in \\mathbb{N}^* \\}\\) une famille de variables i.i.d., distribuées suivant la loi de Bernoulli de parmaètre \\(1/2\\). On définit \\(X=2 \\sum_{j \\ge 1} \\frac{X_j}{3^j}\\). Quelle est la loi de \\(X\\)?\nCalculer \\(\\Phi_X\\) et montrer qu’elle est constante sur \\(\\{3^k \\pi, k \\in \\mathbb{N}^* \\}\\).\n\n\nExercice 2  \n\nDans ce qui suit \\(||.||\\) désigne la norme euclidienne.\nPour \\(\\varepsilon \\in (0,1)\\), \\(n \\in \\mathbb{N}\\)\non note \\(A_{n, \\varepsilon} := \\{ x \\in \\mathbb{R}^n : (1-\\varepsilon)(n/3)^{1/2} \\le ||x|| \\le (1+\\varepsilon)(n/3)^{1/2}\\}\\), et \\(C_n=[-1,1]^n\\).\nMontrer que pour tout \\(\\varepsilon \\in (0,1)\\), la mesure de Lebesgue de \\(C_n \\cap A_{n, \\varepsilon}^c\\) est négligeable, pour \\(n \\to \\infty\\), devant celle de \\(C_n\\).\nIndication : On notera qu’un point générique dans \\(C_n\\) a pour coordonnées \\((X_1,...,X_n)\\), avec \\(\\{X_i, i \\ge 1\\}\\) une famille de variables aléatoires i.i.d, uniformes sur \\([-1,1]\\).\nNote : Ceci signifie que pour \\(\\varepsilon\\) aussi petit qu’on souhaite le prendre, lorsque \\(n\\) est grand, la majeure partie du volume de l’hypercube \\(C_n\\) se trouve dans la couronne \\(A_{n,\\varepsilon}\\) (!)\n\nExercice 3  \n\nSoit \\(X \\sim \\Gamma(1,s)\\). On définit \\(Y \\sim \\mathrm{Poisson}(X)\\), (c’est-à-dire que conditionnellement à \\(X=x\\), \\(Y \\sim \\mathrm{Poisson}(x)\\)).\n\nCalculer \\(\\Phi_Y\\).\nMontrer que lorsque \\(s \\to \\infty\\), \\[\\frac{Y-E[Y]}{\\sqrt{\\mathrm{var}(Y)}} \\overset{(\\mathrm{loi})}{\\longrightarrow} Z,\\] où \\(Z \\sim \\mathcal{N}(0,1)\\). Y a-t-il un lien avec le théorème central limite?\n\n\nExercice 4  \n\n(Une remarque importante sur la convergence en loi.)\nSoit, pour \\(n \\ge 1\\), \\(X_n\\) un variable aléatoire dont la fonction de répartition est définie par \\[ F_n(x) = x - \\frac{\\sin(2n \\pi x)}{2n\\pi}, \\ 0 \\le x \\le 1.\\]\n\nMontrer que \\(X_n\\) converge en loi vers une variable \\(X\\) (dont on décrira la loi).\n\nVérifier que \\(X_n\\) et \\(X\\) sont des variables à densité. On note \\(f_n := F_n'\\) la densité de \\(X_n\\), \\(f\\) celle de \\(X\\). A-t-on \\(f_n \\underset{n \\to \\infty}{\\rightarrow} f\\) simplement?\n\n\nExercice 5  \n\n(Le problème des anniversaires)\nOn cherche a approximer la probabilité que parmi \\(N\\) personnes, au moins deux (resp. trois) aient leur anniversaire le même jour.\n\nPour \\(1 \\le i &lt;j \\le N\\), on note \\(E_{ij}\\) l’événement que les personnes \\(i,j\\) aient le même anniversaire. Quelle est la probabilité de \\(E_{ij}\\)? Dans la suite de l’exercice on omet de considérer le cas des \\(29\\) février. Que vaut alors \\(P(E_{ij})\\)? Montrer que les événements \\(E_{12}, E_{23}, E_{13}\\) sont indépendants deux à deux, mais pas mutuellement indépendants. Exprimer la probabilité qu’il y ait au moins deux personnes qui aient leur anniversaire le même jour.\nLe but de cette question n’est pas d’obtenir un résultat exact, mais de faire une approximation assez bonne de la probabilité que parmi \\(N\\) personnes, il y en ait au moins deux qui partagent le même anniversaire. On admettra donc que dans la suite on fait l’approximation que les \\(\\{E_{ij}, 1 \\le i &lt; j \\le N\\}\\) sont mutuellement indépendants. Quelle est alors la loi du nombre de couples ayant le même anniversaire? Approximer le résultat pour \\(N=30, N=60\\).\n\nPar une méthode similaire, approximer la probabilité que parmi \\(60\\) personnes, il y en ait au moins trois qui ont leur anniversaire le même jour. Finalement, approximer la probabilité de cet événement lorsque \\(N=90\\).\n\n\nExercice 6  \n\n(Quelques variantes du modèle de branchement aléatoire)\nOn considère le modèle de branchement aléatoire qui est un modèle pour l’évolution d’une population haploïde.\nOn part initialement de \\(Z_0\\) ancêtres, et on note \\(Z_n\\) le nombre total d’individus de la génération \\(n\\).\nOn fait l’hypothèse (pas très réaliste) que tous les individus ont exactement le même temps de vie (disons \\(1\\)), et que de plus deux générations distinctes ne cohabitent pas. Ainsi chacun des ancêtre meurt au temps \\(1\\), et donne naissance à un nombre aléatoire d’individus, de façon indépendante de ses contemporains. Tous les individus qui sont nés au temps \\(1\\) forment la génération \\(1\\). La génération \\(1\\) meurt au temps \\(2\\) en donnant naissance à la génération \\(2\\), etc…\nFormellement, on se donne une loi de branchement \\(\\nu\\) sur les entiers positifs, telle que \\(\\nu(1)&lt; 1\\) et \\(\\sum_{k \\ge 0} k \\nu(k) = \\mu \\in [0,\\infty)\\) On introduit alors une famille \\((X,X_{i,k}, i \\ge 0, k\\ge 1)\\) de variables aléatoires i.i.d, toutes de loi \\(\\mu\\). Si jamais la génération \\(i\\) possède un \\(k\\)-ième individu, \\(X_{i,k}\\) désigne le nombre de ses descendants.\n\nPour \\(n \\ge 1\\), exprimer \\(Z_n\\) en fonction de \\(Z_{n-1}\\) et des \\((X_{n-1,k}, k \\ge 1)\\).\nOn note \\(G_{X}(t) = E[t^{X}]\\), \\(G_{Z_n}(t) = E[t^{Z_n}]\\) (qui ne sont autres, modulo un simple changement de variable, que les fonctions génératrice des moments de \\(X\\), \\(Z_n\\)). Exprimer \\(G_{Z_n}\\) en fonction de \\(G_X, G_{Z_0}\\).\nOn note \\(\\zeta\\) le probabilité d’extinction. A quelle condition sur \\(\\mu\\) a-t-on \\(\\zeta=1\\) p.s.?\nIndication : On pourra considérer la suite \\(\\zeta_n = \\mathbb{P}(Z_n=0)\\)\nUne première martingale, Calculer \\(\\mathbb{E}[Z_{n-1}|Z_n]\\). On pose \\(M_{n}=Z_n/\\mu^n\\). Calculer \\(\\mathbb{E}[M_{n+1}|M_n]\\).\n\nPour plus de détails, on pourra voir l’introduction du livre de D.WILLIAMS, Probability with martingales, Cambridge University Press, 1991.\n\nOn considère (dans cette question uniquement) un processus de branchement avec immigration. On garde la même dynamique : à sa mort, un individu donne naissance à un nombre al'atoire de descendants suivant la loi \\(\\mu\\). De plus, chaque génération voit maintenant arriver un nombre aléatoire d’immigrants, indistinguables des individus déjà présents.\nFormellement, on se donne une loi \\(\\nu\\) sur \\(\\mathbb{N}\\) et on introduit \\(Y,Y_1,...Y_n\\) des variables \\(i.i.d.\\) de loi \\(\\nu\\). La génération \\(n\\) reçoit \\(Y_n\\) individus au temps \\(n\\). On note \\(G_Y(t)= E[t^{Y}]\\). Trouver une relation entre \\(G_{Z_n}, G_Y\\) et \\(G_{Z_0}\\).\nEnfin, on considère un processus de branchement dont les individus n’ont pas tous un temps de vie égal à \\(1\\). Pour simplifier un peu, on va seulement considérer la descendance d’un unique ancêtre, i.e. on pose \\(Z_0=1\\). \\ Formellement, on se donne une loi \\(P_{T}\\) sur \\(\\mathbb{R}_+\\), dont la densité est notée \\(f_T\\). On définit famille de variables aléatoires \\((T_{i}, i \\in \\mathbb{N})\\) i.i.d, de loi \\(P_T\\). On peut numéroter les individus en les classant suivant leur date de naissance. On définit alors \\(T_i\\) comme le temps de vie du \\(i\\)-ième individu.\nPour \\(t \\ge 0\\), on note \\(Z_t\\) le nombre total d’individus qui sont en vie au temps \\(t\\). On note enfin \\(G_t(s) = E[s^{Z_t}]\\).\n\n\nMontrer que \\(G_t(s) = \\int_0^{\\infty} E[s^{Z_t} |T_0 =u] f_T(u) du\\)\n\nEn déduire \\[ G_t(s) = \\int_0^t G_X(G_{t-u}(s))f_T(u) du + \\int_t^{\\infty} sf_T(u) du.\\]\n\nDans le cas où \\(f_T(s)= \\lambda \\exp(-\\lambda s)\\), montrer alors que \\[\\frac{\\partial t}{\\partial t} G_t(s) = \\lambda \\left[ G_X(G_t(s)) - G_t(s)\\right]  \\]"
  },
  {
    "objectID": "exercices/td2-supplement.html",
    "href": "exercices/td2-supplement.html",
    "title": "MA1AY010 Probabilités",
    "section": "",
    "text": "NoteTD II : Espérances et lois conditionnelles (supplément)\n\n\n\n22 Septembre 2025-26 Septembre 2025\n\nMaster I Isifar\nProbabilités\n\n\n\n\nExercice 1 (Questionnaire)  \n\nSoient \\((\\Omega; \\mathcal{A}; P)\\) un espace de probabilité, \\(X\\) et \\(Y\\) des v.a.r., \\(T\\) une v.a. à valeurs dans \\(\\mathbb{R}^d\\).\nQue peut-on dire, sous réserve d’hypothèses d’intégrabilité adéquates, des espérances conditionnelles suivantes :\n\n\\(\\mathbb{E}(f(T)\\mid T)\\) avec \\(f : \\mathbb{R}^d \\to \\mathbb{R}\\) borélienne,\n\\(\\mathbb{E}(X\\mid T)\\) avec \\(X\\) \\(\\ \\sigma(T)\\)-mesurable,\n\\(\\mathbb{E}(XY \\mid T)\\) avec \\(X \\ \\sigma(T)\\)-mesurable,\n\\(\\mathbb{E}(f(X)\\mid T)\\) avec \\(f : \\mathbb{R}^d \\to \\mathbb{R}\\) borélienne, \\(X\\) et \\(T\\) indépendantes,\n\\(\\mathbb{E}(\\mathbb{E}(X \\mid T))\\),\n\\(\\mathbb{E}[S_{10} |S_{8}]\\) lorsque \\(S_n = \\sum_{i=1}^n X_i\\) et les \\((X_i)_{i \\ge 1}\\) sont i.i.d.,\n\\(\\mathbb{E}[S_{31}\\mid X_1]\\) lorsque \\(S_n = \\sum_{i=1}^n X_i\\) et les \\((X_i)_{i \\ge 1}\\) sont i.i.d.,\n\\(\\mathbb{E}[\\Pi_{4} \\mid \\Pi_2]\\) lorsque \\(\\Pi_n = \\prod_{i=1}^n X_i\\) et les \\((X_i)_{i \\ge 1}\\) sont i.i.d.,\n\\(\\mathbb{E}[\\phi(X,Y) \\mid Y]\\) lorsque \\(X\\) et \\(Y\\) sont indépendantes,\n\\(\\mathbb{E}[f(S_2+X_8) \\mid S_2]\\), lorsque \\(S_n = \\sum_{i=1}^n X_i\\) et les \\((X_i)_{i \\ge 1}\\) sont i.i.d.\n\n\nExercice 2  \n\nOn considère un processus de Galton-Watson de loi de branchement \\[ \\mathbb{P}(\\xi=0)= \\mathbb{P}(\\xi=2)=1/2.\\] issu à la génération \\(0\\) d’un unique individu ancestral. On note \\(Z_n\\) la taille de la population à la génération \\(n\\).\nExprimer \\(\\mathbb{E}[(Z_{2}-1)^2 \\mid Z_1].\\)"
  },
  {
    "objectID": "exercices/td3.html",
    "href": "exercices/td3.html",
    "title": "MA1AY010 Probabilités",
    "section": "",
    "text": "NoteTD III : Processus de branchement\n\n\n\n\n18 Septembre 2025-25 Septembre 2025\nMaster I ISIFAR\nProbabilités\n\n\n\nUn processus de Galton-Watson (processus de branchement homogène) est un processus stochastique en temps discret utilisé pour modéliser l’évolution d’une population où chaque individu se reproduit indépendamment des autres, suivant une même loi de probabilité donnée (appelée loi de reproduction).\nIl a été introduit au XIXᵉ siècle par Francis Galton et Henry Watson our étudier la probabilité d’extinction des noms de famille (nobles). On commence avec une génération initiale (génération \\(0\\)) formée d’un individu. Chaque individu de la génération \\(n\\) engendre un nombre aléatoire de descendants distribué selon la loi de reproduction. Les descendants forment la génération suivante (\\(n+1\\)). Les nombres de descendants des individus de la géneration \\(n\\) forment une famille indépendante. Le nombre d’individus dans la génération \\(n\\) est noté \\(Z_n\\).\nLa question centrale (et angoissante) est : la population s’éteint-elle presque sûrement (\\(Z_n \\to 0\\)) ou bien survit-elle avec une probabilité non nulle ?\nOn note \\(Q\\) la loi de reproduction (loi sur \\((\\mathbb{N}, 2^{\\mathbb{N}})\\). On note \\(\\mu\\) son espérance. On note \\(G_Q\\) la fonction génératrice des probabilités de la loi \\(Q\\).\nOn convient de \\(\\mathbb{N}^* = \\mathbb{N}\\setminus \\{0\\}\\).\n\nExercice 1 (Branchement (Modélisation))  \n\nProposer une formalisation, c’est à dire un univers \\(\\Omega\\), une tribu \\(\\mathcal{F}\\) de parties de \\(\\Omega\\), et une loi de probabilité \\(P\\) sur \\((\\Omega, \\mathcal{F})\\) sur lesquels on peut définir la collection de variables aléatoires \\(Z_0, Z_1, \\ldots, Z_n, \\ldots\\). Préciser la loi conditionnelle de \\(Z_{n+1}\\) sachant \\(\\{ Z_n =k\\},  k \\in \\mathbb{N}\\).\n\nExercice 2 (Branchement, Espérance conditionnelle)  \n\nCalculer l’espérance conditionnelle de \\(Z_{n+1}\\) sachant \\(\\sigma(Z_n)\\)\n\nExercice 3 (Branchement. Espérance taille des générations)  \n\nCalculer \\(\\mathbb{E} Z_n\\) en fonction de \\(\\mu\\) et \\(n\\).\n\n\n\n\n\n\nImportantConvention\n\n\n\nSelon la valeur de \\(\\mu\\) (espérance de la loi de reproduction), on distingue trois cas :\n\n\\(\\mu&lt;1\\), cas sous-critique\n\\(\\mu=1\\), cas critique\n\\(\\mu&gt;1\\), cas sur-critique\n\nDans tous les cas, on note \\(p_E\\) la probabilité d’extinction (probabilité de l’événement \\(\\cup_{n \\in \\mathbb{N}} \\{ Z_n =0\\}\\)).\n\n\n\nExercice 4 (Branchement. Cas sous-critique)  \n\nCacluler \\(p_E\\), la probabilité d’extinction dans le cas sous-critique.\n\nExercice 5 (Branchement. Cas sûr-critique)  \n\nMontrer que dans tous les cas, \\(p_E\\) est solution de l’équation \\(G_Q(x)=x \\, .\\)\n\nExercice 6 (Branchement. Cas sur-critique I)  \n\nÉtudier les solutions de l’équation \\(x=G_Q(x)\\).\n\nExercice 7 (Branchement. Cas sur-critique II)  \n\nDéterminer \\(p_E\\) dans le cas sur-critique."
  },
  {
    "objectID": "exercices/td6.html",
    "href": "exercices/td6.html",
    "title": "MA1AY010 Probabilités",
    "section": "",
    "text": "NoteTD VI : Gaussiennes\n\n\n\n\n13 Octobre 2025-17 octobre 2025\nMaster I ISIFAR\nProbabilités\n\n\n\n\nExercice 1 (Lemme de Stein)  \n\nVérifier que les deux propriétés suivantes sont équivalentes:\n\n\\(X \\sim \\mathcal{N}(0,1)\\)\nPour toute fonction \\(g\\) absolument continue avec une dérivée \\(g'\\) telle que \\(\\mathbb{E}[ |X g(X)|]&lt;\\infty\\), on a\n\n\\(g'(X)\\) intégrable\n\n\\(\\mathbb{E}[g'(X)] = \\mathbb{E}[Xg(X)]\\)\n\n\n\nExercice 2 (Invariance par rotation) Si \\(X \\sim \\mathcal{N}(0, \\text{Id}_n)\\), et \\(A\\) est une matrice orthogonale (\\(A \\times A^\\top = A^\\top \\times A =\\text{Id}_n\\)), comment est distribué \\(A X\\) ?\n\n\nExercice 3 (Maxima de Gaussiennes)  \n\nVérifier que si \\(X_1, \\ldots, X_n \\sim_{\\text{i.i.d.}} \\mathcal{N}(0,1)\\):\n\\[\\mathbb{E}\\left[\\max(X_1, \\ldots, X_n) \\right] \\leq \\sqrt{2 \\ln n}\\]\nSuggestion : Majorer \\(\\mathbb{E} \\mathrm{e}^{\\lambda \\max(X_1, \\ldots, X_n )}\\) en comparant à \\(\\mathbb{E} \\sum_{i=1}^n \\mathrm{e}^{\\lambda X_i}\\). Comparer \\(\\mathbb{E}  \\mathrm{e}^{\\lambda \\max(X_1, \\ldots, X_n )}\\) et \\(\\mathrm{e}^{ \\lambda \\mathbb{E} \\max(X_1, \\ldots, X_n )}\\).\n\nExercice 4 (Norme de vecteurs gaussiens centrés)  \n\nMontrer que \\(X\\) est un vecteur gaussien centré, la loi du carré de la norme euclidienne de \\(X\\) ne dépend que des valeurs propres de la matrice de covariance.\n\nExercice 5 (Norme de vecteurs gaussiens non centrés)  \n\nMontrer que \\(X\\) est un vecteur gaussien standard et \\(\\mu\\) un vecteur, la loi du carré de la norme euclidienne de \\(X + \\mu\\) ne dépend que la norme de \\(\\mu\\).\nMontrer que\n\\[P \\left\\{ \\Vert X + \\mu \\Vert \\leq x \\right\\} \\leq P \\left\\{ \\Vert X \\Vert \\leq x \\right\\}\\]\nSuggestion : vérifier que c’est vrai en dimension 1, utiliser un argument de couplage.\n\n\nExercice 6 (Ratios de Mills)  \n\nSoit \\(Z \\sim \\mathcal{N}(0,1)\\). On note \\(\\Phi\\) la fonction de répartition de \\(\\mathcal{N}(0,1)\\), et \\(\\phi\\) sa densité.\nMontrer que pour tout \\(x&gt;0\\),\n\\[\\left(1 - \\frac{1}{x^2}\\right)  \\frac{1}{x} \\phi(x) \\leq  1 - \\Phi(x) \\leq \\frac{1}{x} \\phi(x)\\]\nSuggestion : utiliser l’intégration par parties.\n\nExercice 7 Dans cet exercice \\(T \\sim \\mathcal{N}(\\mu, \\tau^2)\\) et la distribution conditionnelle de \\(X\\) sachant \\(T\\) est \\(\\mathcal{N}(T, \\sigma^2)\\).\n\nCaractériser la loi jointe de \\((T,X)\\).\nQuelle est la loi de \\(X\\) ?\nQuelle est la distribution conditionnelle de \\(T\\) sachant \\(X\\) ?\n\n\n\nExercice 8 (Conditionnement gaussien)  \n\nSoit \\(\\rho\\in ]-1,1[\\) et \\((X,Y)\\) un vecteur gaussien centré de matrice de covariances \\[M = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}\\] On notera \\(\\sigma = \\sqrt{1-\\rho^2}\\).\n\nCalculer \\(\\text{det}(M)\\), \\(M^{-1}\\), puis exprimer la densité jointe \\(f_{(X,Y)}\\) du vecteur \\((X,Y)\\).\nMontrer que\n\\[g_x(y) := \\frac{f_{(X,Y)}(x,y)}{f_X(x)} = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp \\left( - \\frac{1}{2(1-\\rho^2)} \\left(y-\\rho x\\right)^2\\right).\\]\nMontrer que pour tout \\(x \\in \\mathbb{R}\\), \\(y \\to g_x(y)\\) définit une densité.\nSi on note \\(Y_x\\) une variable de densité \\(g_x\\), que pouvez-vous dire sur la loi de \\(Y_x\\)?\n\nTrouver \\(\\alpha\\), \\(\\beta\\) deux réels tels que \\((X, \\alpha X + \\beta Y)\\) suit la loi normale centrée réduite.\n\nRemarquer que l’on peut écrire \\(Y= -\\frac{\\alpha}{\\beta} X +\\frac{1}{\\beta}(\\alpha X + \\beta Y)\\). Sauriez-vous dire pourquoi cette écriture est intéressante?\n\nExercice 9 Dans cet exercice, \\(Y_1, \\ldots, Y_n, \\ldots\\) sont i.i.d. selon \\(\\mathcal{N}(0,1)\\), \\(X_0\\) est gaussienne \\(\\mathcal{N}(\\mu, \\tau^2)\\), indépendante de \\(Y_1, \\ldots, Y_n, \\ldots\\).\nOn définit \\(X_1, \\ldots, X_n, \\ldots\\) par\n\\[X_{i+1} = \\theta X_i  + \\sigma Y_{i+1}\\]\nOn note \\(\\mathcal{F}_i = \\sigma\\left(X_0, X_1, \\ldots, X_i \\right)\\).\n\nCalculer \\(\\mathbb{E}[ X_{i+1} \\mid \\mathcal{F}_i]\\).\nCalculer \\(\\mathbb{E}X_i\\), \\(\\text{Var}(X_i)\\).\nPeut-on choisir \\(\\mu, \\tau, \\sigma, \\theta\\), pour que les \\(X_i\\) soient tous distribués identiquement?\nÀ quelle condition sur \\(\\theta\\), les \\(X_i\\) convergent-elles en distribution ? Préciser la limite si possible.\nSi \\(\\theta\\) satisfait la condition de la question précédente, calculer \\(\\text{cov}\\left(X_i, X_{i+k}\\right)\\), pour \\(i, k \\in \\mathbb{N}\\)"
  },
  {
    "objectID": "weeks/week-6.html",
    "href": "weeks/week-6.html",
    "title": "Semaine 6",
    "section": "",
    "text": "Important\n\n\n\nLa semaine V (13- 17 octobre 2025) est consacrée aux vecteurs gaussiens :\n\nConvergence presque sûre, en probabilité, en loi\nLoi(s) des grands nombres\nThéorème central limite\n\nCC 2 : jeudi 17 octobre (durée 1h30 de 15h à 16h30)."
  },
  {
    "objectID": "weeks/week-6.html#préparervoirrevoir",
    "href": "weeks/week-6.html#préparervoirrevoir",
    "title": "Semaine 6",
    "section": "Préparer/Voir/Revoir",
    "text": "Préparer/Voir/Revoir\n\n\nLoi faible des grands nombres\nThéorème central limite\nCaractérisations\n\n\n\n\nVecteurs gaussiens\nConvergence en probabilité/presque sûre et loi des grands nombres\nConvergence en loi et TCL"
  },
  {
    "objectID": "weeks/week-6.html#exercices",
    "href": "weeks/week-6.html#exercices",
    "title": "Semaine 6",
    "section": "Exercices",
    "text": "Exercices\n\nFeuille TD VI\nFeuille TD V\nCorrection CC 1\n\n\nBack to course schedule ⏎"
  }
]