[
  {
    "objectID": "weeks/week-3.html",
    "href": "weeks/week-3.html",
    "title": "Week 3",
    "section": "",
    "text": "Important\n\n\n\nLa semaine III (22-26 septembre 2025) est consacrée aux calculs de moments, et au conditionnement.",
    "crumbs": [
      "Agenda",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#préparervoirrevoir",
    "href": "weeks/week-3.html#préparervoirrevoir",
    "title": "Week 3",
    "section": "Préparer/Voir/Revoir",
    "text": "Préparer/Voir/Revoir\n\n\nEspérances, moments, Espaces \\(L^p\\) (2025-09-22)\nConditionnement (2025-09-23 et 2025-09-25)\n\n\n\n\nVoir Notes, Espérances, Moments, Espaces \\(L^p\\), …\nVoir Notes, Conditionnement",
    "crumbs": [
      "Agenda",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#exercices",
    "href": "weeks/week-3.html#exercices",
    "title": "Week 3",
    "section": "Exercices",
    "text": "Exercices\n\nFeuille TD II\nFeuille TD II supplément\nQuestionnaire I\n\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Agenda",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-1.html",
    "href": "weeks/week-1.html",
    "title": "Semaine 1",
    "section": "",
    "text": "Important\n\n\n\nLa semaine I (8-12 septembre 2025) est consacrée aux révisions de Licence. On (re)-présentera beaucoup de définitions, quelques outils essentiels, et on tentera d’utiliser ces définitions et outils sur la liste d’exercices des feuilles de TD.",
    "crumbs": [
      "Agenda",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#préparerrevoir",
    "href": "weeks/week-1.html#préparerrevoir",
    "title": "Semaine 1",
    "section": "Préparer/Revoir",
    "text": "Préparer/Revoir\n\nEspace probabilisable\n\nTribu/\\(\\sigma\\)-algèbre\nTribu borélienne\nTribu de Lebesgue\nLemme des classes monotones\n\nLoi de probabilité, mesure positive, \\(\\sigma\\)-additivité\nFonctions mesurables et variables aléatoires\nEspérance, variance, moments\nThéorèmes de convergence (monotone, Fatou, dominée)\nIndépendance (Évenements, Tribus)",
    "crumbs": [
      "Agenda",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#exercices",
    "href": "weeks/week-1.html#exercices",
    "title": "Semaine 1",
    "section": "Exercices",
    "text": "Exercices\nFeuille TD I\nCorrigés :\n\n1 (transformation affine),\n2 (minima, maxima),\n10 (transformées de Laplace),\n11 (densité de la loi du carré),\n17 (somme de Poisson indépendantes, conditionnement discret),\n18 (sommes aléatoires de variables indépendantes).",
    "crumbs": [
      "Agenda",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#exercices-supplémentaires",
    "href": "weeks/week-1.html#exercices-supplémentaires",
    "title": "Semaine 1",
    "section": "Exercices supplémentaires",
    "text": "Exercices supplémentaires\nFeuille TD I suppléments\n\n\nRetour à l’agenda ⏎",
    "crumbs": [
      "Agenda",
      "Week 1"
    ]
  },
  {
    "objectID": "exercices/td2-supplement.html",
    "href": "exercices/td2-supplement.html",
    "title": "MA1AY010 Probabilités",
    "section": "",
    "text": "NoteTD II : Espérances et lois conditionnelles (supplément)\n\n\n\n22 Septembre 2025-26 Septembre 2025\n\nMaster I Isifar\nProbabilités\n\n\n\n\nExercice 1 (Questionnaire)\nSoient \\((\\Omega; \\mathcal{A}; P)\\) un espace de probabilité, \\(X\\) et \\(Y\\) des v.a.r., \\(T\\) une v.a. à valeurs dans \\(\\mathbb{R}^d\\).\nQue peut-on dire, sous réserve d’hypothèses d’intégrabilité adéquates, des espérances conditionnelles suivantes :\n\n\\(\\mathbb{E}(f(T)\\mid T)\\) avec \\(f : \\mathbb{R}^d \\to \\mathbb{R}\\) borélienne,\n\\(\\mathbb{E}(X\\mid T)\\) avec \\(X\\) \\(\\ \\sigma(T)\\)-mesurable,\n\\(\\mathbb{E}(XY \\mid T)\\) avec \\(X \\ \\sigma(T)\\)-mesurable,\n\\(\\mathbb{E}(f(X)\\mid T)\\) avec \\(f : \\mathbb{R}^d \\to \\mathbb{R}\\) borélienne, \\(X\\) et \\(T\\) indépendantes,\n\\(\\mathbb{E}(\\mathbb{E}(X \\mid T))\\),\n\\(\\mathbb{E}[S_{10} |S_{8}]\\) lorsque \\(S_n = \\sum_{i=1}^n X_i\\) et les \\((X_i)_{i \\ge 1}\\) sont i.i.d.,\n\\(\\mathbb{E}[S_{31}\\mid X_1]\\) lorsque \\(S_n = \\sum_{i=1}^n X_i\\) et les \\((X_i)_{i \\ge 1}\\) sont i.i.d.,\n\\(\\mathbb{E}[\\Pi_{4} \\mid \\Pi_2]\\) lorsque \\(\\Pi_n = \\prod_{i=1}^n X_i\\) et les \\((X_i)_{i \\ge 1}\\) sont i.i.d.,\n\\(\\mathbb{E}[\\phi(X,Y) \\mid Y]\\) lorsque \\(X\\) et \\(Y\\) sont indépendantes,\n\\(\\mathbb{E}[f(S_2+X_8) \\mid S_2]\\), lorsque \\(S_n = \\sum_{i=1}^n X_i\\) et les \\((X_i)_{i \\ge 1}\\) sont i.i.d.\n\n\n\nExercice 2\nOn considère un processus de Galton-Watson de loi de branchement \\[ \\mathbb{P}(\\xi=0)= \\mathbb{P}(\\xi=2)=1/2.\\] issu à la génération \\(0\\) d’un unique individu ancestral. On note \\(Z_n\\) la taille de la population à la génération \\(n\\).\nExprimer \\(\\mathbb{E}[(Z_{2}-1)^2 \\mid Z_1].\\)"
  },
  {
    "objectID": "exercices/td1-supplement.html",
    "href": "exercices/td1-supplement.html",
    "title": "MA1AY010 Probabilités",
    "section": "",
    "text": "NoteTD I : Révisions de Licence (exercices supplémentaires)\n\n\n\n\n8 Septembre 2025-12 Septembre 2025\nMaster I Isifar\nProbabilités"
  },
  {
    "objectID": "exercices/td1-supplement.html#pour-aller-plus-loin",
    "href": "exercices/td1-supplement.html#pour-aller-plus-loin",
    "title": "MA1AY010 Probabilités",
    "section": "Pour aller plus loin",
    "text": "Pour aller plus loin\n\nExercice 1 (un (contre)-exemple à garder en tête)\nOn note \\(C\\) l’ensemble de Cantor, qui est obtenu, récursivement, en enlevant \\(I_1^{(1)} : = (1/3,2/3)\\) à l’intervalle \\([0,1]\\), puis en enlevant les tiers intermédiaires \\(I_2^{(1)} = (1/9,2/9), I_2^{(2)}= (7/9,8/9)\\) des deux intervalles restants, et ainsi de suite, de sorte qu’à l’etape \\(n\\) on enlève \\(2^{n-1}\\) intervalles tous de taille \\(1/3^n\\), notés \\(I_n^{(j)},j=1...2^{n-1}\\).\nOn définit la fonction croissante :\n\\[F(x) := \\begin{cases}  0 & \\text{ si } x \\le 0 \\\\    1 & \\text{ si } x \\ge 1 \\\\ \\frac{2j-1}{2^n} \\text{ si } x \\in I_n^{(j)}. \\end{cases}\\]\n\nLa fonction \\(F\\) est-elle continue?\nDans quel ensemble la variable \\(X\\), de fonction de répartition \\(F\\) —définie ci-dessus— prend elle ses valeurs? Que vaut \\(\\mathbb{E}[X]\\)? Que vaut \\(\\mathbb{P}(|X-\\mathbb{E}[X]|&lt;1/8)\\)? Pour \\(x \\in C\\), que vaut \\(\\mathbb{P}(X=x)\\)?\nJustifier que la variable \\(X\\) est uniforme sur \\(C\\). Admet-elle une densité sur \\([0,1]\\)?\n\nQuelle est la loi de \\(Y=F(X)\\)?\nPourrait-on ainsi définir une variable aléatoire uniforme sur \\(\\mathbb{Q}\\cap [0,1]\\)?\nSoient \\(\\{X_j, j \\in \\mathbb{N}^* \\}\\) une famille de variables i.i.d., distribuées suivant la loi de Bernoulli de parmaètre \\(1/2\\). On définit \\(X=2 \\sum_{j \\ge 1} \\frac{X_j}{3^j}\\). Quelle est la loi de \\(X\\)?\nCalculer \\(\\Phi_X\\) et montrer qu’elle est constante sur \\(\\{3^k \\pi, k \\in \\mathbb{N}^* \\}\\).\n\n\n\nExercice 2\nDans ce qui suit \\(||.||\\) désigne la norme euclidienne.\nPour \\(\\varepsilon \\in (0,1)\\), \\(n \\in \\mathbb{N}\\)\non note \\(A_{n, \\varepsilon} := \\{ x \\in \\mathbb{R}^n : (1-\\varepsilon)(n/3)^{1/2} \\le ||x|| \\le (1+\\varepsilon)(n/3)^{1/2}\\}\\), et \\(C_n=[-1,1]^n\\).\nMontrer que pour tout \\(\\varepsilon \\in (0,1)\\), la mesure de Lebesgue de \\(C_n \\cap A_{n, \\varepsilon}^c\\) est négligeable, pour \\(n \\to \\infty\\), devant celle de \\(C_n\\).\nIndication : On notera qu’un point générique dans \\(C_n\\) a pour coordonnées \\((X_1,...,X_n)\\), avec \\(\\{X_i, i \\ge 1\\}\\) une famille de variables aléatoires i.i.d, uniformes sur \\([-1,1]\\).\nNote : Ceci signifie que pour \\(\\varepsilon\\) aussi petit qu’on souhaite le prendre, lorsque \\(n\\) est grand, la majeure partie du volume de l’hypercube \\(C_n\\) se trouve dans la couronne \\(A_{n,\\varepsilon}\\) (!)\n\n\nExercice 3\nSoit \\(X \\sim \\Gamma(1,s)\\). On définit \\(Y \\sim \\mathrm{Poisson}(X)\\), (c’est-à-dire que conditionnellement à \\(X=x\\), \\(Y \\sim \\mathrm{Poisson}(x)\\)).\n\nCalculer \\(\\Phi_Y\\).\nMontrer que lorsque \\(s \\to \\infty\\), \\[\\frac{Y-E[Y]}{\\sqrt{\\mathrm{var}(Y)}} \\overset{(\\mathrm{loi})}{\\longrightarrow} Z,\\] où \\(Z \\sim \\mathcal{N}(0,1)\\). Y a-t-il un lien avec le théorème central limite?\n\n\n\nExercice 4\n(Une remarque importante sur la convergence en loi.)\nSoit, pour \\(n \\ge 1\\), \\(X_n\\) un variable aléatoire dont la fonction de répartition est définie par \\[ F_n(x) = x - \\frac{\\sin(2n \\pi x)}{2n\\pi}, \\ 0 \\le x \\le 1.\\]\n\nMontrer que \\(X_n\\) converge en loi vers une variable \\(X\\) (dont on décrira la loi).\n\nVérifier que \\(X_n\\) et \\(X\\) sont des variables à densité. On note \\(f_n := F_n'\\) la densité de \\(X_n\\), \\(f\\) celle de \\(X\\). A-t-on \\(f_n \\underset{n \\to \\infty}{\\rightarrow} f\\) simplement?\n\n\n\nExercice 5\n(Le problème des anniversaires)\nOn cherche a approximer la probabilité que parmi \\(N\\) personnes, au moins deux (resp. trois) aient leur anniversaire le même jour.\n\nPour \\(1 \\le i &lt;j \\le N\\), on note \\(E_{ij}\\) l’événement que les personnes \\(i,j\\) aient le même anniversaire. Quelle est la probabilité de \\(E_{ij}\\)? Dans la suite de l’exercice on omet de considérer le cas des \\(29\\) février. Que vaut alors \\(P(E_{ij})\\)? Montrer que les événements \\(E_{12}, E_{23}, E_{13}\\) sont indépendants deux à deux, mais pas mutuellement indépendants. Exprimer la probabilité qu’il y ait au moins deux personnes qui aient leur anniversaire le même jour.\nLe but de cette question n’est pas d’obtenir un résultat exact, mais de faire une approximation assez bonne de la probabilité que parmi \\(N\\) personnes, il y en ait au moins deux qui partagent le même anniversaire. On admettra donc que dans la suite on fait l’approximation que les \\(\\{E_{ij}, 1 \\le i &lt; j \\le N\\}\\) sont mutuellement indépendants. Quelle est alors la loi du nombre de couples ayant le même anniversaire? Approximer le résultat pour \\(N=30, N=60\\).\n\nPar une méthode similaire, approximer la probabilité que parmi \\(60\\) personnes, il y en ait au moins trois qui ont leur anniversaire le même jour. Finalement, approximer la probabilité de cet événement lorsque \\(N=90\\).\n\n\n\nExercice 6\n(Quelques variantes du modèle de branchement aléatoire)\nOn considère le modèle de branchement aléatoire qui est un modèle pour l’évolution d’une population haploïde.\nOn part initialement de \\(Z_0\\) ancêtres, et on note \\(Z_n\\) le nombre total d’individus de la génération \\(n\\).\nOn fait l’hypothèse (pas très réaliste) que tous les individus ont exactement le même temps de vie (disons \\(1\\)), et que de plus deux générations distinctes ne cohabitent pas. Ainsi chacun des ancêtre meurt au temps \\(1\\), et donne naissance à un nombre aléatoire d’individus, de façon indépendante de ses contemporains. Tous les individus qui sont nés au temps \\(1\\) forment la génération \\(1\\). La génération \\(1\\) meurt au temps \\(2\\) en donnant naissance à la génération \\(2\\), etc…\nFormellement, on se donne une loi de branchement \\(\\nu\\) sur les entiers positifs, telle que \\(\\nu(1)&lt; 1\\) et \\(\\sum_{k \\ge 0} k \\nu(k) = \\mu \\in [0,\\infty)\\) On introduit alors une famille \\((X,X_{i,k}, i \\ge 0, k\\ge 1)\\) de variables aléatoires i.i.d, toutes de loi \\(\\mu\\). Si jamais la génération \\(i\\) possède un \\(k\\)-ième individu, \\(X_{i,k}\\) désigne le nombre de ses descendants.\n\nPour \\(n \\ge 1\\), exprimer \\(Z_n\\) en fonction de \\(Z_{n-1}\\) et des \\((X_{n-1,k}, k \\ge 1)\\).\nOn note \\(G_{X}(t) = E[t^{X}]\\), \\(G_{Z_n}(t) = E[t^{Z_n}]\\) (qui ne sont autres, modulo un simple changement de variable, que les fonctions génératrice des moments de \\(X\\), \\(Z_n\\)). Exprimer \\(G_{Z_n}\\) en fonction de \\(G_X, G_{Z_0}\\).\nOn note \\(\\zeta\\) le probabilité d’extinction. A quelle condition sur \\(\\mu\\) a-t-on \\(\\zeta=1\\) p.s.?\nIndication : On pourra considérer la suite \\(\\zeta_n = \\mathbb{P}(Z_n=0)\\)\nUne première martingale, Calculer \\(\\mathbb{E}[Z_{n-1}|Z_n]\\). On pose \\(M_{n}=Z_n/\\mu^n\\). Calculer \\(\\mathbb{E}[M_{n+1}|M_n]\\).\n\nPour plus de détails, on pourra voir l’introduction du livre de D.WILLIAMS, Probability with martingales, Cambridge University Press, 1991.\n\nOn considère (dans cette question uniquement) un processus de branchement avec immigration. On garde la même dynamique : à sa mort, un individu donne naissance à un nombre al'atoire de descendants suivant la loi \\(\\mu\\). De plus, chaque génération voit maintenant arriver un nombre aléatoire d’immigrants, indistinguables des individus déjà présents.\nFormellement, on se donne une loi \\(\\nu\\) sur \\(\\mathbb{N}\\) et on introduit \\(Y,Y_1,...Y_n\\) des variables \\(i.i.d.\\) de loi \\(\\nu\\). La génération \\(n\\) reçoit \\(Y_n\\) individus au temps \\(n\\). On note \\(G_Y(t)= E[t^{Y}]\\). Trouver une relation entre \\(G_{Z_n}, G_Y\\) et \\(G_{Z_0}\\).\nEnfin, on considère un processus de branchement dont les individus n’ont pas tous un temps de vie égal à \\(1\\). Pour simplifier un peu, on va seulement considérer la descendance d’un unique ancêtre, i.e. on pose \\(Z_0=1\\). \\ Formellement, on se donne une loi \\(P_{T}\\) sur \\(\\mathbb{R}_+\\), dont la densité est notée \\(f_T\\). On définit famille de variables aléatoires \\((T_{i}, i \\in \\mathbb{N})\\) i.i.d, de loi \\(P_T\\). On peut numéroter les individus en les classant suivant leur date de naissance. On définit alors \\(T_i\\) comme le temps de vie du \\(i\\)-ième individu.\nPour \\(t \\ge 0\\), on note \\(Z_t\\) le nombre total d’individus qui sont en vie au temps \\(t\\). On note enfin \\(G_t(s) = E[s^{Z_t}]\\).\n\n\nMontrer que \\(G_t(s) = \\int_0^{\\infty} E[s^{Z_t} |T_0 =u] f_T(u) du\\)\n\nEn déduire \\[ G_t(s) = \\int_0^t G_X(G_{t-u}(s))f_T(u) du + \\int_t^{\\infty} sf_T(u) du.\\]\n\nDans le cas où \\(f_T(s)= \\lambda \\exp(-\\lambda s)\\), montrer alors que \\[\\frac{\\partial t}{\\partial t} G_t(s) = \\lambda \\left[ G_X(G_t(s)) - G_t(s)\\right]  \\]"
  },
  {
    "objectID": "exercices-listings.html",
    "href": "exercices-listings.html",
    "title": "TDs",
    "section": "",
    "text": "Note\n\n\n\nLes séances d’exercice sont organisées autour des feuilles de TD. Sentez vous libres d’attaquer les exercices à l’avance.\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Titre\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Tags\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitre\n\n\n\nTags\n\n\n\n\n\n\n\n\nSep 8, 2025\n\n\nTD I : Rappels de Licence\n\n\nVariables aléatoires réelles\n\n\n\n\n\n\nSep 11, 2025\n\n\nTD I supplément : Rappels de Licence\n\n\nVariables aléatoires réelles, Branchement\n\n\n\n\n\n\nSep 22, 2025\n\n\nTD II : Conditionnement\n\n\nConditionnement, Branchement\n\n\n\n\n\n\nSep 22, 2025\n\n\nTD II : Conditionnement (supplément)\n\n\nConditionnement, Branchement\n\n\n\n\n\n\nSep 22, 2025\n\n\nQuestionnaire I\n\n\n \n\n\n\n\n\n\nSep 22, 2025\n\n\nTD III : Branchement\n\n\nConditionnement, Branchement\n\n\n\n\n\n\nSep 29, 2025\n\n\nTD IV : Conditionnement\n\n\nConditionnement\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nTipSuggestion",
    "crumbs": [
      "TDs"
    ]
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Cliquer ici pour télécharger une copie PDF du syllabus.",
    "crumbs": [
      "Informations",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#informations",
    "href": "course-syllabus.html#informations",
    "title": "Syllabus",
    "section": "Informations",
    "text": "Informations\n\n\n\n\n\n\n\n\n\n\nJour\nHoraire\nSalle\n\n\n\n\nCours/Exercices\nLundi\n13:30 - 16:30\nSophie Germain 1015\n\n\nExercices\nMardi\n8:30 - 10:30\nHalle aux Farines 227C\n\n\nCours/Exercices\nJeudi\n13:30 - 16:30\nSophie Germain 1015",
    "crumbs": [
      "Informations",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#objectifs",
    "href": "course-syllabus.html#objectifs",
    "title": "Syllabus",
    "section": "Objectifs",
    "text": "Objectifs\nA la fin du premier semestre, vous saurez…\n\nDéfinir, caractériser une loi de probabilité sur un espace simple.\nÉtablir et utiliser les différents types de convergence de variables aléatoires.\nManipuler les espérances conditionnelles.\nManipuler les vecteurs gaussiens (en dimension finie).",
    "crumbs": [
      "Informations",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#bibliographie",
    "href": "course-syllabus.html#bibliographie",
    "title": "Syllabus",
    "section": "Bibliographie",
    "text": "Bibliographie\nBerger, Q., Caravenna, F., & Dai Pra, P. (2021). Probabilità (Vol. 9). Springer.\n\nEn italien Via ENT Paris Cité\nEn français Via ENT Paris Cité",
    "crumbs": [
      "Informations",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#cours-et-exercices",
    "href": "course-syllabus.html#cours-et-exercices",
    "title": "Syllabus",
    "section": "Cours et exercices",
    "text": "Cours et exercices\n\nNotes de cours (~ Poly)\nExercices",
    "crumbs": [
      "Informations",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#validation",
    "href": "course-syllabus.html#validation",
    "title": "Syllabus",
    "section": "Validation",
    "text": "Validation\n\nContrôle continu\n4 épreuves (Examen terminal inclus)\n\n\nExamens\nL’examen terminal de session I dure trois heures, il porte sur l’ensemble du cours",
    "crumbs": [
      "Informations",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#intégrité",
    "href": "course-syllabus.html#intégrité",
    "title": "Syllabus",
    "section": "Intégrité",
    "text": "Intégrité\nTL;DR: Ne trichez pas!",
    "crumbs": [
      "Informations",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#sauvez-les-dates",
    "href": "course-syllabus.html#sauvez-les-dates",
    "title": "Syllabus",
    "section": "Sauvez les dates!",
    "text": "Sauvez les dates!\n\nContrôle Continu 1 Mardi 7 octobre 2025 (Programme : semaines I à III)\nContrôle Continu 2 Jeudi 16 octobre 2025 (Programme : semaines I à V)\nContrôle Continu 3 Novembre\nExamen entre le Lundi 8 et le Vendredi 19 décembre\nExamen session II en juin 2026\n\nCliquer ici for l’emploi du temps.",
    "crumbs": [
      "Informations",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-links.html",
    "href": "course-links.html",
    "title": "Quelques liens",
    "section": "",
    "text": "Entrepot GitHub du cours\n🔗 sur GitHub\n\n\nEntrepot GitHub des Notes de cours\n🔗 sur GitHub\n\n\nPortail Moodle du cours\n🔗 sur Moodle",
    "crumbs": [
      "Informations",
      "Liens"
    ]
  },
  {
    "objectID": "course-faq.html",
    "href": "course-faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Pas nécessairement.",
    "crumbs": [
      "Informations",
      "FAQ"
    ]
  },
  {
    "objectID": "course-faq.html#les-corrigés-des-exercices-sont-ils-disponibles-en-ligne",
    "href": "course-faq.html#les-corrigés-des-exercices-sont-ils-disponibles-en-ligne",
    "title": "FAQ",
    "section": "",
    "text": "Pas nécessairement.",
    "crumbs": [
      "Informations",
      "FAQ"
    ]
  },
  {
    "objectID": "course-faq.html#comment-travailler-avec-les-cahiers-de-simulation",
    "href": "course-faq.html#comment-travailler-avec-les-cahiers-de-simulation",
    "title": "FAQ",
    "section": "Comment travailler avec les cahiers de simulation ?",
    "text": "Comment travailler avec les cahiers de simulation ?\n\nEn R\n\nTélécharger et installer R 4.x.y: https://cran.r-project.org/\nTélécharger et installer RStudio: https://dailies.rstudio.com/\nInstaller Quarto CLI: https://quarto.org/docs/getting-started/installation.html\nInstaller Git: https://happygitwithr.com/install-git.html\nInstaller tout package utile avec install.packages(\"___\") (voir aussi le package pak)\n\n\n\nEn Python",
    "crumbs": [
      "Informations",
      "FAQ"
    ]
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "MA1AY010 : Enseignants",
    "section": "",
    "text": "S. Boucheron est professeur dans l’Équipe de Statistique du Laboratoire de Probabilités, Statistiques et Modélisation (Université Paris Cité, Sorbonne Université, CNRS).\n\n\n\n\n\n\n\nConsultations\nBureau\n\n\n\n\nLundi 11:00 am - 12:00 pm\nSophie Germain 5013\n\n\n\nS. Abbes est professeur à l’IRIF (Université Paris Cité, CNRS).",
    "crumbs": [
      "Informations",
      "Enseignants"
    ]
  },
  {
    "objectID": "course-team.html#instructeurs",
    "href": "course-team.html#instructeurs",
    "title": "MA1AY010 : Enseignants",
    "section": "",
    "text": "S. Boucheron est professeur dans l’Équipe de Statistique du Laboratoire de Probabilités, Statistiques et Modélisation (Université Paris Cité, Sorbonne Université, CNRS).\n\n\n\n\n\n\n\nConsultations\nBureau\n\n\n\n\nLundi 11:00 am - 12:00 pm\nSophie Germain 5013\n\n\n\nS. Abbes est professeur à l’IRIF (Université Paris Cité, CNRS).",
    "crumbs": [
      "Informations",
      "Enseignants"
    ]
  },
  {
    "objectID": "exercices/quizz_a.html",
    "href": "exercices/quizz_a.html",
    "title": "MA1AY010 Probabilités",
    "section": "",
    "text": "NoteQuestionnaire I\n\n\n\n\n8 Septembre 2025-12 Septembre 2025\nMaster I Isifar\nProbabilités\n\n\n\n\nQuestion 1\n\nEst-il vrai qu’une fonction croissante sur \\(\\mathbb{R}\\) est toujours mesurable ?\nSi \\(X,Y\\) sont indépendantes, de lois à densité sur \\(\\mathbb{R}\\), est-il vrai que \\(\\mathbb{P} \\{ X= Y \\}=0\\)?\n\n\n\nQuestion 2\nSoit \\(X\\) une variable aléatoire réelle, dont la loi admet une densité \\(f\\) par rapport à la mesure de Lebesgue. Soit \\(a\\) un réel positif et \\(b\\) un réel quelconque.\n\nLa loi de la variable aléatoire \\(aX +b\\) admet-elle une densité ?\nSi oui, exprimer cette densité en fonction de \\(f, a, b\\).\n\n\n\nQuestion 3\nSur un espace de probabilité \\((\\Omega, \\mathcal{F}, P)\\), on dispose d’une famille infinie de variables aléatoires indépendantes \\(X_1, X_2, \\ldots, X_n, \\dots\\) toutes distribuées selon la loi exponentielle standard (densité \\(\\mathrm{e}^{-x}\\) sur \\([0, \\infty)\\)).\nOn définit \\(Y_n = \\sum_{i=1}^n \\left( X_i/i - \\frac{1}{i}\\right)\\) pour tout \\(n\\).\n\nCalculer l’espérance et la variance de \\(Y_n\\).\nLa suite \\((Y_n)_n\\) converge-t-elle dans \\(\\mathcal{L}_2\\) ?\nSi oui, pouvez-vous caractériser la loi de la limite?\n\n\n\nQuestion 4 (Inégalité d’association)\nSoit \\(X\\) une variable aléatoire réelle sur un espace de probabilité \\((\\Omega, \\mathcal{F}, P)\\) et \\(f,g\\) deux fonctions croissantes positives sur \\(\\mathbb{R}\\).\nMontrer que \\[\n\\mathbb{E} \\left[ f(X) g(X)\\right] \\geq \\mathbb{E}\\left[ f(X)\\right]  \\times \\mathbb{E} \\left[ g(X) \\right] \\, .\n\\]\n\n\nQuestion 5\nSoit \\(X_1, \\ldots, X_n\\) une suite de variables aléatoires i.i.d. selon une loi exponentielle. On note \\((X_{i,n})_{i\\leq n}\\) le réarrangement croissant de \\(X_1, \\ldots, X_n\\), (les statisques d’ordre)\n\nQuelle est la loi de \\(X_{1,n}\\), le minimum ?\nQuelle est la loi de \\(X_{2,n} - X_{1, n}\\) ?\nQuelle est la densité jointe de la loi de \\((X_{i,n})_{i\\leq n}\\) ?\nQuelle est la densité jointe de la loi de \\((X_{i,n} - X_{i-1,n})_{i\\leq n}\\) avec la convention \\(X_{0,n}=0\\)?\n\n\n\nQuestion 6\nSoit \\(X\\) une variable aléatoire distribuée selon une loi binomiale de paramètres \\(n\\) et \\(p \\in (0,1)\\). Montrer que \\[\n\\frac{1}{\\mathbb{E}X+1} &lt; \\mathbb{E}\\frac{1}{X+1} \\leq \\frac{1}{\\mathbb{E}X + p} \\, .\n\\] Montrer que si \\(X\\) est distribuée selon une loi de Poisson \\[\n\\mathbb{E}\\frac{1}{X+1} = \\frac{1 - \\mathrm{e}^{-\\mathbb{E}X}}{\\mathbb{E}X} \\, .\n\\] On note \\(E\\) l’événement \\(\\{ X &gt;0 \\}\\) montrer que \\[\n\\mathbb{E}_{P (\\cdot|E)}\\left[ \\frac{1}{X}\\right] \\geq \\mathbb{E}\\frac{1}{X+1} \\, .\n\\]\n\n\nQuestion 7\nLe jeu oppose un présentateur à un candidat (le joueur). Ce joueur est placé devant trois portes fermées. Derrière l’une d’elles se trouve une voiture et derrière chacune des deux autres se trouve une chèvre. Il doit tout d’abord désigner une porte. Puis le présentateur doit ouvrir une porte qui n’est ni celle choisie par le candidat, ni celle cachant la voiture (le présentateur sait quelle est la bonne porte dès le début). Le candidat a alors le droit d’ouvrir la porte qu’il a choisie initialement, ou d’ouvrir la troisième porte.\nOn suppose que l’organisateur du jeu place la voiture uniformément au hasard derrière l’une des trois portes avant la partie.\nLes questions qui se posent au candidat sont :\n\nQue doit-il faire ?\nQuelles sont ses chances de gagner la voiture en agissant au mieux ?"
  },
  {
    "objectID": "exercices/td1.html",
    "href": "exercices/td1.html",
    "title": "Variables aléatoires réelles",
    "section": "",
    "text": "NoteTD I : Révisions de Licence\n\n\n\n\n8 Septembre 2025-12 Septembre 2025\nMaster I Isifar\nProbabilités"
  },
  {
    "objectID": "exercices/td1.html#fonctions-de-répartition",
    "href": "exercices/td1.html#fonctions-de-répartition",
    "title": "Variables aléatoires réelles",
    "section": "Fonctions de répartition",
    "text": "Fonctions de répartition\n\nExercice 1 (transformation affine)\nSoit \\(X\\) une variable aléatoire réelle, \\(F_X\\) sa fonction de répartition, \\(a, b\\) deux réels fixés, et \\(Y := aX+b\\)\n\nOn suppose dans cette question que \\(a=1\\). Comment déduire \\(F_Y\\) de \\(F_X\\)?\nSi (la loi de) \\(X\\) admet une densité, en est-il de même de \\(Y\\)? Si oui, exprimer dans ce cas \\(f_Y\\) à l’aide de \\(f_X\\).\nOn suppose dans cette question que \\(b=0\\) et \\(a&gt;0\\). Comment déduire \\(F_Y\\) de \\(F_X\\)?\nSi (la loi de ) \\(X\\) admet une densité, à quelle condition sur \\(a\\) en est-il de même de (la loi de) \\(Y\\)? Exprimer dans ce cas \\(f_Y\\) à l’aide de \\(f_X\\).\nRépondre aux mêmes questions lorsque \\(b=0\\) et \\(a=-1\\)?\nRépondre enfin aux mêmes questions lorsque \\(a\\) et \\(b\\) sont quelconques.\n\n\n\nExercice 2 (Minimum, maximum de variables indépendantes)\nSoient \\(X_i, i \\ge 1\\), des variables indépendantes. Pour \\(k \\ge 2\\), on note \\(Y_k = \\min (X_1,...,X_k)\\), \\(Z_k = \\max(X_1,...,X_k).\\)\n\nDans cette question on s’intéresse à \\(k=2\\).\nComment déduire \\(F_{Y_2}\\) de \\(F_{X_1}, F_{X_2}\\)? Même question pour \\(F_{Z_2}\\).\nGénéraliser à \\(k\\) quelconque.\nQuelle est la loi de \\(Z_k\\) lorsque les \\(\\{X_i, i \\ge 1\\}\\) sont i.i.d., \\(\\sim \\mathrm{Unif}[0,1]\\)?\nQuelle est la loi de \\(Y_k\\) lorsque les \\(\\{X_i, i \\ge 1\\}\\) sont des variables exponentielles indépendantes, avec \\(X_i \\sim \\text{Exp}(\\lambda_i)\\) (où pour tout \\(i\\), \\(\\lambda_i &gt;0\\))?\n\n\n\nExercice 3\nOn suppose que \\(X \\sim \\mathcal{N}(0,1)\\). Que valent\n\\[\\mathbb{P}(X \\le 1), \\quad \\mathbb{P}(-1.23 \\le X \\le 0.43), \\quad \\mathbb{P}(X&gt;0.32)?\\]\n\n\nExercice 4\nOn suppose que l’écart à la taille moyenne \\(T=15.5\\) des individus d’une population suit une loi normale centrée réduite.\nDans quel intervalle centré en \\(T\\) se situent les tailles de \\(99\\%\\) des individus de la population?\n\n\nExercice 5\nEtant donnée X une variable aléatoire gaussienne de paramètres \\(\\mu\\) et \\(\\sigma^2\\), donner la probabilité que \\(|X - \\mu|\\) dépasse \\(k\\sigma\\) pour \\(k = 1, 2, 3\\).\nSuggestion: On commencera par montrer que \\(\\sigma^{-1}(X-\\mu)\\) suit une loi normale centrée réduite.\nReprendre les questions de l’exercice précédent lorsque \\(\\mu=2, \\sigma=2\\).\nReprendre les questions de l’exercice précédent lorsque \\(\\mu=0, \\sigma=1/2\\)."
  },
  {
    "objectID": "exercices/td1.html#densités",
    "href": "exercices/td1.html#densités",
    "title": "Variables aléatoires réelles",
    "section": "Densités",
    "text": "Densités\n\nExercice 6\nDans les cas suivants, trouver la valeur de \\(C\\) pour que \\(f\\) soit une densité de probabilité.\n\n\\(f(x) = C \\frac{1}{\\sqrt{x (1-x)}}, 0 &lt;x &lt;1,\\)\n\\(f(x) = C \\exp(-x-\\exp(-x)), x \\in \\mathbb{R},\\)\n\\(f(x) = C \\frac{1}{1+x^2}\\).\n\n\n\nExercice 7\n(Mélange)\nOn suppose que \\(X\\) et \\(Y\\) sont deux variables de densités respectives \\(f_X, f_Y\\), et que \\(\\alpha \\in [0,1]\\). Montrer que \\(g : = \\alpha f_X + (1-\\alpha) f_Y\\) est également une densité de probabilité.\nTrouver une variable aléatoire dont \\(g\\) est la densité.\n\n\nExercice 8\nSoit \\(X\\) de densité \\(f\\), et \\((\\alpha, \\beta) \\in \\overline{\\mathbb{R}}^2\\) sont supposés tels que \\[\\mathbb{P}(\\alpha &lt; X &lt; \\beta) =1\\] On suppose que \\(g\\) est un \\(C^{1}\\)-difféomorphisme croissant de \\((\\alpha, \\beta)\\) sur \\((g(\\alpha),g(\\beta))\\).\n\nMontrer que \\(g(X)\\) a pour densité \\(\\frac{f(g^{-1}(x))}{g'(g^{-1}(x))} \\mathbf{1}_{\\{x \\in (g(\\alpha), g(\\beta))\\}}\\).\nQuelle est la densité de la variable \\(aX+b\\), où \\(a&gt;0\\) et \\(b\\in \\mathbb{R}\\) sont fixés?\n\nSoit \\(Y \\sim \\mathcal{N}(0,1)\\). Quelle est la densité de \\(Z = \\exp(Y)\\)?"
  },
  {
    "objectID": "exercices/td1.html#lois-usuelles-calculs-de-loi",
    "href": "exercices/td1.html#lois-usuelles-calculs-de-loi",
    "title": "Variables aléatoires réelles",
    "section": "Lois usuelles, calculs de loi",
    "text": "Lois usuelles, calculs de loi\n\nExercice 9\n(Fonctions de répartition et fonctions caractéristiques de lois usuelles)\n\n\n\n\n\n\nAttention : dans le cas d’une variable continue, quand on calcule \\(\\Phi_X\\) on %doit intégrer sur \\(\\mathbb{R}\\) une fonction complexe. Trois méthodes sont envisageables.\nParfois on peut intégrer séparément partie réelle et partie imaginaire.\nParfois il est utile de se servir de la formule de Cauchy. En particulier, cette formule assure que si \\(f\\) est une fonction holomorphe, si \\(\\mathcal{C}\\) est un contour fermé “raisonnable” (en particulier tout cercle ou polygone régulier), et enfin si \\(\\overset{\\circ}{\\mathcal{C}}\\) désigne l’ensemble des points se trouvant à l’intérieur de ce contour, alors\n\\[\\forall a \\in \\overset{\\circ}{\\mathcal{C}}, \\quad f(a) = \\frac{1}{2\\pi i} \\oint_{\\mathcal{C}}  \\frac{f(z)}{z-a} \\mathrm{d}z.\\]\nAttention : cette formule montre bien que l’on ne peut pas traiter l’intégrale d’une fonction complexe en faisant “comme si” \\(i\\) était réél (!) La méthode des résidus est en outre une conséquence directe de la formule de Cauchy.\nEnfin, on peut utiliser le prolongement analytique (voir l’exemple de la fonction \\(\\Gamma\\)).\n\n\n\n\nExprimer le plus simplement \\(F_X\\) dans les cas suivants (on pourra se contenter de tracer l’allure du graphe de la fonction de répartition lorsque celle-ci ne possède pas d’expression simple).\n\n\\(n \\in \\mathbb{N}^*, p \\in [0,1], X \\sim \\text{Bin}(n,p)\\),\n\\(\\lambda&gt;0, X \\sim \\text{Poisson}(\\lambda)\\),\n\\(a&gt;0, X\\sim \\text{Unif}[-a,a]\\),\n\\(\\lambda&gt;0, X \\sim \\mathbf{exp}(\\lambda)\\),\n\\(\\lambda&gt;0, s \\in \\mathbb{N}^*\\), \\(X \\sim \\Gamma(\\lambda, s)\\) (on rappelle que la densité \\(f_X\\) de \\(X \\sim \\Gamma(\\lambda,s)\\) est telle que \\(f_X(x) =\\frac{1}{\\Gamma(s)} \\lambda^s x^{s-1}\\exp(-\\lambda x) \\mathbf{1}_{[0,\\infty[}(x), x \\in \\mathbb{R}\\)),\n\\(X \\sim \\mathcal{N}(0,1)\\),\n\\(\\mu \\in \\mathbb{R}, \\sigma&gt;0, X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\).\n\n\\(a&gt;0, X \\sim \\mathrm{Cauchy}(a)\\) (on rappelle que la densité \\(f_X\\) de la loi de Cauchy de paramètre \\(a\\) est telle que \\(f_X(x) = \\frac{a}{\\pi(x^2+a^2)}, x\\in \\mathbb{R}\\)).\n\n(*) \\(X \\sim \\mathrm{stable}(1/2)\\) (cette loi a pour densité \\(\\sqrt{2\\pi x^{-3}} \\exp(-1/2x)\\mathbf{1}_{[0,\\infty[}(x).\\))\n\nLesquelles parmi ces variables possèdent une densité?\n\nExprimer le plus simplement \\(\\Phi_X\\) pour les \\(7\\) premières variables de la première question ci-dessus. En déduire, ou trouver par un calcul direct, \\(E[X],\\) et \\(\\mathrm{Var}[X]\\).\n\n\n\nExercice 10\nPour des valeurs de \\(t\\) que l’on précisera, calculer la transformée de Laplace \\(L(t) := \\mathbb{E}[\\exp(-t X)]\\) et la fonction génératrice des moments \\(G(u) := \\mathbb{E}[u^X]\\) de la variable \\(X\\) dans les cas suivants.\n\n\\(X \\sim \\mathrm{Ber}(p)\\), où \\(p \\in [0,1]\\),\n\\(X \\sim \\mathrm{Bin}(n,p)\\), où \\(n \\in \\mathbb{N}^*, p \\in [0,1]\\),\n\\(X \\sim \\mathrm{Geom}(p)\\), où \\(p \\in [0,1]\\),\n\\(X \\sim \\mathrm{Poisson}(\\lambda)\\), où \\(\\lambda&gt;0\\),\n\n\\(X=Y_1+...+Y_n\\), où les \\(Y_i, 1 \\le i \\le n\\) sont des variables indépendantes, et \\(Y_i \\sim \\mathrm{Poisson}(\\lambda_i)\\), avec \\(\\lambda_i &gt;0\\).\n\n\n\nExercice 11\nSoit \\(X\\) une v.a.r. de densité \\(f\\). Quelle est la densité de \\(X^2\\)? Qu’obtient-on dans le cas où \\(X \\sim \\mathcal{N}(0,1)\\)?\n\n\nExercice 12\nSoit \\(X \\sim \\exp(1)\\). Calculer la densité des variables suivantes :\n\n\\(Y = aX+b\\), où \\(a&gt;0\\) et \\(b \\in \\mathbb{R}\\). Qu’observe-t-on dans le cas où \\(b=0\\)?\n\n\\(Z = X^2\\).\n\\(U = \\exp(-X)\\).\n\n\n\nExercice 13\nTrouver la loi de \\(\\arcsin(X)\\) lorsque\n\n\\(X \\sim \\mathrm{Unif}[0,1]\\),\n\\(X \\sim \\mathrm{Unif}[-1,1]\\).\n\n\n\nExercice 14\nOn souhaite peindre un mur (infini!) en utilisant un arroseur automatique qui effectue des demi-révolutions successives. Pour simplifier le modèle, on représentera le mur par une droite verticale \\(\\Delta\\), et l’arroseur par une source ponctuelle \\(O\\) située à \\(1\\) mètre du mur, et émettant en tout instant \\(t\\) de façon parfaitement rectiligne dans la direction \\(\\overset{\\rightarrow}{u}(t)\\). On note \\(M\\) la projection orthogonale de \\(O\\) sur \\(\\Delta\\) et \\(\\theta(t)\\) l’angle entre \\(O \\overset{\\rightarrow}{u}(t)\\) et \\(\\overset{\\rightarrow}{OM}\\). L’intersection de \\(O\\overset{\\rightarrow}{u}\\) avec \\(\\Delta\\) est notée \\(H(\\theta)\\).\nOn suppose en outre que lors d’une demi-révolution, \\(\\theta(t)\\) parcourt exactement l’intervalle \\((-\\pi/2,\\pi/2)\\).\nOn fait l’hypothèse que la demi-révolution s’effectue à vitesse angulaire constante, et on se demande quelle sera la répartition de l’épaisseur de la couche de peinture le long de \\(\\Delta\\) après un nombre entier de demi-révolutions.\n\nJustifier qu’une particule de peinture choisie uniformément au hasard parmi toutes les particules est envoyée suivant un angle \\(\\theta \\sim \\mathrm{Unif}(-\\pi/2,\\pi/2)\\). Une telle particule se pose alors en \\(H(\\theta)\\), On note \\(h(\\theta)\\) l’ordonnée de \\(H(\\theta)\\) (c’est également la distance algébrique entre \\(O\\) et \\(H\\)).\nExprimer \\(h(\\theta)\\) en fonction de \\(\\theta\\). Quelle est la loi de \\(h(\\theta)\\)? Que pouvez-vous en déduire sur la distribution de l’épaisseur de la couche de peinture le long du mur?\nA posteriori, quelle critique peut-on formuler sur le modèle?\n\n\n\nExercice 15\nSoit \\(X \\sim \\mathrm{Cauchy}\\) (de paramètre \\(1\\)).\nQuelle est la loi de\n\n\\(Y:=\\frac{1}{X}\\)?\n\\(Z:= \\frac{1}{1+X^2}\\)?\n\n\n\nExercice 16\nSoit \\(Z \\sim \\mathcal{N}(0,1)\\). Montrer que pour tout \\(x&gt;0\\),\n\\[\\left(x^{-1}-x^{-3}\\right) \\exp(-x^2/2) \\le \\sqrt{2\\pi} \\mathbb{P}(Z&gt;x) \\le x^{-1} \\exp(-x^2/2).\\]\nIndication : on pourra penser à utiliser le changement de variable \\(y=x+z\\) pour obtenir l’inégalité de droite, et on commencera par calculer la dérivée de \\(\\left(x^{-1}-x^{-3}\\right) \\exp(-x^2/2)\\) pour obtenir celle de gauche.\n\n\nExercice 17 (calcul d’une loi conditionnelle discrète)\nSoient \\(X_1,...,X_n\\) des variables de Poisson, indépendantes, de paramètres respectifs \\(\\lambda_1,...,\\lambda_n\\).\n\nDéterminer la loi de \\(Y:=\\sum_{k=1}^n X_k\\).\nPour \\(r \\in \\mathbb{N}\\), que vaut la loi conditionnelle de \\((X_1,...,X_n)\\) sachant \\(Y=r\\)?"
  },
  {
    "objectID": "exercices/td1.html#exemples-divers",
    "href": "exercices/td1.html#exemples-divers",
    "title": "Variables aléatoires réelles",
    "section": "Exemples divers",
    "text": "Exemples divers\n\nExercice 18\nOn suppose que le nombre \\(X\\) d’oeufs pondus par un insecte suit une loi de Poisson de paramètre \\(\\lambda&gt;0\\) et que la probabilité qu’un oeuf meurt sans éclore est, indépendamment des autres oeufs, égale à \\(1-p\\), où \\(p \\in ]0,1[\\).\n\nDémontrer que le nombre \\(Y\\) d’oeufs qui arrivent à éclosion suit une loi de Poisson de paramètre \\(\\lambda p\\).\nQuelle est la loi jointe de \\((Y,Z)\\), où \\(Z= X-Y\\) est le nombre d’oeufs morts avant éclosion?\n\n\n\nExercice 19\n(Somme d’exponentielles et \\(\\chi^2\\))\nSoit \\(\\lambda&gt;0\\) et \\((Y_i)_{1 \\le i \\le n}\\) des variables i.i.d, \\(\\sim \\exp(\\lambda)\\).\n\nCalculer \\(\\mathbb{E}[\\exp(-tY_1)]\\) pour \\(t \\ge 0\\).\n\nCalculer \\(\\mathbb{E}\\left[\\exp(-t\\sum_{i=1}^n) Y_i \\right]\\) pour \\(t \\ge 0\\).\nMontrer que la densité de la variable \\(X= \\sum_{i=1}^n Y_i\\) est proportionnelle à \\(x^{n-1} \\exp(-\\lambda x) \\mathbf{1}_{x \\ge 0}\\). En déduire la valeur de cette densité.\nSoient \\(X_n, n \\ge 1\\) des variables i.i.d, \\(\\sim \\mathcal{N}(0,1)\\). On pose \\(\\chi^2(n):=\\sum_{i=1}^n X_i^2\\), \\(Z:=X_1X_2 +X_3X_4\\). \\ Calculer \\(\\Phi_{\\chi^2(n)}, \\Phi_Z\\). Pouvez-vous deviner la distribution de \\(Z\\) (on pourra utiliser un résultat d’un exercice précédent)?\n\n\n\nExercice 20\nSoit \\(Z = (X, Y )\\), une variable aléatoire à valeurs dans \\(\\mathbb{R}^2\\). On suppose que \\(Z\\) admet une densité \\(f\\) définie par\n\\[f(x, y) = \\frac{4}{\\pi \\sigma^2} \\exp\\left(-\\frac{x^2+y^2}{\\sigma^2}\\right) \\mathbf{1}_{\\{x \\ge |y|\\}},\\]\noù \\(\\sigma&gt;0\\).\n\nVérifier que \\(f\\) est bien une densité de probabilité.\nCalculer les lois de \\(X\\) et de \\(Y\\) . Les variables aléatoires \\(X\\) et \\(Y\\) sont-elles indépendantes ?\nCalculer la loi de \\((X - Y, X + Y )\\) et montrer que \\(X - Y\\) et \\(X + Y\\) sont indépendantes.\n\n\n\nExercice 21\n\nSoit \\(X_1,...,X_n\\) des variables i.i.d, \\(\\sim \\mathcal{N}(0,1)\\), et \\(Z=\\sum_{i=1}^n \\alpha_i X_i\\) (où les \\(\\alpha_i, i=1,...,n\\) sont de rééls fixés). Quelle est la loi de \\(Z\\)?\nSoit \\((X_1,...,X_n) \\sim \\mathcal{N}(0,A)\\). Quelle est la loi de \\(Z=\\sum_{i=1}^n \\alpha_i X_i\\)?\nSoit \\((X_1,...,X_n) \\sim \\mathcal{N}(0,A)\\). Quelle est la loi de \\((Z_1,Z_2)\\), où, pour des rééls \\(\\alpha_{i,1}, \\alpha_{i,2}, i=1,...,n\\) fixés, \\[  Z_1=\\sum_{i=1}^n \\alpha_{i,1} X_i,  Z_2=\\sum_{i=1}^n \\alpha_{i,2} X_i.\\]\nGénéraliser la question précédente en exprimant la loi de \\[ (Z_1,...,Z_n) =  (X_1,...,X_n) P,\\] où \\(P\\) est une matrice \\(n \\times n\\).\nSoient \\(X,Y\\) deux variables indépendantes, \\(\\sim \\mathrm{Unif}[0,1]\\). Quelle est la loi de \\(S=X+Y\\)?\nSoient \\(X, Y\\) des variables indépendantes de loi respectives \\(\\Gamma(a,c), \\Gamma(b,c)\\), où \\(a,b,c&gt;0\\). On pose \\(S=X+Y,\nT= \\frac{X}{X+Y}\\) Quelle est la loi du couple \\((S,T)\\)?\nSoient \\(X,Y\\) deux variables indépendantes, \\(\\sim \\mathrm{Unif}[0,1]\\). On pose \\(U = \\sqrt{-2\\log(X)} \\cdot \\cos(2\\pi Y), V = \\sqrt{-2\\log(X)} \\cdot \\sin(2\\pi Y)\\). Quelle est la loi du couple \\((X,Y)\\)?\nSoient \\(X,Y\\) deux variables indépendantes, \\(\\sim \\mathcal{N}(0,1)\\). On pose \\(T = \\frac{Y}{X}\\). Quelle est la loi de \\(T\\)?\nSoient \\((X,Y)\\) un couple de variables indépendantes, \\(\\sim \\mathcal{N}(0,1)\\). On pose \\(U=X, V= X^2 + Y^2\\). Quelle est la loi de \\((U,V)\\)?\nSoit \\(X\\) de densité \\(\\exp(-x) \\mathbf{1}_{\\mathbb{R}_+}(x)\\). On pose \\(U = [X]\\) et \\(V= X-[X]\\), la partie entière, resp. la partie décimale de \\(X\\). Quelle est la loi de \\((U,V)\\)?\n\n\n\nExercice 22\nSoient \\(X_1, X_2\\) deux variables indépendantes et identiquement distribuées suivant la loi \\(\\mathrm{Unif}\\{1,2,3\\}\\).\nOn note \\(U = \\min\\{X_1,X_2\\}\\), \\(V= \\max\\{X_1,X_2\\}\\) et enfin \\(S= U+V\\).\n\nDéterminer la loi jointe de \\((U, V)\\) et \\((V, S)\\).\nEn déduire les lois de \\(U, V\\), et \\(S\\). Calculer les lois de \\(UV\\) et \\(VS\\).\nCalculer les covariances et les coefficients de corrélation de \\((U, V)\\) et \\((V, S)\\)."
  },
  {
    "objectID": "exercices/td1.html#le-cadre-gaussien",
    "href": "exercices/td1.html#le-cadre-gaussien",
    "title": "Variables aléatoires réelles",
    "section": "Le cadre gaussien",
    "text": "Le cadre gaussien\n\nExercice 23\nOn considère deux variables indépendantes \\(Y \\sim \\mathcal{N}(0,1)\\) et\n\\[\\varepsilon= \\begin{cases} & 1 \\mbox{ avec probabilité } p \\\\ & -1 \\mbox{ avec probabilité } 1-p, \\end{cases}\\]\noù \\(p \\in (0,1)\\).\n\nQuelle est la loi de \\(Z= \\varepsilon Y\\)\nQuelle est la loi de \\(Y+Z\\)?\nLe vecteur \\((Y,Z)\\) est-il un vecteur gaussien?\n\n\n\nExercice 24\nSoit \\((X,Y,Z)\\) le vecteur aléatoire gaussien d’espérance \\((1,1,0)\\) et de matrice de covariance\n\\[K = \\begin{pmatrix} 2 & 1 & 1 \\\\ 1 & 2 & 2 \\\\ 1 & 2 & 2  \\end{pmatrix}.\\]\n\nEcrire la fonction caractéristique de \\((X,Y,Z)\\).\n\nTrouver la loi de \\(2X+Y+Z\\), de \\(4X-2Y+Z\\), enfin de \\(Y-Z\\).\nLe vecteur \\((X,Y)\\) admet-il une densité dans \\(\\mathbb{R}^2\\). Si oui, laquelle?\nPour \\(a \\in \\mathbb{R}\\) on définit \\(u_a : \\mathbb{R}^3 \\to \\mathbb{R}^3\\) de matrice\n\\[A = \\begin{pmatrix} 1/\\sqrt{2} & 0 & 0 \\\\ a & -2a & 0 \\\\ 0 & 1 & -1\\end{pmatrix}.\\]\nDéterminer la loi de \\(u_a(X-1,Y-1,Z)\\) en fonction de \\(a\\).\nPour quelle valeur de \\(a\\) les deux premières coordonnées de \\(u_a(X-1,Y-1,Z)\\) suivent-ils une loi normale centrée réduite sur \\(\\mathbb{R}^2\\)?\n\n\n\nExercice 25\nSoit \\(\\rho\\in ]-1,1[\\) et \\((X,Y)\\) un vecteur gaussien centré de matrice de covariances \\(M = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}\\). On notera \\(\\sigma = \\sqrt{1-\\rho^2}\\).\n\nCalculer det(\\(M\\)), \\(M^{-1}\\), puis exprimer la densité \\(f_{(X,Y)}\\) du vecteur \\((X,Y)\\).\nMontrer que\n\\[g_x(y) := \\frac{f_{(X,Y)}(x,y)}{f_X(x)} = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp \\left( - \\frac{1}{2(1-\\rho^2)} \\left(y-\\rho x\\right)^2\\right).\\]\nMontrer que pour tout \\(x \\in \\mathbb{R}\\), \\(y \\to g_x(y)\\) définit une densité.\nSi on note \\(Y_x\\) une variable de densité \\(g_x\\), que pouvez-vous dire sur la loi de \\(Y_x\\)?\n\nTrouver \\(\\alpha\\), \\(\\beta\\) deux réels tels que \\((X, \\alpha X + \\beta Y)\\) suit la loi normale centrée réduite.\n\nRemarquer que l’on peut écrire \\(Y= -\\frac{\\alpha}{\\beta} X +\\frac{1}{\\beta}(\\alpha X + \\beta Y)\\). Sauriez-vous dire pourquoi cette écriture est intéressante?\n\n\nExercice 26\nMontrer que le vecteur aléatoire de dimension \\(3\\) de moyenne \\(m = (7,0,1)\\) et de matrice de covariances\n\\[K = \\begin{pmatrix} 10 & -1 & 4 \\\\ -1 & 1 & -1 \\\\ 4 & -1 & 2 \\end{pmatrix}\\]\nappartient presque sûrement à un hyperplan affine de \\(\\mathbb{R}^3\\) que l’on déterminera."
  },
  {
    "objectID": "exercices/td2.html",
    "href": "exercices/td2.html",
    "title": "MA1AY010 Probabilités",
    "section": "",
    "text": "NoteTD II : Espérances et lois conditionnelles\n\n\n\n22 Septembre 2025-26 Septembre 2025\n\nMaster I Isifar\nProbabilités\n\n\n\n\nExercice 1\n\n\n\n\n\n\nNoteCours\n\n\n\nEspérance conditionnelle par rapport à une tribu engendrée par une partition dénombrable.\n\n\n\nSoit \\((A_n, n \\in \\mathbb{N}^*)\\) une partition de \\(\\Omega\\) et \\(\\mathcal{F}= \\sigma(A_n, n \\ge 1)\\) la tribu engendrée par les \\(A_n, n \\ge 1\\). Rappelons qu’une v.a.r. \\(Y\\) est \\(\\mathcal{F}\\)-mesurable si et seulement si il existe une suite de réls \\((a_n)\\) telle que \\(Y= \\sum_{n \\ge 1} a_n \\mathbf{1}_{A_n}\\). Exprimer \\(\\mathbb{E}[X \\mid \\mathcal{F}]\\).\nSoient \\(X,Y\\) deux variables i.i.d. \\(\\sim\\) Ber\\((p)\\). On considère \\(\\mathcal{G} = \\sigma(\\{X+Y=0\\})\\). Calculer \\(\\mathbb{E}[X \\mid \\mathcal{G}], \\mathbb{E}[Y\\mid \\mathcal{G}]\\). Les variables obtenues sont-elles toujours indépendantes?\n\n\n\nExercice 2\n\n\n\n\n\n\nNoteCours\n\n\n\nConditionnement continu\n\n\nSoient \\((X,Y)\\) un couple de v.a. réelles intégrables de densité jointe \\(f\\), \\(g : \\mathbb{R}^2 \\to \\mathbb{R}\\) borélienne telle que \\(g(X,Y) \\in \\mathbb{L}^1\\).\nRappeler l’expression de \\(\\phi, \\psi\\) telles que \\[\\mathbb{E}[g(X,Y)\\mid Y] = \\phi(Y), \\quad \\mathbb{E}[g(X,Y)|X] = \\psi(X).\\]\n\nOn considère \\((X,Y)\\) de densité jointe \\(f(x,y)= \\frac{1}{x} \\mathbf{1}_{\\{0 \\le y \\le x \\le 1\\}}.\\) Quelle est la loi de \\(X\\)? Calculer la distribution conditionnelle \\(f_{Y \\mid X}\\) de \\(Y\\) sachant \\(X\\). Calculer \\(\\mathbb{P}(X^2 +Y^2 \\le 1 |X)\\), puis en déduire \\(\\mathbb{P}(X^2+Y^2 \\le 1)\\).\nPour simplifier l’expression obtenue on pourra utiliser que \\(x \\to \\sqrt{1-x^2} - \\tanh^{-1}(\\sqrt{1-x^2}) = \\sqrt{1-x^2}-\\frac{1}{2} \\ln(1+\\sqrt{1-x^2}) + \\frac{1}{2} \\ln(1-\\sqrt{1-x^2})\\) est une primitive de \\(x \\to \\frac{\\sqrt{1-x^2}}{x}\\).\nDans le cas général, montrer que \\(\\mathbb{E}[\\mathbb{E}[Y|X]] = \\mathbb{E}[Y]\\). Que vaut \\(\\mathbb{E}[Y]\\) dans l’exemple de la question précédente?\nMontrer, dans le cas général, que \\[ \\mathbb{E}[\\mathbb{E}[Y|X] g(X)] = \\mathbb{E}[Yg(X)],\\] pour toute fonction \\(g\\) telle que les deux espérances sont définies. Que vaut \\(\\mathbb{E}[Y g(X) \\mid X]\\)?\n\n\n\nExercice 3\n\n\n\n\n\n\nNotePartiel passé\n\n\n\n\n\n\nSoient \\(0 \\le r \\le p \\le 1\\) tels que \\(1-2p+r \\ge 0\\).\nSoient \\(X_1, X_2\\) tels que\n\\[\\begin{eqnarray*}\n&&  \\mathbb{P}(X_1=1, X_2=1)=r, \\quad   \\mathbb{P}(X_1=0, X_2=1)=p-r, \\\\\n&& \\mathbb{P}(X_1=1, X_2=0)=p-r, \\quad  \\mathbb{P}(X_1=0, X_2=0)=1-2p+r.\n\\end{eqnarray*}\\]\n\nQuelle est la loi de \\(X_1\\)? celle de \\(X_2\\)?\nCalculer \\(Y = \\mathbb{E}[X_1\\mid X_2]\\) et vérifier que \\[Y= \\begin{cases} & \\frac{p-r}{1-p} \\mbox{ avec probabilité } 1-p\\\\\n& \\frac{r}{p} \\mbox{ avec probabilité } p.\\end{cases}\\]\nRappelons que par définition \\(\\mathrm{Var}[X_1 \\mid X_2] = \\mathbb{E}[X_1^2\\mid X_2] - \\mathbb{E}[X_1\\mid X_2]^2\\). Montrer que \\[\\mathrm{Var}[X_1 \\mid X_2] = \\left( \\frac{p-r}{1-p} - \\left(\\frac{p-r}{1-p}\\right)^2\n\\right) \\mathbf{1}_{\\{X_2=0\\}} + \\left( \\frac{r}{p} - \\left(\\frac{r}{p}\\right)^2 \\right)\n\\mathbf{1}_{\\{X_2=1\\}}.\\]\nQue vaut \\(\\mathrm{Var}(\\mathbb{E}[X_1\\mid X_2])\\)? \\(\\mathbb{E}[\\mathrm{Var}[X_1\\mid X_2]]\\)? Vérifier qu’on a bien \\[\\mathrm{Var}(X_1) = \\mathrm{Var}(\\mathbb{E}[X_1\\mid X_2]) + \\mathbb{E}[\\mathrm{Var}[X_1\\mid X_2]].\\]\n\n\n\nExercice 4\nSoit \\((X_n)\\) une suite de v.a. .i.i.d intégrables, et \\(S_n = \\sum_{i=1}^n X_i\\).\n\nQue valent \\(\\mathbb{E}[X_1\\mid X_2], \\mathbb{E}[S_n \\mid X_1], \\mathbb{E}[S_n \\mid S_{n-1}]?\\)\nMontrer que si les paires de variables \\((X,Z)\\), \\((Y,Z)\\) ont la même loi jointe, alors pour toute fonction réelle positive (ou satisfaisant une condition d’intégrabilité), \\(\\mathbb{E}[f(X)\\mid Z] = \\mathbb{E}[f(Y)\\mid Z]\\). En déduire \\(\\mathbb{E}[X_1 \\mid S_n]\\).\n\n\n\nExercice 5\n\n\n\n\n\n\nNote(Examen passé)\n\n\n\n\n\n\nSoit \\((X_n, n \\ge 0)\\) une suite de variables i.i.d, avec \\(X_1 \\sim \\text{Ber}(1/2)\\). On pose \\(S_n = \\sum_{i=1}^n (X_i -1/2)\\), \\(\\mathcal{F}_n= \\sigma(X_1,...,X_n)\\).\nCalculer \\(\\mathbb{E}[S_n \\mid \\mathcal{F}_5]\\) en fonction de \\(n\\). Quelle est la loi de cette variable aléatoire?\n\n\nExercice 6\n\n\n\n\n\n\nNotePartiel passé\n\n\n\n\n\n\nSoient \\(\\{\\mathbf{e}_i, i \\in \\mathbb{N} \\}\\) des variables i.i.d exponentielles de paramètre \\(1\\). Pour \\(n \\in \\mathbb{N}^*\\) on note \\(S_n := \\sum_{i=1}^n \\mathbf{e}_i\\).\n\nOn note \\(f_n\\) la fonction de densité de la variable \\(S_n\\). Montrer que pour tout \\(t \\ge 0\\) \\[ f_n(t) = \\frac{t^{n-1}}{(n-1)!} \\exp(-t).\\]\nPour \\(t &gt;0, n \\in \\mathbb{N}^*\\), que vaut \\(\\mathbb{P}(S_n \\le t)\\)?\nOn fixe \\(t&gt;0\\) et on suppose \\(X_t \\sim \\mathrm{Poisson}(t)\\). Que vaut \\(\\mathbb{P}(X_t \\ge n)\\), pour \\(n \\in \\mathbb{N}^*\\)?\nSur la demi-droite \\(\\mathbb{R}_+\\) on place les points \\(S_1, S_2, S_3,...\\). On note \\(N_t\\) le nombre de ces points qui tombent dans l’intervalle \\([0,t]\\). Exprimer l’événement \\(\\{N_t \\ge n\\} = \\{S_n \\le t\\}\\). Déterminer la loi de \\(N_t\\) à l’aide des questions préc'dentes.\n\nMontrer que, conditionnellement à \\(\\{N_t=1\\}\\), la loi de \\(\\mathbf{e}_1\\) est uniforme sur \\([0,t]\\).\nConditionnellement à \\(\\{N_t=2\\}\\), quelle est la loi du vecteur \\((\\mathbf{e}_1; \\mathbf{e}_2)\\)?\n\n\n\nExercice 7\n\n\n\n\n\n\nNoteCC2 2023\n\n\n\n\n\n\nOn considère \\[ X \\sim \\mathcal{N} \\left( \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 2 & 1 & -1 & -1 \\\\ 1 & 2 & -1 & 0 \\\\ -1 & -1 & 3 & -1 \\\\ -1 & 0 & -1 & 5 \\end{pmatrix}\\right).\\]\n\nCalculer \\(\\mathbb{E}[X_3 \\mid X_4]\\), et déterminer la loi conditionnelle de \\(X_3\\) sachant \\(X_4\\).\n\nOn pose \\(A = \\begin{pmatrix} 2  & 1 \\\\ 1 & 2 \\end{pmatrix}\\), \\(B= \\begin{pmatrix} -1 & -1 \\\\ -1 & 0 \\end{pmatrix}\\). Calculer \\(BA^{-1}\\), puis vérifier que \\[B A^{-1} B^T = \\begin{pmatrix} \\frac{2}{3} & \\frac{1}{3} \\vspace{0.1cm} \\\\ \\frac{1}{3} & \\frac{2}{3}\\end{pmatrix}.\\]\nDéterminer \\(\\mathbb{E}\\left[\\begin{pmatrix} X_3 \\\\ X_4 \\end{pmatrix} \\ \\bigg| \\ \\begin{pmatrix} X_1 \\\\ X_2 \\end{pmatrix}\\right]\\). et la loi conditionnelle de \\(\\begin{pmatrix} X_3 \\\\ X_4 \\end{pmatrix}\\) sachant \\(\\begin{pmatrix} X_1 \\\\ X_2 \\end{pmatrix}\\).\n\n\n\nExercice 8\n\n\n\n\n\n\n(Partiel passé)\n\n\n\nSoit \\((X_1,X_2,X_3) \\sim \\mathcal{N}(\\mu, M)\\) où \\[\\mu = \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}, \\quad M = \\begin{pmatrix} 2 & 1 & 0 \\\\ 1 & 2 & 1 \\\\ 0 & 1 & 2 \\end{pmatrix}.\\]\n\nQuelle est la loi du couple \\((X_1,X_2)\\)?\nDéterminer \\(\\alpha\\) un réel tel que $Y = X_1 + X_2 $ est indépendante de \\(X_1\\). Que vaut \\(\\mathbb{E}[Y]\\)? \\(\\text{Var}(Y)\\)?\nEn déduire \\(\\mathbb{E}[X_2 \\mid X_1]\\). Quelle est la loi conditionnelle de \\(X_2\\) sachant \\(X_1\\)?\nDéterminer un réel \\(\\beta\\) tels que \\(Z=\\beta X_1 + X_3\\) est indépendante de \\(X_1\\). En déduire \\[ \\mathbb{E}[X_3 \\mid X_1], \\quad \\mathbb{E}[X_3^2 \\mid X_1].\\]\nCalculer \\(\\mathbb{E}\\left[X_1^2X_2 + X_3^2 X_1 \\mid X_1\\right]\\).\n\n\n\nExercice 9\n\n\n\n\n\n\nNoteExamen passé\n\n\n\n\n\n\nSoit \\((X_1,X_2,X_3) \\sim \\mathcal{N}(\\mu,M)\\), où \\[ \\mu = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\qquad M=  \\begin{pmatrix} 1 & 1/2 & 2 \\\\ 1/2 & 1 & 1 \\\\ 2 & 1 & 3 \\end{pmatrix}.\\] Calculer \\(\\mathbb{E}[X_1+2X_2 \\mid X_3].\\) Quelle est la loi conditionnelle de \\(X_1+2X_2\\) sachant \\(X_3\\)?\n\n\nExercice 10\n\n\n\n\n\n\nNote(CC2 2023)\n\n\n\n\n\n\nOn suppose dans cet exercice que \\((X,Y)\\) est un couple de variables aléatoires tel que pour toute \\(\\phi : \\mathbb{R}^2\\to \\mathbb{R}_+\\) borélienne, \\[ \\mathbb{E}[\\phi(X,Y)] = \\sum_{n \\ge 1} \\frac{2}{3^{n}\\sqrt{2\\pi n}}  \\int_{\\mathbb{R}} \\phi(n,y) \\exp\\left(-\\frac{y^2}{2n}\\right) dy.\\]\n\nMontrer que \\(X \\sim \\mathrm{Geom}(2/3)\\).\nVérifier que pour une fonction \\(f : \\mathbb{R} \\to \\mathbb{C}\\) telle que \\(f(Y) \\in \\mathbb{L}^1\\), on a\n\\[ \\mathbb{E}[f(Y) \\mid X] = \\sum_{n \\ge 1} \\left(\\int_{\\mathbb{R}} \\frac{1}{\\sqrt{ 2\\pi n}} f(y) \\exp\\left(-\\frac{y^2}{2n} \\right) dy \\right) \\mathbb{I}_{\\{X=n\\}}\\] %1. En déduire que pour tout \\(k \\in \\mathbb{N}\\), %\\[\\mathbb{E}[Y^k \\mid X] = \\frac{k!}{X^{2k}}\\]\nCalculer \\(\\mathbb{E}[\\exp(itY) \\mid X]\\), \\(t \\in \\mathbb{R}\\), quelle est la loi conditionnelle de \\(Y\\) sachant \\(X\\) ?\nDéduire que si \\(t\\in \\mathbb{R}\\) \\[ \\mathbb{E}[\\exp(itY)] = \\frac{2 \\exp\\left(-\\frac{t^2}{2}\\right)}{3-\\exp\\left(-\\frac{t^2}{2}\\right)}.\\]\n\n\n\nExercice 11\n\n\n\n\n\n\nNotePartiel passé\n\n\n\n\n\n\n\nPartie I\nOn considère le couple \\((X,Z)\\) de densité jointe \\[ f(x,z) := (z-x)\\exp(-z) \\mathbf{1}_{\\{z \\ge x \\ge 0\\}}.\\]\n\nCalculer la loi de \\(X\\), puis celle de \\(Z\\).\nEn déduire que \\[ f_{X \\mid Z}(x \\mid z) = \\frac{2(z-x)}{z^2} \\mathbf{1}_{\\{0 \\le x \\le z, z &gt;0\\}}.\\]\nCalculer \\(\\mathbb{E}[X \\mid Z]\\), puis \\(\\mathrm{Var}[X\\mid Z]\\).\nCalculer \\(f_{Z \\mid X}(z \\mid x)\\), puis démontrer que \\(\\mathbb{E}[Z \\mid X] = X + 2\\).\nQuelle est la loi du couple \\((X, Z-X)\\)? En déduire la loi de \\(Z-X\\).\n\n\n\nPartie II\n\nSoit \\(z &gt;0\\). On suppose que \\(U_1^z \\sim \\mathrm{Unif}[0,z]\\), \\(U_2^z \\sim\n\\mathrm{Unif}[0,z]\\) et que \\(U_1^z\\) est indépendante de \\(U_2^z\\). Calculer la densité de \\(\\min(U_1^z, U_2^z)\\).\nOn suppose à présent que {}, \\(U_1^Z \\sim \\mathrm{Unif}[0,Z]\\), \\(U_2^Z \\sim \\mathrm{Unif}[0,Z]\\) et que \\(U_1^Z\\) est (toujours conditionnellement à \\(Z\\)) indépendante de \\(U_2^Z\\). Montrer que, conditionnellement à \\(Z\\), \\(\\mathrm{min}(U_1^Z, U_2^Z)\\) a la même loi que X.\nSoient \\(X_1,X_2,X_3\\) trois variables indépendantes, toutes trois distribuées suivant la distribution exponentielle de paramètre \\(1\\). On note \\(S= X_1+X_2+X_3\\). Déterminer la loi de \\((X_1,S)\\). \\ Que vaut \\(\\mathbb{E}[X_1\\mid S]\\)? \\(\\mathbb{E}[S \\mid X_1]\\)?\\ Montrer finalement que conditionnellement à \\(S\\), le couple \\((X_1,X_1+X_2)\\) a la même loi que \\(\\left(\\mathrm{min}(U_1^S, U_2^S), \\mathrm{max}(U_1^S, U_2^S)\\right)\\).\n\n\n\n\nExercice 12\n\n\n\n\n\n\nNotePartiel passé\n\n\n\n\n\n\nPour \\((x,y) \\in \\mathbb{R}^2\\) on définit\n\\[f(x,y) := \\frac{4y}{x^3} \\mathbf{1}_{\\{0&lt;x&lt;1, 0&lt;y &lt;x^2\\}}.\\]\nVérifier que \\(f\\) est bien une densité de probabilité, puis calculer les densités marginales \\(f_X\\), \\(f_Y\\).\nCalculer \\(f_{Y\\mid X}(y \\mid x)\\) et en déduire que \\[\\mathbb{E}[Y \\mid X] = \\frac{2}{3} X^2.\\]\nMontrer que \\[ f_{X \\mid Y}(x\\mid y) = \\frac{2y}{1-y} \\frac{1}{x^3} \\mathbf{1}_{\\{0&lt;x&lt;1, 0&lt;y &lt;x^2\\}},\\] puis calculer \\(\\mathbb{E}[X \\mid Y]\\).\n\n\nExercice 13\n\n\n\n\n\n\nNoteCC2 2023\n\n\n\n\n\n\nDans cet exercice on suppose que \\[ \\begin{pmatrix} X \\\\ Y\\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 2 & 1 \\\\ 1 & 1 \\end{pmatrix} \\right),\\]\net on pose \\(U= X^2\\).\n\nVérifier que \\(U \\sim \\mathrm{Gamma}(1/2,1/4)\\).\nMontrer que \\((X,Y)\\) possède une densité jointe \\(g\\) que l’on déterminera.\n\nMontrer que \\((U,Y)\\) possède la densité jointe \\[ f(u,y) = \\frac{1}{{ 4} \\pi \\sqrt{u}} \\left( \\exp\\left(- \\frac{u}{2} - y^2 + y \\sqrt{u} \\right) + \\exp\\left(-\\frac{u}{2}- y^2 - y\\sqrt{u} \\right) \\right) \\mathbb{I}_{\\{u &gt;0\\}}. \\]\nCalculer \\(f_{Y \\mid U}(y \\mid u)\\). En déduire \\(\\mathbb{E}[Y \\mid U], \\mathbb{E}[Y^2 \\mid U]\\) et \\(\\mathrm{Var}(Y \\mid U)\\). Vérifier qu’on a bien \\[\\mathrm{Var}[Y] = \\mathbb{E}[\\mathrm{Var}[Y \\mid U]] + \\mathrm{Var}[\\mathbb{E}[Y \\mid U]]\\, .\\]\nOn suppose que conditionnellement à \\(U\\), \\(\\xi\\) et \\(Z\\) sont indépendantes avec \\(\\xi \\sim \\mathrm{Ber}(1/2)\\) et \\(Z \\sim \\mathcal{N}\\left(\\frac{\\sqrt{U}}{2},\\frac{1}{2}\\right)\\). Montrer que conditionnellement à \\(U\\), \\((2\\xi-1) Z\\) a même loi que \\(Y\\). Vérifier alors les calculs de la question précédente.\n\n\n\n\n\n\n\nTipIndications\n\n\n\n\nrappelle que pour \\(a&gt;0, \\lambda &gt;0\\), la densité d’une variable \\(G \\sim \\mathrm{Gamma}(a,\\lambda)\\) est donnée par \\[ f_G(x) = \\frac{\\lambda^a x^{a-1}}{\\Gamma(a)} \\exp(-\\lambda x) \\mathbb{I}_{\\{x &gt;0 \\}}\\]\n\nOn fera attention à distinguer les domaines \\(D_1 = \\mathbb{R}_-^*\\times \\mathbb{R}\\) et \\(D_2 = \\mathbb{R}_+^* \\times \\mathbb{R}\\) pour pouvoir considérer les \\(\\mathcal{C}^1\\)-difféomorphismes \\(\\displaystyle{\\Psi_1 : \\begin{cases} \\!\\!&\\!\\! D_1 \\to \\mathbb{R}_+^* \\times \\mathbb{R} \\\\ \\!\\!&\\!\\! (x,y) \\to (x^2,y) \\end{cases}, \\ \\ \\Psi_2 :  \\begin{cases} \\!\\!&\\!\\! D_2 \\to \\mathbb{R}_+^* \\times \\mathbb{R} \\\\ \\!\\!&\\!\\! (x,y) \\to (x^2,y)\\end{cases}}\\).\n\nPour \\(\\alpha \\in \\mathbb{R}\\), les deux premiers moments de la variable \\(\\zeta \\sim \\mathcal{N}\\left(\\alpha \\frac{\\sqrt{u}}{2}, \\frac{1}{2}\\right)\\) sont \\[\\begin{align*}\n\\mathbb{E}[\\zeta]\n& = \\int_{\\mathbb{R}} \\frac{1}{\\sqrt{\\pi}} y \\exp\\left(-  \\left(y - \\alpha \\frac{\\sqrt{u}}{2}\\right)^2 \\right) dy \\\\\n& = \\alpha \\frac{\\sqrt{u}}{2},  \\\\\n\\mathbb{E}[\\zeta^2]\n& = \\mathbb{E}[\\zeta]^2+ \\mathrm{Var}[\\zeta] = \\int_{\\mathbb{R}} \\frac{1}{\\sqrt{\\pi}} y^2 \\exp\\left(-  \\left(y - \\alpha \\frac{\\sqrt{u}}{2}\\right)^2 \\right) dy \\\\\n& = \\alpha^2 \\frac{u}{4} + \\frac{1}{2}\n\\end{align*}\\]\n\n\n\n\n\nExercice 14\n\n\n\n\n\n\nNoteRattrapage passé\n\n\n\n\n\n\nSoit \\(X=(X_1,X_2,X_3) \\sim \\mathcal{N}(0,M)\\), où\n\\[M := \\begin{pmatrix} 2& 2 &-2  \\\\ 2& 5 & 1 \\\\ -2 & 1 & 5 \\end{pmatrix},\\]\n\nMontrer que \\(\\det(M)=0\\). Le vecteur \\(X\\) possède-t-il une densité dans \\(\\mathbb{R}^3\\)?\nTrouver \\(a \\in \\mathbb{R}\\) tel que \\(X_1\\) et \\(Y=X_2-a X_1\\) soient indépendantes. Calculer \\(\\mathrm{Var}(Y)\\) et en déduire la loi de \\((X_1,Y)\\).\nTrouver la loi conditionnelle de \\(X_2\\) sachant \\(X_1\\).\n\n\n\nExercice 15\nOn considère \\(X_0 =0\\), et \\((X_n)_{n \\ge 1}\\) une suite de variables aléatoires réelles indépendantes, identiquement distribuées suivant la loi normale centrée réduite.\nOn introduit les variables\n\\[Y_i = \\frac{X_i-X_{i-1}}{i}, i \\ge 1.\\]\nPour \\(n \\ge 1\\), montrer que le vecteur \\((Y_1,...,Y_n)\\) est gaussien, puis calculer le vecteur moyenne et la matrice de covariances de \\((Y_1,...,Y_n)\\).\nCalculer, pour \\(n \\ge 1\\), \\(\\mathbb{E}[Y_{n+1}\\mid Y_n]\\).\n\n\nExercice 16\nSoient \\((X,Y)\\) dont la loi jointe a pour densité \\(f(x,y) = x(y-x) \\exp(-y), 0 \\le x \\le y &lt;\\infty\\). On introduit la notation \\(f_{X|Y}(x|y) := f(x,y)/f_Y(y)\\) lorsque le quotient est \\(&gt;0\\), \\(0\\) sinon.\n\nExprimer \\(f_{X|Y}(x|y)\\), puis \\(f_{Y|X}(y|x)\\).\nEn déduire les expressions de \\(\\mathbb{E}[X|Y], \\mathbb{E}[Y|X]\\).\n\n\n\nExercice 17\nSoient \\(Y,Z\\) deux v.a.r. indépendantes \\(\\sim \\mathrm{exp}(\\lambda)\\) où \\(\\lambda&gt;0\\). On pose \\(X= Y+Z\\). Quelle est la loi conditionnelle de \\(Y\\) sachant \\(X\\)? Que vaut \\(\\mathbb{E}[Y|X]\\)? En déduire l’expression de \\(\\mathbb{E}[Y|X]\\)\n\n\nExercice 18\nSoient \\(X\\) et \\(Y\\) deux variables aléatoires indépendantes, toutes deux normales centrées réduites. On définit pour \\(\\sigma_1 &gt;0, \\sigma_2&gt;0, |\\rho|\\le 1\\), \\[U = \\sigma_1 X, \\quad V= \\sigma_2 \\rho X + \\sigma_2 \\sqrt{1- \\rho^2} Y.\\]\n\nQuelle est la loi de \\((U,V)\\)?\nQue vaut \\(\\mathbb{E}[UV]\\)?\nQue vaut \\(\\mathbb{E}[U \\mid V]? \\mathbb{E}[V \\mid U]? \\mathrm{Var}[U \\mid V]? \\mathrm{Var}[V \\mid U]?\\)\n\n\n\nExercice 19\nSoit \\(Z = (X, Y )\\) un vecteur aléatoire gaussien à valeurs dans \\(\\mathbb{R}^2\\). On suppose que \\(E(X) = E(Y ) = 0\\), \\(\\mathrm{Var}(X) = \\mathrm{Var}(Y ) = 1\\) et que \\(\\mathrm{Cov}(X; Y ) = \\rho\\) avec \\(|\\rho|^2 \\ne 1\\). On pose \\(U = X -\\rho Y , V = \\sqrt{1-\\rho^2} Y\\).\n\nQuelles sont les lois de \\(U\\) et \\(V\\) ? Les v.a. \\(U\\) et \\(V\\) sont-elles indépendantes ?\nCalculer \\(\\mathbb{E}(U^2V^2), \\mathbb{E}(U V^3), \\mathbb{E}(V^4)\\). En déduire \\(\\mathbb{E}(X^2Y^2)\\).\nRetrouver ce dernier résultat par conditionnement.\n\n\n\nExercice 20\nSoient \\(U, V, W\\) trois v.a.r. gaussiennes centrées réduites. On pose \\[Z =\\frac{U + VW}{\\sqrt{1+W^2}}.\\]\n\nQuelle est la loi conditionnelle de \\(Z\\) sachant \\(W\\)?\nEn déduire que \\(Z\\) et \\(W\\) sont indépendantes et donner la loi de \\(Z\\).\n\n\n\nExercice 21\nSoient \\(X_1\\) et \\(X_2\\) des v.a. indépendantes, de lois exponentielles de paramètres respectifs \\(\\lambda_1\\) et \\(\\lambda_2\\).\n\nCalculer \\(\\mathbb{E}[\\max(X_1,X_2) \\mid X_1]\\).\nCalculer \\(\\mathbb{E}[\\max(X_1;X_2)]\\).\n\n\n\nExercice 22\nOn pose \\(h(x) = \\frac{1}{\\Gamma(a+1)} \\exp(-x)x^{a-1}\\) (\\(a &gt; 0\\) fixé) et \\(D = \\{0 &lt; y &lt; x\\}\\). Soit \\(f(x, y) = h(x)\\mathbf{1}_D(x, y)\\):\n\nMontrer que \\(f\\) est une densité de probabilité sur \\(\\mathbb{R}^2\\). On considère dans la suite un couple \\((X, Y)\\) de v.a.r. de densité \\(f\\).\nLes v.a. \\(X\\) et \\(Y/X\\) sont-elles indépendantes?\nQuelle est la loi conditionnelle de \\(Y\\) sachant \\(X\\) ?\nSoit \\(U\\) une v.a.r. indépendante du couple \\((X, Y)\\) telle que \\(\\mathbb{P}(U = 1) = p\\) et \\(\\mathbb{P}(U = 0) = 1 - p\\). On pose \\(Z = UX + (1 - U)Y\\). Quelle est l’espérance conditionnelle de \\(Z\\) sachant \\(X\\)?\n\n\n\nExercice 23\nSoit \\((X_n, n \\in \\mathbb{N})\\) une suite de v.a.r.i.i.d. de densité \\(f\\) et fonction de répartition \\(F\\). Soient \\(N := \\min\\{ n \\ge 1 : X_n &gt;X_0\\}\\) et\n\\(M := \\min \\{n \\ge 1 : X_0 \\ge X_1 \\ge ... \\ge X_{n-1} &lt;X_n \\}.\\)\n\nTrouver \\(\\mathbb{P}(N=n)\\), puis montrer que la fonction de répartition de \\(X_N\\) est \\(F +(1-F) \\log(1-F)\\) (on pourra conditionner par les événements \\(\\{N=n\\}, n \\in \\mathbb{N}\\)).\nExprimer \\(\\mathbb{P}(M=m), m \\ge 1\\).\nOn suppose dans cette question que \\(f = \\mathbf{1}_{[0,1]}\\). Pour \\(x \\in (0,1)\\) on introduit \\(R^x := \\min\\{ n \\ge 1 : X_1+...+X_n &gt;x \\}\\). Montrer que \\(\\mathbb{E}[\\mathbf{1}_{\\{R^x&gt;n\\}} \\mid X_n] = \\Phi(X_n)\\) où \\(\\Phi(u) = \\mathbb{I}_{\\{u&lt;x\\}}  \\mathbb{P}(R^{x-u} &gt; n-1)\\). En déduire \\(H_n(x):= \\mathbb{P}(R^x&gt;n)\\).\n\n\n\nExercice 24\nSoient \\(X\\) et \\(Y\\) deux v.a.r. indépendantes de loi uniforme sur \\([0, 1]\\).\n\nQuelle est l’espérance conditionnelle de \\((Y - X)_+\\) sachant \\(X\\)?\nQuelle est la loi conditionnelle de \\((Y - X)_+\\) sachant \\(X\\)?\n\n\n\nExercice 25\nSoient \\(X_1, X_2, X_3\\) trois v. a. r. gaussiennes centrées réduites indépendantes. On pose \\(U = 2X_1 - X_2 - X_3, V = X_1 + X_2 + X_3, W = 3X_1 + X_2 - 4X_3\\).\n\nQuelles sont les lois de \\(U, V\\) et \\(W\\)? Quels sont les couples de v.a. indépendantes parmi les couples \\((U, V), (U,W), (V,W)\\)?\nMontrer qu’il existe \\(a \\in \\mathbb{R}\\) tel que \\(W = aU + Z\\) avec \\(U\\) et \\(Z\\) indépendantes. En déduire \\(\\mathbb{E}(W \\mid U)\\).\n\n\n\nExercice 26\nSoient \\(X\\) et \\(Y\\) deux v. a. r. gaussiennes centrées réduites indépendantes. On pose \\(Z = X + Y\\) , \\(W = X - Y\\).\n\nMontrer que \\(Z\\) et \\(W\\) sont indépendantes. Quelle est la loi de \\(W\\)?\nEn déduire l’espérance conditionnelle et la loi conditionnelle de \\(X\\) sachant \\(Z\\).\nCalculer \\(\\mathbb{E}(XY \\mid Z)\\) et \\(\\mathbb{E}(XYZ \\mid Z)\\)."
  },
  {
    "objectID": "exercices/td4.html",
    "href": "exercices/td4.html",
    "title": "MA1AY010 Probabilités",
    "section": "",
    "text": "NoteTD IV : Espérance conditionnelle/Catactérisations\n\n\n\n\n29 Septembre 2025-2 octobre 2025\nMaster I ISIFAR\nProbabilités\n\n\n\n\n\n\n\n\n\nNoteConventions\n\n\n\nDans les 3 exercices qui suivent, \\(X_1, \\ldots, X_n, ...\\) constituent une famille indépendante de variables aléatoires identiquement distribuées à valeur dans \\(\\{-1,1\\}\\).\nL’univers des possibles est \\(\\Omega = \\{-1,1\\}^{\\mathbb{N}}\\). Les \\(X_i\\) sont les projections canoniques.\nOn note \\(\\mathcal{F}_n\\) la tribu engendrée par les \\(n\\) premières coordonnées :\n\\[\n\\mathcal{F}_n =  \\sigma(X_1, \\ldots, X_n) \\,\n\\]\nL’univers est muni de la tribu des cylindres \\(\\mathcal{F} = \\sigma\\left(\\bigcup_n \\mathcal{F}_n\\right)\\).\nOn note \\(\\Delta\\) une constante à valeur dans \\((0,1)\\) (la dérive de la marche aléatoire).\nOn note \\(\\mathbb{P}\\) la loi produit infini, telle que pour tout \\(x \\in \\{-1, 1\\}^n\\)\n\\[\n\\mathbb{P}\\left\\{\\bigwedge_{i=1}^n X_i = x_i\\right\\} = \\prod_{i=1}^n \\frac{1}{2}\\left(1 + x_i \\Delta\\right)\n\\]\nOn étudie la marche alétoire sur \\(\\mathbb{Z}\\) de dérive \\(\\Delta\\).\nOn note \\(S_n = \\sum_{i=1}^n X_i\\).\nL’indice \\(n\\) représente le temps, \\(S_n\\) la position à l’instant \\(n\\).\n\n\n\nExercice 1 (marches aléatoires biaisées i)\n\nQuelle est la loi de \\(S_n\\) ?\n\\(S_n\\) est-elle \\(\\mathcal{F}_n, \\mathcal{F}_{n-1}, \\mathcal{F}_{n+1}\\) mesurable ?\nQuelle est l’espérance de \\(S_n\\) ?\nQuelle est la variance de \\(S_n\\) ?\n\n\n\n\n\n\n\nNoteOn admettra l’inégalité de Hoeffding:\n\n\n\nSi \\(Y_1, \\ldots, Y_n\\) sont des variables aléatoires indépendantes telles que \\(a_i \\leq Y_i \\leq b_i\\) (les \\(Y_i\\) sont bornées), alors \\[\nP \\left\\{  Z - \\mathbb{E} Z \\geq t  \\right\\} \\leq \\mathrm{e}^{- 2 \\frac{t^2}{\\sum_{i=1}^n (b_i-a_i)^2}}\n\\] avec \\(Z = \\sum_{i=1}^n Y_i\\).\n\n\n\n\nExercice 2 (marches aléatoires biaisées ii)\nPour \\(0 \\leq \\tau \\leq  n \\Delta\\),\n\nMajorer \\(\\mathbb{P}\\{ S_n \\leq \\tau \\}\\) à l’aide de l’inégalité de Chebyshev\nMajorer \\(\\mathbb{P}\\{ S_n \\leq \\tau \\}\\) à l’aide de l’inégalité de Hoeffding\nL’ensemble \\[\nE = \\left\\{ \\omega :  \\forall n,  S_n(\\omega) &lt; \\tau \\right\\}\n\\] appartient-il à la tribu \\(\\mathcal{F}_m\\) pour un \\(m\\) donné ? est-il un événement de \\(\\mathcal{F}\\) ?\nSi \\(E\\) est un événement, quelle est sa probabilité ?\n\n\n\n\n\n\n\nNoteConvention\n\n\n\nOn suppose \\(\\tau \\in \\mathbb{N} ∖  \\{0\\}\\).\nOn note \\(T = \\inf \\left\\{ n : S_n \\geq \\tau \\right\\}\\). Si \\(\\forall n, \\quad S_n(\\omega)&lt; \\tau\\), alors \\(T(\\omega) = \\infty\\).\nOn note \\(S_T\\), la fonction définie par \\[\nS_T(\\omega) = \\sum_{n=1}^\\infty \\mathbb{I}_{T(\\omega)=n} S_n(\\omega)\\qquad \\text{si } T(\\omega) &lt; \\infty\n\\] et \\(S_T(\\omega) =0\\) si \\(T(\\omega) =\\infty\\).\n\n\n\n\nExercice 3 (marches aléatoires biaisées iii)\n\nPourquoi peut-on considérer que \\(T\\) est une variable aléatoire (à valeur dans \\(\\mathbb{N} \\cup \\{\\infty\\}\\)) ?\nQuelle est la probabilité que \\(T = \\infty\\) ?\nL’événement \\(\\{ T \\leq n \\}\\) est-il \\(\\mathcal{F}_{n-1}, \\mathcal{F}_n, \\mathcal{F}_{n+1}\\) mesurable ?\nPourquoi peut-on considérer que \\(S_T\\) est une variable aléatoire ?\nQuelle est l’espérance de \\(S_T\\) ?\nMontrer que \\(\\mathbb{E} S_T = \\Delta \\mathbb{E} T\\) En déduire \\(\\mathbb{E} T\\).\n\n\n\nExercice 4\nLes variables \\(X_1, X_2, \\ldots, X_n, \\ldots\\) sont des variables de Bernoulli de probabilité de succès \\(p \\in (0,1)\\), indépendantes. On définit \\(T_1 = \\min \\{i : X_i=1\\}\\) (temps du premier succès), \\(T_1 = \\min \\{i : i &gt; T_1, X_i=1\\}\\) (temps du premier succès après \\(T_1\\)), et récursivement \\(T_{n+1} = \\min \\{ i : i &gt; T_n, X_i =1\\}\\) (temps du \\(n+1\\)eme succès).\nOn admet l’existence d’un espace de probabilité \\((\\Omega, \\mathcal{F}, P)\\) où \\(\\Omega = \\{0,1\\}^{\\mathbb{N}}\\), \\(\\mathcal{F}\\) est une tribu pour laquelle les \\(X_i\\) sont mesurables, et \\(P\\) tel que \\(X_1, \\ldots, X_n, \\ldots\\) est une famille indépendante.\n\n\\(T_1\\) et plus généralement \\(T_n\\) sont-elles des variables aléatoires?\nCalculer \\(P \\{ T_1 &gt; k  \\}\\) pour \\(k \\in \\mathbb{N}\\).\nCalculer \\(P \\{ T_1 = k  \\}\\) pour \\(k \\in \\mathbb{N}\\)\nCalculer \\(\\mathbb{E}T_1\\).\nCalculer \\(P \\{ T_1 = k  \\wedge T_2 = k+j\\}\\) pour \\(k, j \\in \\mathbb{N}\\)\nCalculer \\(P \\{ T_2 = k  \\}\\) pour \\(k \\in \\mathbb{N}\\)\nCalculer \\(\\mathbb{E}T_2\\)\nCalculer \\(\\mathbb{E} T_n\\)\n\n\n\nExercice 5\nOn dispose de \\(n\\) urnes numérotées de \\(1\\) à \\(n\\) et de \\(n\\) boules. Les boules sont réparties de manière uniforme dans les urnes (chaque boule se comporte de manière indépendante des autres et a probabilité \\(1/n\\) de tomber dans chaque urne). On note \\(U_i\\) la variable aléatoire désignant le nombre de boules qui tombent dans l’urne \\(i\\). Soit \\(\\alpha &gt;1\\) un réel.\n\nDéterminer la loi de \\(U_i\\).\nMontrer que l’on a : \\[\\mathbb{P}( \\max_{1 \\leq i \\leq n} U_i &gt; \\alpha \\log n) \\leq n \\mathbb{P}( U_1 &gt; \\alpha \\log n).\\]\nCalculer \\(\\mathbb{E}(\\exp(U_1))\\).\nMontrer que pour tout \\(\\beta &gt; -n\\), on a \\((1+\\beta/n)^n \\leq \\exp(\\beta)\\).\nMontrer que \\(\\mathbb{P}(U_1 &gt; \\alpha \\log n) \\leq \\frac{\\exp(\\exp(\\alpha)-1)}{n^\\alpha}\\).\nEn déduire que si \\(\\alpha &gt;1\\), on a \\[\\mathbb{P}( \\max_{1 \\leq i \\leq n} U_i &gt; \\alpha \\log n) \\rightarrow_{n \\rightarrow \\infty} 0.\\]\n\n\n\nExercice 6 (Restitution Organisée de Connaissances)\n\nSoient \\(A, B, C\\) trois événements dans un espace probabilisé. A-t-on toujours: \\(A \\perp\\!\\!\\!\\perp B \\text{ et } B \\perp\\!\\!\\!\\perp C \\Rightarrow A \\perp\\!\\!\\!\\perp C\\)?\nSoient \\(P\\) et \\(Q\\) deux lois de probabilités sur \\((\\Omega, \\mathcal{F})\\), on définit l’ensemble \\(\\mathcal{M} = \\big\\{ A : A \\in \\mathcal{F}, P(A)=Q(A)\\}\\). Répondre par vrai/faux/je ne sais pas aux questions suivantes:\n\n\\(\\mathcal{M}\\) est-il toujours une classe monotone ?\n\\(\\mathcal{M}\\) est-il toujours une \\(\\sigma\\)-algèbre ?\n\\(\\mathcal{M}\\) est-il toujours une \\(\\pi\\)-classe ?\n\nSoient \\(G\\) et \\(F\\) sont deux fonctions génératrices de probabilité. Répondre par vrai/faux aux questions suivantes:\n\nEst-il vrai que \\(G \\times F\\) est toujours une fonction génératrice ?\nEst-il vrai que \\(G + F\\) est toujours une fonction génératrice de probabilité ?\nEst-il vrai que \\(\\lambda G + (1-\\lambda) F\\) avec \\(\\lambda \\in [0,1]\\) est toujours une fonction génératrice de probabilité ?\n\nSi \\(\\widehat{F}\\) est la fonction caractéristique de la loi de \\(X\\), et si \\(\\epsilon \\perp\\!\\!\\!\\perp X\\), avec \\(P\\{\\epsilon=1\\}= P\\{\\epsilon=-1\\}=1/2\\), quelle est la fonction caractéristique de la loi de \\(\\epsilon X\\)?\n\n\n\nExercice 7\nSi \\(X\\) est une variable aléatoire positive intégrable, la version biaisée par la taille de \\(X\\) est la variable aléatoire \\(X^*\\) dont la loi \\(Q\\) est absolument continue par rapport à celle de \\(X\\) (notée \\(P\\)) et dont la densité (par rapport à celle de \\(X\\)) est proportionnelle à \\(X\\): \\[\n\\frac{\\mathrm{d}Q}{\\mathrm{d}P}(x) = \\frac{x}{\\mathbb{E}X} \\, .\n\\]\n\nCaractériser \\(X^*\\) lorsque \\(X\\) est une Bernoulli.\nCaractériser \\(X^*\\) lorsque \\(X\\) est binomiale.\nCaractériser \\(X^*\\) lorsque \\(X\\) est Poisson.\nCaractériser \\(X^*\\) lorsque \\(X\\) est Gamma.\nSi \\(X\\) est à valeurs entières, exprimer la fonction génératrice de \\(X^*\\) en fonction de celle de \\(X\\).\nExprimer la transformée de Laplace de \\(X^*\\) en fonction de celle de \\(X\\).\nSi \\(U\\) est une transformée de Laplace, dérivable à droite en \\(0\\), \\(U'/U'(0)\\) est-elle la transformée de Laplace d’une loi sur \\([0, \\infty)\\)?\n\n\n\nExercice 8\n\n\n\n\n\n\nNoteRappel\n\n\n\nLa loi normale centrée réduite \\(\\mathcal{N}(0,1)\\) admet pour densité \\(x\\mapsto \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)\\),\n\n\n\nSi \\(X \\sim \\mathcal{N}(0,1)\\), donner une densité de la loi de \\(Y=\\exp(X)\\) (Loi log-normale). Calculer l’espérance et la variance de \\(Y\\).\nMême question si \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\).\nSi \\(X, Y \\sim \\mathcal{N}(0,1)\\), avec \\(X \\perp\\!\\!\\!\\perp Y\\), donner une densité de la loi de \\(Z=Y/X\\) (Loi de Student à 1 degré de liberté)\nSi \\(X, Y \\sim \\mathcal{N}(0,1)\\), avec \\(X \\perp\\!\\!\\!\\perp Y\\), donner une densité de la loi de \\(W = Y/ \\sqrt{X^2}\\).\nSi \\(X \\sim \\mathcal{N}(0,1)\\) et \\(\\epsilon\\) vaut \\(\\pm 1\\) avec probabilité \\(1/2\\) (variable de Rademacher) avec \\(X \\perp\\!\\!\\!\\perp \\epsilon\\), donner une densité de la loi de \\(Y = \\epsilon X\\). \\(Y\\) et \\(X\\) sont-elles indépendantes ?\n\n\n\nExercice 9\nPrincipe de réflexion\nDans cet exercice, \\(X_1, X_2, \\ldots\\) sont des variables de Rademacher indépendantes (\\(P\\{X_i = \\pm 1\\} = \\frac{1}{2}\\)), \\(S_n =\\sum_{i=1}^n X_i, S_0=0\\) et \\(M_n = \\max_{k \\leq n} S_n\\).\nMontrer que, pour \\(a&gt; 0\\),\n\\[P\\left\\{ M_n &gt; a \\right\\}\\leq 2 P\\left\\{ S_n &gt; a \\right\\}\\]\n\n\n\n\n\n\nNoteStatistique des rangs/Statistiques d’ordre\n\n\n\nLes statistiques d’ordre \\(X_{1:n}\\leq X_{2:n}\\leq X_{n:n}\\) d’un \\(n\\)-échantillon \\(X_1,\\ldots,X_n\\) d’observations indépendantes identiquement distribuées sont formées par le réarrangement croissant (convention) de l’échantillon.\nQuand \\(n\\) est clair d’après le contexte on peut les noter \\(X_{(1)} \\leq \\ldots \\leq X_{(n)}\\).\n\n\n\n\nExercice 10\n\nVérifier que la loi jointe des statistiques d’ordre est absolument continue par rapport à la loi de l’échantillon.\nOn suppose que \\(X\\) est une variable aléatoire réelle, absolument continue, de densité continue. Montrer que l’échantillon est presque sûrement formé de valeurs deux à deux distinctes.\nDonner la densité de la loi jointe des statistiques d’ordre.\nSi la loi des \\(X_i\\) définie par sa fonction de répartition \\(F\\), admet une densité \\(f\\), quelle est la densité de la loi de \\(X_{k:n}\\) pour \\(1\\leq k\\leq n\\) ?\n\nMontrer que conditionnellement à \\(X_{k:n}=x\\), la suite\n\\[(X_{i:n}-X_{k:n})_{i=k+1,\\ldots, n}\\]\nest distribuée comme les statistiques d’ordre d’un \\(n-k\\) échantillon de la loi d’excès au dessus de \\(x\\) (fonction de survie \\(\\overline{F}(x+\\cdot)/\\overline{F}(x))\\) avec la convention \\(\\overline{F}=1-F\\)).\nSi \\(X_1,\\ldots,X_n\\) est un échantillon i.i.d. de la loi exponentielle d’espérance \\(1\\) (densité \\(\\mathbb{I}_{x&gt;0} \\mathrm{e}^{-x}\\)), et $X_{n:n}X_{n-1:n}X_{1:n} $ les statistiques d’ordre associées, montrer que:\navec la convention \\(X_{0:n}=0\\), les écarts \\((X_{i:n}-X_{i-1:n})_{1\\leq i\\leq n}\\) () forment une collection de variables aléatoires indépendantes ;\n\\(X_{i:n}-X_{i-1:n}\\) est distribuée selon une loi exponentielle d’espérance \\(\\tfrac{1}{i}\\) .\nMaintenant, \\(X_1,\\ldots,X_n\\) est un échantillon i.i.d. de la loi exponentielle d’espérance \\(1\\) (densité \\(\\mathbb{I}_{x&gt;0} \\mathrm{e}^{-x}\\)), et \\(X_{n:n}\\geq X_{n-1:n}\\geq  \\ldots \\geq X_{1:n}\\) les statistiques d’ordre associées, et \\((k_n)_n\\) est une suite croissante d’entiers qui tend vers l’infini, telle que \\(k_n/n\\) tende vers une limite finie (éventuellement nulle). Montrer que\n\\[\\frac{X_{k_n:n} -\\mathbb{E} X_{k_n:n} }{\\sqrt{\\operatorname{var}(X_{k_n:n} )}}\\]\nconverge en loi vers une Gaussienne centrée réduite."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA15Y010 Probabilités et Extrêmes",
    "section": "",
    "text": "Cette page résume le déroulement du semestre\n\n\n\n\n\n\n\n\n\n\n\n\n\nSemaine\nDate\nThème(s)\nPréparer\nCours\nExercices\nÉvaluation\n\n\n\n\n1\n8 Sep 2025\nRévisions Licence\n\n\nTD 1\n\n\n\n\n9 Sep\nRévisions Licence\n\n\nTD 1\n\n\n\n\n11 Sep\nRévisions Licence: théoremes de convergence (intégration)\n\n\nTD 1\n\n\n\n2\n15 Sep\nRévisions Licence: Espaces roduits\n\n\n\n\n\n\n\n16 Sep\nChangement de variables. Calcul Gamma/Beta\n\n\n\n\n\n\n\n18 Sep\nFonctions génératrices/Conditionnement discret\n\n\n\n\n\n\n3\n22 Sep\nMoments, Espaces \\(L^p\\)\n\n\n\n\n\n\n\n23 Sep\nEspérance conditionnelle\n\n\n\n\n\n\n\n25 Sep\nConditionnement\n\n\nTD 2 TD 2 supplément\n\n\n\n4\n29 Sep\nCaractérisations\n\n\nTD4\n\n\n\n\n30 Sep\nCaractérisations\n\n\nCorrections TD1\n\n\n\n\n2 Oct\nCaractérisations\n\n\n\n\n\n\n5\n6 Oct\n\n\n\n\n\n\n\n\n7 Oct\nCC 1\n\n\n\n\n\n\n\n9 Oct\n\n\n\n\n\n\n\n6\n13 Oct\n\n\n\n\n\n\n\n\n14 Oct\n\n\n\n\n\n\n\n\n16 Oct\nCC 2",
    "crumbs": [
      "Informations",
      "Agenda"
    ]
  },
  {
    "objectID": "weeks/week-2.html",
    "href": "weeks/week-2.html",
    "title": "Semaine 2",
    "section": "",
    "text": "Important\n\n\n\nLa semaine II (15-19 septembre 2025) est encore consacrée aux révisions de Licence.",
    "crumbs": [
      "Agenda",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#préparerrevoir",
    "href": "weeks/week-2.html#préparerrevoir",
    "title": "Semaine 2",
    "section": "Préparer/Revoir",
    "text": "Préparer/Revoir\n\n\nEspaces produits, tribus produits (2025-09-15)\nLois produits (2025-09-15)\nThéorème de Fubini (2025-09-15)\nCalculs de densités image, formules de changement de variable (2025-09-16)\nEspérances, moments, Espaces \\(L^p\\) (2025-09-16)\nConditionnement discret (2025-09-18)\nProcessus de branchement aléatoire (2025-09-18)\nEnregistrement du cours du 18 septembre\n\n\n\n\nVoir Notes, Espaces produits, etc\nVoir Notes, calculs de densité image\nVoir Notes, Fonctions génératrices\nVoir Notes, Conditionnement discret\nVoir Notes, Espérances, Moments, Espaces \\(L^p\\), …",
    "crumbs": [
      "Agenda",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#exercices",
    "href": "weeks/week-2.html#exercices",
    "title": "Semaine 2",
    "section": "Exercices",
    "text": "Exercices\nFeuille TD I Feuille TD I suppléments\n\nTD I.18 (sommes aléatoires de variables indépendantes) (fin).\nSupplément TD I Exercice 5 (Anniversaires)\nTD I.7\nTD I….\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Agenda",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-4.html",
    "href": "weeks/week-4.html",
    "title": "Week 4",
    "section": "",
    "text": "Important\n\n\n\nLa semaine IV (29 septembre- 2 octobre 2025) est consacrée aux caractérisations des lois de probabilités (Transformées de Laplace et de Fourier).",
    "crumbs": [
      "Agenda",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#préparervoirrevoir",
    "href": "weeks/week-4.html#préparervoirrevoir",
    "title": "Week 4",
    "section": "Préparer/Voir/Revoir",
    "text": "Préparer/Voir/Revoir\n\n\nTransformée de Laplace\nTransformée de Fourier\n\n\n\n\nVoir Transformée de Laplace …\nVoir Transformée de Fourier …",
    "crumbs": [
      "Agenda",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#exercices",
    "href": "weeks/week-4.html#exercices",
    "title": "Week 4",
    "section": "Exercices",
    "text": "Exercices\n\nFeuille TD IV\nCorrections TD I\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Agenda",
      "Week 4"
    ]
  },
  {
    "objectID": "corriges/td1.html",
    "href": "corriges/td1.html",
    "title": "Variables aléatoires réelles",
    "section": "",
    "text": "NoteTD I : Révisions de Licence\n\n\n\n\n8 Septembre 2025-12 Septembre 2025\nMaster I Isifar\nProbabilités"
  },
  {
    "objectID": "corriges/td1.html#fonctions-de-répartition",
    "href": "corriges/td1.html#fonctions-de-répartition",
    "title": "Variables aléatoires réelles",
    "section": "Fonctions de répartition",
    "text": "Fonctions de répartition\n\nExercice 1 (transformation affine)\nSoit \\(X\\) une variable aléatoire réelle, \\(F_X\\) sa fonction de répartition, \\(a, b\\) deux réels fixés, et \\(Y := aX+b\\)\n\nOn suppose dans cette question que \\(a=1\\). Comment déduire \\(F_Y\\) de \\(F_X\\)?\nSi (la loi de) \\(X\\) admet une densité, en est-il de même de \\(Y\\)? Si oui, exprimer dans ce cas \\(f_Y\\) à l’aide de \\(f_X\\).\nOn suppose dans cette question que \\(b=0\\) et \\(a&gt;0\\). Comment déduire \\(F_Y\\) de \\(F_X\\)?\nSi (la loi de ) \\(X\\) admet une densité, à quelle condition sur \\(a\\) en est-il de même de (la loi de) \\(Y\\)? Exprimer dans ce cas \\(f_Y\\) à l’aide de \\(f_X\\).\nRépondre aux mêmes questions lorsque \\(b=0\\) et \\(a=-1\\)?\nRépondre enfin aux mêmes questions lorsque \\(a\\) et \\(b\\) sont quelconques.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\nSoit \\(F_Y\\) la fonction de répartition de \\(Y\\).\n\nOn a\n\\[F_Y(x) = \\mathbb{P}(Y \\le x) = \\mathbb{P}(X+b \\le x) = \\mathbb{P}(X \\le x-b) = F_X(x-b)\\]\nAinsi, le graphe de \\(F_Y\\) se déduit de celui de \\(F_X\\) en le translatant de \\(b\\) vers la droite. Si \\(X\\) admet une densité \\(f_X\\),\n\\[F_Y(x) = \\int_{-\\infty}^{x-b} f_X(u) du = \\int_{-\\infty}^b f_Y(v) dv,\\]\noù \\(f_Y(v) = f_X(v-b), u \\in \\mathbb{R}\\).\nAinsi, si \\(X\\) admet une densité \\(f_X\\), \\(Y\\) admet la densité \\(f_Y\\) définie ci-dessus.\nOn a dans ce cas \\[F_Y(x) = \\mathbb{P}(Y \\le x) = \\mathbb{P}(aX \\le x) = \\mathbb{P}(X \\le \\frac{x}{a}) = F_X(\\frac{x}{a}).\\] Ainsi le graphe de \\(F_Y\\) se déduit de celui de \\(F_X\\) en dilatant ce dernier par \\(a\\).\nSi \\(X\\) admet une densité \\(f_X\\), on a \\[F_Y(x) = \\int_{-\\infty}^{\\frac{x}{a}} f_X(u) du = \\int_{-\\infty}^x \\frac{1}{a} f_X(\\frac{v}{a}) dv = \\int_{-\\infty}^x f_Y(v) dv,\\] où \\(f_Y(v) = \\frac{1}{a} f_X(\\frac{v}{a}), v \\in \\mathbb{R}\\). Ainsi, si \\(X\\) admet une densité \\(f_X\\), alors \\(Y\\) admet la densité \\(f_Y\\) définie ci-dessus.\nDans ce cas \\[F_Y(x) = \\mathbb{P}(-X \\le x) = \\mathbb{P}(X \\ge -x) = 1-F_X((-x)^-).\\] Si \\(X\\) admet une densité \\(f_X\\), \\[F_Y(x) = \\int_{-x}^{\\infty} f_X(u) du = \\int_{-\\infty}^x f_X(-v) dv = \\int_{-\\infty}^x f_Y(v)dv,\\] où \\(f_Y(v) = f_X(-v), x \\in \\mathbb{R}\\). Ainsi, si \\(X\\) admet une densité \\(f_X\\), alors \\(Y\\) admet la densité \\(f_Y\\) définie ci-dessus.\nTraitons d’abord le cas \\(a=0\\). Dans ce cas, quelque soit \\(X\\), on obtient que \\(Y\\) est déterministe, \\(\\mathbb{P}(Y=b)=1\\) (en particulier \\(Y\\) n’a pas de densité même si \\(X\\) en possède une.\nLorsque \\(a &gt; 0\\), on obtient par un raisonnement similaire à 1,2, \\[F_Y(x) = F_X\\left(\\frac{x-b}{a}\\right),\\] et si \\(X\\) possède la densité \\(f_X\\), alors \\(Y\\) possède la densité \\(f_Y\\) telle que \\(f_Y(v) = \\frac{1}{a} f_X\\left( \\frac{v-b}{a} \\right), v \\in \\mathbb{R}\\).\nEnfin lorsque \\(a&lt;0\\), \\[F_Y(x) = \\mathbb{P}(aX+b \\le x) = \\mathbb{P}(aX \\le x-b) = \\mathbb{P}\\left(X \\ge \\frac{x-b}{a}\\right) = 1-F_X\\left(\\left(\\frac{x-b}{a}\\right)^-\\right),\\] et si \\(X\\) possède la densité \\(f_X\\), \\[F_Y(x) = \\int_{\\frac{x-b}{a}}^{+\\infty} f_X(u) du = \\frac{1}{|a|} \\int_{-\\infty}^{x} f_X\\left(\\frac{v-b}{a}\\right) dv,\\] de sorte que \\(Y\\) possède la densité \\(f_Y\\) où \\(f_Y(v) = \\frac{1}{|a|} f_X\\left(\\frac{v-b}{a}\\right), v \\in \\mathbb{R}\\).\n\n\n\n\n\nExercice 2 (Minimum, maximum de variables indépendantes)\nSoient \\(X_i, i \\ge 1\\), des variables indépendantes. Pour \\(k \\ge 2\\), on note \\(Y_k = \\min (X_1,...,X_k)\\), \\(Z_k = \\max(X_1,...,X_k).\\)\n\nDans cette question on s’intéresse à \\(k=2\\).\nComment déduire \\(F_{Y_2}\\) de \\(F_{X_1}, F_{X_2}\\)? Même question pour \\(F_{Z_2}\\).\nGénéraliser à \\(k\\) quelconque.\nQuelle est la loi de \\(Z_k\\) lorsque les \\(\\{X_i, i \\ge 1\\}\\) sont i.i.d., \\(\\sim \\mathrm{Unif}[0,1]\\)?\nQuelle est la loi de \\(Y_k\\) lorsque les \\(\\{X_i, i \\ge 1\\}\\) sont des variables exponentielles indépendantes, avec \\(X_i \\sim \\text{Exp}(\\lambda_i)\\) (où pour tout \\(i\\), \\(\\lambda_i &gt;0\\))?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nOn a pour tout \\(x \\in \\mathbb{R}\\), \\[1-F_{Y_2}(x) = \\mathbb{P}(Y_2 &gt; x) = \\mathbb{P}(\\min(X_1,X_2) &gt;x)) = \\mathbb{P}(X_1 &gt;x, X_2 &gt;x) = \\mathbb{P}(X_1&gt;x) \\mathbb{P}(X_2&gt;x),\\] où pour la dernière égalité on s’est servi de l’indépendance de \\(X_1\\) et \\(X_2\\). On déduit que pour tout \\(x \\in \\mathbb{R}\\), \\[1-F_{Y_2}(x) = (1-F_{X_1}(x)) (1-F_{X_2}(x)).\\]\n\nDe manière similaire, on obtient pour tout \\(x \\in \\mathbb{R}\\) \\[F_{Z_2}(x) = F_{Y_1}(x) F_{Y_2}(x).\\]\n\nPar un raisonnement similaire, pour tout \\(x \\in \\mathbb{R}\\),\n\\[ 1-f_{Y_k}(x) = \\prod_{i=1}^k (1-F_{X_i}(x))\\]\nPar ailleurs, pour tout \\(x \\in \\mathbb{R}\\), \\[F_{Z_k}(x) = \\mathbb{P}(\\max(X_1,...,X_k) \\le x) = \\mathbb{P}(X_i \\le x, 1 \\le i \\le k) = \\prod_{i=1}^k \\mathbb{P}(X_i \\le x)\\] où pour la dernière égalité on s’est servi de l’indépendance des \\((X_i, 1 \\le i \\le k)\\). On déduit, pour tout \\(x \\in \\mathbb{R}\\), \\[F_{Z_k}(x) = \\prod_{i=1}^k F_{X_i}(x)\\]\nDans ce cas \\(F_{X_i}(x) = x \\mathbb{I}_{[0,1]}(x) + \\mathbb{I}_{]1,+\\infty[}(x)\\), et donc\n\\[F_{Z_k}(x) = \\begin{cases} 0 & \\text{si } x \\le 0\\\\ x^k & \\text{si } 0 \\le x \\le 1 \\\\  1 & \\mbox{si } x \\ge 1 \\end{cases},\\]\non déduit donc que \\(Z_k\\) est une variable de densité \\[f_{Z_k}(x) = k x^{k-1} \\mathbf{1}_{[0,1]}(x), x \\in \\mathbb{R}.\\]\nDans ce cas \\(1-F_{X_i}(x) = \\exp(-\\lambda_i x) \\mathbf{1}_{\\{x \\ge 0\\}} + \\mathbf{1}_{\\{x&lt;0\\}}\\), on a donc\n\\[1-F_{Y_k}(x) = \\begin{cases} 1 & \\mbox{si } x \\le 0 \\\\ \\exp\\left(-x \\sum_{i=1}^k \\lambda_i\\right)  & \\text{sinon}\\end{cases}\\]\nde sorte que \\(Y_k \\sim \\exp(\\Lambda)\\), avec \\(\\Lambda = \\sum_{i=1}^k \\lambda_i\\).\n\n\n\n\n\nExercice 3\nOn suppose que \\(X \\sim \\mathcal{N}(0,1)\\). Que valent\n\\[\\mathbb{P}(X \\le 1), \\quad \\mathbb{P}(-1.23 \\le X \\le 0.43), \\quad \\mathbb{P}(X&gt;0.32)?\\]\n\n\n\n\n\n\nNoteSolution\n\n\n\nEn utillisant le logiciel R, la fonction de répartition de la loi \\(\\mathcal{N}(0,1)\\) est désignée par pnorm(), la fonction réciproque (fonction quantile) est désignée par qnorm\n\n\n\n\n\n\n\n\n\n\n\\(\\approx\\)\n\n\n\n\n\\(\\mathbb{P}(X \\le 1)\\)\npnorm(1)\n0.84\n\n\n\\(\\mathbb{P}(-1.23 \\le X \\le 0.43)\\)\npnorm(.43) - pnorm(-1.23)\n0.56\n\n\n\\(\\mathbb{P}(X &gt; .32 1)\\)\n1 - pnorm(.32)\n0.37\n\n\n\n\n\n\n\nExercice 4\nOn suppose que l’écart à la taille moyenne \\(T=15.5\\) des individus d’une population suit une loi normale centrée réduite.\nDans quel intervalle centré en \\(T\\) se situent les tailles de \\(99\\%\\) des individus de la population?\n\n\n\n\n\n\nNoteSolution\n\n\n\nD’après la table, pour \\(Z \\sim \\mathcal{N}(0,1)\\), on a \\(\\mathbb{P}(Z \\le 2.57) \\approx 0.9949),\\) et \\(\\mathbb{P}(Z \\le 2.58) \\approx 0.9951\\), de sorte que \\(\\mathbb{P}(|Z| \\ge 2.57) \\approx 0.0102\\), et \\(\\mathbb{P}(|Z| \\ge 2.58) \\approx 0.0098\\).\nOn déduit que les tailles de \\(99\\%\\) de la population se situent entre \\(15.5 - 2,58 = 12.92\\) et \\(15.5+2.58 = 18.08\\).\n\n\n\n\nExercice 5\nEtant donnée X une variable aléatoire gaussienne de paramètres \\(\\mu\\) et \\(\\sigma^2\\), donner la probabilité que \\(|X - \\mu|\\) dépasse \\(k\\sigma\\) pour \\(k = 1, 2, 3\\).\nSuggestion: On commencera par montrer que \\(\\sigma^{-1}(X-\\mu)\\) suit une loi normale centrée réduite.\nReprendre les questions de l’exercice précédent lorsque \\(\\mu=2, \\sigma=2\\).\nReprendre les questions de l’exercice précédent lorsque \\(\\mu=0, \\sigma=1/2\\).\n\n\n\n\n\n\nNoteSolution\n\n\n\nPuisque \\((X-\\mu)/\\sigma \\sim \\mathcal{N}(0,1)\\) on a\n\\[\\mathbb{P}(|X- \\mu| \\ge k \\sigma)= \\mathbb{P}(|Z| \\ge k),\\] et donc d’après la table, pour \\(k=1\\) ceci vaut\n\\[\\mathbb{P}(|Z| \\ge 1) =  2 (1-\\mathbb{P}(Z \\le 1)) \\approx 2 \\times (1-0.8413) = 2 * 0.1587 = 0.3174\\] pour \\(k=2\\),\n\\[\\mathbb{P}(|Z| \\ge 2) = 2(1-\\mathbb{P}(Z \\le 2) \\approx 2 \\times (1- 0.9772) = 0.0456.\\] enfin pour \\(k=3\\),\n\\[\\mathbb{P}(|Z| \\ge 3) = 2 (1- \\mathbb{P}(Z \\le 3) \\approx 2 \\times (1-0.9987) = 0.0026.\\]\nPour \\(\\mu=2, \\sigma=2\\), on a\n\\[\\mathbb{P}(X \\le 1) = \\mathbb{P}(Z \\le -1/2) \\approx 0.3085,\\]\n\\[ \\mathbb{P}(-1.23 \\le X \\le 0.43) = \\mathbb{P}(-\\frac{3.23}{2} \\le Z \\le -\\frac{1.57}{2}) \\approx 0.9463-0.7823 = 0.1640,\\]\n\\[\\mathbb{P}(X &gt; 0.32) = \\mathbb{P}(Z&gt;-0.84) \\approx 0.7995.\\]\nPour \\(\\mu=0, \\sigma=1/2\\), on trouve \\[\\mathbb{P}(X \\le 1) = \\mathbb{P}(Z \\le 2) \\approx 0.9772\\]\n\\[\\mathbb{P}(-1.23 \\le X \\le 0.43 ) = \\mathbb{P}(-2.46 \\le Z \\le 0.86) \\approx 0.8051 - (1-0.9931) = 0.7982,\\]\n\\[\\mathbb{P}(X&gt;0.32) = \\mathbb{P}(Z &gt; 0.64) \\approx 1-0.7389 = 0.2611\\]"
  },
  {
    "objectID": "corriges/td1.html#densités",
    "href": "corriges/td1.html#densités",
    "title": "Variables aléatoires réelles",
    "section": "Densités",
    "text": "Densités\n\nExercice 6\nDans les cas suivants, trouver la valeur de \\(C\\) pour que \\(f\\) soit une densité de probabilité.\n\n\\(f(x) = C \\frac{1}{\\sqrt{x (1-x)}}, 0 &lt;x &lt;1,\\)\n\\(f(x) = C \\exp(-x-\\exp(-x)), x \\in \\mathbb{R},\\)\n\\(f(x) = C \\frac{1}{1+x^2}\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nOn a \\[x(1-x) = x-x^2 = \\frac{1}{4} - \\left(x-\\frac{1}{2}\\right)^2\\] et on sait que la primitive de \\(x \\to \\frac{1}{\\sqrt{a^2-x^2}}\\) est \\(x \\to \\arcsin\\left(\\frac{x}{a}\\right)\\). On a donc en effectuant le changement de variable \\(y = x-1/2\\), \\[\\int_{0}^1 \\frac{1}{\\sqrt{x(1-x)}} = \\int_{-1/2}^{1/2} \\frac{dy}{\\sqrt{\\frac{1}{4}-y^2}} = \\left[\\arcsin(2y)\\right]_{-1/2}^{1/2} = \\pi,\\] et il faut donc poser \\(C = \\frac{1}{\\pi}\\) pour que \\(f\\) soit une densité de probabilité sur \\(]0,1[\\).\n\nSoit \\(h(x) = \\exp(-\\exp(-x)), x \\in \\mathbb{R}\\), on a pour \\(x \\in \\mathbb{R}\\), \\[h'(x) =\\exp(-x) \\exp(-\\exp(-x)) = \\exp(-x-\\exp(-x)),\\] de sorte que \\[ \\int_{-\\infty}^{\\infty} \\exp(-x-\\exp(-x)) = \\left[\\exp(-\\exp(-x))\\right]_{-\\infty}^{\\infty} = 1,\\] et il faut poser \\(C=1\\) pour que \\(f\\) soit une densité de probabilité sur \\(\\mathbb{R}\\).\n\nOn a \\[\\int_{\\infty}^{\\infty} \\frac{dx}{1+x^2} = \\left[ \\arctan(x) \\right]_{-\\infty}^{\\infty} = \\pi,\\] et il faut donc poser \\(C=\\frac{1}{\\pi}\\) pour que \\(f\\) soit une densité de probabilité sur \\(\\mathbb{R}\\).\n\n\n\n\n\nExercice 7\n(Mélange)\nOn suppose que \\(X\\) et \\(Y\\) sont deux variables de densités respectives \\(f_X, f_Y\\), et que \\(\\alpha \\in [0,1]\\). Montrer que \\(g : = \\alpha f_X + (1-\\alpha) f_Y\\) est également une densité de probabilité.\nTrouver une variable aléatoire dont \\(g\\) est la densité.\n\n\n\n\n\n\nNoteSolution\n\n\n\nLa fonction \\(g\\) reste bien évidemment borélienne, et puisque \\(\\alpha \\in [0,1]\\), positive, de plus\n\\[\\int_{\\mathbb{R}} g(x) dx = \\alpha \\int_{\\mathbb{R}} f_X(x) dx + (1-\\alpha) \\int_{\\mathbb{R}} f_Y(x) dx = \\alpha + (1-\\alpha) = 1,\\]\non conclut que \\(g\\) est bien une densité de probabilité.\nSoit \\(\\xi \\sim \\mathrm{Ber}(\\alpha)\\), indépendante de \\((X,Y)\\). Alors \\(Z= \\xi X + (1-\\xi)Y\\) possède la densité \\(g\\). En effet, en utilisant l’indépendance de \\(\\xi\\) et \\((X,Y)\\) à la deuxième ligne ci-dessous,\n\\[\\begin{align*}\n\\mathbb{P}(Z \\le x) & =  \\mathbb{P}(\\xi=1,  X \\le x) + \\mathbb{P}(\\xi=0, Y \\le x) \\\\ & =  \\mathbb{P}(\\xi=1) \\mathbb{P}(X \\le x) + \\mathbb{P}(\\xi=0) \\mathbb{P}(Y \\le x) \\\\ & =  \\alpha F_X(x) + (1-\\alpha) F_Y(x) \\\\ & =  \\int_{-\\infty}^x (\\alpha f_X(x) + (1-\\alpha)f_Y(x)) dx\n\\end{align*}\\]\n\n\n\n\nExercice 8\nSoit \\(X\\) de densité \\(f\\), et \\((\\alpha, \\beta) \\in \\overline{\\mathbb{R}}^2\\) sont supposés tels que \\[\\mathbb{P}(\\alpha &lt; X &lt; \\beta) =1\\] On suppose que \\(g\\) est un \\(C^{1}\\)-difféomorphisme croissant de \\((\\alpha, \\beta)\\) sur \\((g(\\alpha),g(\\beta))\\).\n\nMontrer que \\(g(X)\\) a pour densité \\(\\frac{f(g^{-1}(x))}{g'(g^{-1}(x))} \\mathbf{1}_{\\{x \\in (g(\\alpha), g(\\beta))\\}}\\).\nQuelle est la densité de la variable \\(aX+b\\), où \\(a&gt;0\\) et \\(b\\in \\mathbb{R}\\) sont fixés?\n\nSoit \\(Y \\sim \\mathcal{N}(0,1)\\). Quelle est la densité de \\(Z = \\exp(Y)\\)?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nPosons \\(T= g(X)\\). Pour \\(h : \\mathbb{R} \\to \\mathbb{R}_+\\) mesurable, on a \\[\\mathbb{E}[h(T)]  = \\mathbb{E}[h(g(X))] = \\int_{\\alpha}^{\\beta} h(g(x)) f(x) dx\\]\nEn effectuant le changement de variables \\(t = g(x)\\) il vient \\[ \\mathbb{E}[h(T)]  = \\int_{g(\\alpha)}^{g(\\beta)} \\frac{h(t) f(g^{-1}(t))}{g'(g^{-1}(t))} dt. \\] Comme ceci est valable pour tout fonction \\(h\\) mesurable positive, on conclut que \\(T\\) possède la densité souhaitée sur \\((g(\\alpha),g(\\beta))\\)\nIci \\(g : x \\to ax+b\\), \\(g^{-1} : x \\to \\frac{x-b}{a}\\), \\(g'(x) = a\\) pour tout \\(x\\) et dans ce cas la densité recherchée s’exprime donc \\[\\frac{1}{a} f\\left( \\frac{x-b}{a} \\right) \\mathbf{1}_{(a\\alpha+b, a\\beta+b)}(x), \\ x \\in \\mathbb{R}.\\] Quitte à prendre \\(\\alpha = -\\infty, \\beta = +\\infty\\), on retrouve le résultat de l’exercice 2.\nIci \\(\\alpha = - \\infty, \\beta=+\\infty\\), \\(f(y) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-y^2/2), y \\in \\mathbb{R}\\), et \\(g: x \\to \\exp(x)\\), \\(g^{-1} : y \\to \\ln(y)\\) et \\(g'(g^{-1}(x)) = x\\). On obtient donc que la densité de \\(Z\\) est \\[\\frac{1}{x \\sqrt{2\\pi}} \\exp\\left(-\\frac{\\ln(x)^2}{2}\\right) \\mathbf{1}_{\\{x &gt;0\\}}.\\]"
  },
  {
    "objectID": "corriges/td1.html#lois-usuelles-calculs-de-loi",
    "href": "corriges/td1.html#lois-usuelles-calculs-de-loi",
    "title": "Variables aléatoires réelles",
    "section": "Lois usuelles, calculs de loi",
    "text": "Lois usuelles, calculs de loi\n\nExercice 9\n(Fonctions de répartition et fonctions caractéristiques de lois usuelles)\n\n\n\n\n\n\nAttention : dans le cas d’une variable continue, quand on calcule \\(\\Phi_X\\) on %doit intégrer sur \\(\\mathbb{R}\\) une fonction complexe. Trois méthodes sont envisageables.\nParfois on peut intégrer séparément partie réelle et partie imaginaire.\nParfois il est utile de se servir de la formule de Cauchy. En particulier, cette formule assure que si \\(f\\) est une fonction holomorphe, si \\(\\mathcal{C}\\) est un contour fermé “raisonnable” (en particulier tout cercle ou polygone régulier), et enfin si \\(\\overset{\\circ}{\\mathcal{C}}\\) désigne l’ensemble des points se trouvant à l’intérieur de ce contour, alors\n\\[\\forall a \\in \\overset{\\circ}{\\mathcal{C}}, \\quad f(a) = \\frac{1}{2\\pi i} \\oint_{\\mathcal{C}}  \\frac{f(z)}{z-a} \\mathrm{d}z.\\]\nAttention : cette formule montre bien que l’on ne peut pas traiter l’intégrale d’une fonction complexe en faisant “comme si” \\(i\\) était réél (!) La méthode des résidus est en outre une conséquence directe de la formule de Cauchy.\nEnfin, on peut utiliser le prolongement analytique (voir l’exemple de la fonction \\(\\Gamma\\)).\n\n\n\n\nExprimer le plus simplement \\(F_X\\) dans les cas suivants (on pourra se contenter de tracer l’allure du graphe de la fonction de répartition lorsque celle-ci ne possède pas d’expression simple).\n\n\\(n \\in \\mathbb{N}^*, p \\in [0,1], X \\sim \\text{Bin}(n,p)\\),\n\\(\\lambda&gt;0, X \\sim \\text{Poisson}(\\lambda)\\),\n\\(a&gt;0, X\\sim \\text{Unif}[-a,a]\\),\n\\(\\lambda&gt;0, X \\sim \\mathbf{exp}(\\lambda)\\),\n\\(\\lambda&gt;0, s \\in \\mathbb{N}^*\\), \\(X \\sim \\Gamma(\\lambda, s)\\) (on rappelle que la densité \\(f_X\\) de \\(X \\sim \\Gamma(\\lambda,s)\\) est telle que \\(f_X(x) =\\frac{1}{\\Gamma(s)} \\lambda^s x^{s-1}\\exp(-\\lambda x) \\mathbf{1}_{[0,\\infty[}(x), x \\in \\mathbb{R}\\)),\n\\(X \\sim \\mathcal{N}(0,1)\\),\n\\(\\mu \\in \\mathbb{R}, \\sigma&gt;0, X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\).\n\n\\(a&gt;0, X \\sim \\mathrm{Cauchy}(a)\\) (on rappelle que la densité \\(f_X\\) de la loi de Cauchy de paramètre \\(a\\) est telle que \\(f_X(x) = \\frac{a}{\\pi(x^2+a^2)}, x\\in \\mathbb{R}\\)).\n\n(*) \\(X \\sim \\mathrm{stable}(1/2)\\) (cette loi a pour densité \\(\\sqrt{2\\pi x^{-3}} \\exp(-1/2x)\\mathbf{1}_{[0,\\infty[}(x).\\))\n\nLesquelles parmi ces variables possèdent une densité?\n\nExprimer le plus simplement \\(\\Phi_X\\) pour les \\(7\\) premières variables de la première question ci-dessus. En déduire, ou trouver par un calcul direct, \\(E[X],\\) et \\(\\mathrm{Var}[X]\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nBinômiale Il s’agit d’une variable discrète à valeurs dans \\([|0,n|]\\), (elle ne possède donc pas de densité), et telle que \\[\\mathbb{P}(X=k) = {n \\choose k} p^k (1-p)^{n-k}, \\ 0\\le k \\le n\\] On a\n\\[F_X(x) = \\sum_{k=0}^n \\mathbf{1}_{\\{x \\ge k\\}} {n \\choose k} p^k(1-p)^{n-k}, x \\in \\mathbb{R}.\\] \\(X\\) a même loi que \\(\\sum_{j=1}^n \\xi_j\\) où les \\((\\xi_j, 0 \\le j \\le n)\\) i.i.d suivant la loi Ber\\((p)\\), de sorte que \\[\\Phi_X(t) = \\mathbb{E}[\\exp(itX)] = \\Phi_{\\xi_1}(t)^n = (p\\exp(it)+(1-p))^n, \\ t \\in \\mathbb{R}\\] On a enfin, toujours gr^ace à l’écriture de \\(X\\) comme somme de \\(n\\) variables de Bernoulli indépendantes et de même paramètre \\(p\\), \\[\\mathbb{E}[X] = np , \\qquad \\mathrm{Var}[X] = np(1-p)\\]\nPoisson Il s’agit d’une variable discrète à valeurs dans \\(\\mathbb{N}\\) (elle ne possède donc pas de densité) et telle que \\[ \\mathbb{P}(X=k) = \\lambda^k \\frac{\\exp(-\\lambda)}{k!}, k \\in \\mathbb{N}.\\] On a \\[ F_X(x) = \\sum_{k \\in \\mathbb{N}} \\mathbf{1}_{\\{x \\ge k\\}} \\exp(-\\lambda)\\frac{\\lambda^k}{k!}, x \\in \\mathbb{R}\\] et \\[ \\Phi_X(t) = \\sum_{ k \\in \\mathbb{N}} \\exp(itk) \\exp(-\\lambda) \\frac{\\lambda^k}{k!} = \\exp(\\lambda \\exp(it) -1), t \\in \\mathbb{R}\\] On a par un calcul direct \\[ \\mathbb{E}[X] = \\mathrm{Var}[X] = \\lambda.\\]\nUniforme continue symétrique\nIl s’agit de la loi de densité \\(\\frac{1}{2a} \\mathbf{1}_{[-a,a]}\\). On a pour \\(x \\in \\mathbb{R}\\),\n\\[F_X(x) = \\begin{cases}  0 & \\mbox{si } x \\le -a \\\\\n  \\frac{1}{2a} (x+a) & \\mbox{si } x \\in [-a,a] \\\\\n  1 & \\mbox{si } x \\ge a \\end{cases}\\]\net pour \\(t \\in \\mathbb{R}\\),\n\\[\\Phi_X(t) = \\begin{cases} 1 & \\mbox{si } t =0 \\\\ \\frac{\\sin(ta)}{ta} & \\mbox{si } t \\ne 0.\\end{cases}\\]\nOn a par un calcul direct\n\\[\\mathbb{E}[X] = 0, \\mathrm{Var}[X] = \\frac{a^2}{3}\\]\nExponentielle\nIl s’agit de la loi de densité \\(x \\to \\lambda \\exp(-\\lambda x) \\mathbf{1}_{\\{x \\ge 0\\}}\\). On a pour \\(x \\in \\mathbb{R}\\),\n\\[F_X(x) =  \\begin{cases}  0 & \\mbox{si } x \\le 0 \\\\\n  1-\\exp(-\\lambda x) & \\mbox{si } x \\ge 0 \\end{cases}\\]\net\n\\[\\Phi_X(t) = \\frac{\\lambda}{\\lambda-it}, t\\ in \\mathbb{R}\\]\nOn a enfin par un calcul direct (i.p.p)\n\\[\\mathbb{E}[X] = \\frac{1}{\\lambda}, \\ \\ \\mathrm{Var}[X] = \\frac{1}{\\lambda^2}\\]\nGamma\nLa densité est rappelée en énoncé. La fonction de répartition n’admet pas en général de forme plus simple que \\(\\int_{-\\infty}^x f_X(u)du\\). On a de plus\n\\[\\Phi_X(t) = \\left(\\frac{\\lambda}{\\lambda-it} \\right)^s, t \\in \\mathbb{R}\\]\nQuitte à calculer la dérivée première et seconde en \\(0\\) de \\(\\Phi_X\\) on trouve\n\\[\\mathbb{E}[X] = \\frac{s}{\\lambda}, \\qquad \\mathrm{Var}[X] = \\frac{s}{\\lambda^2}.\\]\nGaussienne centrée réduite\nIl s’agit de la loi de densité \\(f_X : x \\to \\frac{1}{\\sqrt{2\\pi}} \\exp(-x^2/2)\\), la fonction \\(F_X\\) n’a pas de forme plus simple que \\(\\int_{-\\infty}^x f_X(u)du\\), et \\[ \\Phi_X(t) = \\exp(-t^2/2), \\ t\\in \\mathbb{R}\\] On a (soit par calcul direct, soit en dérivant \\(\\Phi_X\\)) \\[\\mathbb{E}[X] = 0, \\quad \\mathrm{Var}[X] =1\\]\nGaussienne\nSi \\(Z \\sim \\mathcal{N}(0,1)\\), on a \\(X = \\mu + \\sigma Z \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), et \\(X\\) a pour densité \\[f_X : x \\to \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-\\mu)^2}{2}\\right).\\] la fonction \\(F_X\\) n’a pas de forme plus simple que \\(\\int_{-\\infty}^x f_X(u)du\\) et \\[\\Phi_X(t) = \\exp\\left(it \\mu - \\frac{\\sigma^2 t^2}{2}\\right), t\\in \\mathbb{R}.\\] Puisque \\(X = \\mu + \\sigma Z\\) on trouve que \\[\\mathbb{E}[X] = \\mu, \\quad \\mathrm{Var}[X]= \\sigma^2.\\]\nCauchy\nLa densité est rappelée dans l’énoncé, on a pour \\(x \\in \\mathbb{R}\\), \\[F_X(x) = \\frac{1}{\\pi} \\left(\\frac{\\pi}{2}+ \\arctan\\left(\\frac{x}{a}\\right)\\right),\\] et \\[ \\Phi_X(t) = \\exp(-a|t|), t \\in \\mathbb{R}.\\]\nStable \\((1/2)\\)\nLa loi stable\\((1/2)\\) a la densité rappelée en énoncé (la fonction \\(F_X\\) n’a pas de forme plus simple que \\(\\int_{-\\infty}^x f_X(u)du\\)), sa fonction caractéristique est donnée par \\[\\Phi_X(t) = \\exp(-\\sqrt{|t|}(1+\\mathrm{sgn}(t)i), \\ t\\in \\mathbb{R}.\\]\net on note que \\(x \\to x f_X(x)\\) n’est pas intégrable (donc \\(\\mathbb{E}[X]= +\\infty\\), \\(\\mathrm{Var}[X]=+\\infty\\)).\n\n\n\n\n\nExercice 10\nPour des valeurs de \\(t\\) que l’on précisera, calculer la transformée de Laplace \\(L(t) := \\mathbb{E}[\\exp(-t X)]\\) et la fonction génératrice des moments \\(G(u) := \\mathbb{E}[u^X]\\) de la variable \\(X\\) dans les cas suivants.\n\n\\(X \\sim \\mathrm{Ber}(p)\\), où \\(p \\in [0,1]\\),\n\\(X \\sim \\mathrm{Bin}(n,p)\\), où \\(n \\in \\mathbb{N}^*, p \\in [0,1]\\),\n\\(X \\sim \\mathrm{Geom}(p)\\), où \\(p \\in [0,1]\\),\n\\(X \\sim \\mathrm{Poisson}(\\lambda)\\), où \\(\\lambda&gt;0\\),\n\n\\(X=Y_1+...+Y_n\\), où les \\(Y_i, 1 \\le i \\le n\\) sont des variables indépendantes, et \\(Y_i \\sim \\mathrm{Poisson}(\\lambda_i)\\), avec \\(\\lambda_i &gt;0\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\\(G(u) = 1-p + pu, u \\in \\mathbb{R}\\), \\(L(t) = (1-p) + p \\exp(-t) = G(\\exp(-t)), t \\in \\mathbb{R}\\).\n\\(G(u) = (1-p + pu)^n, u \\in \\mathbb{R}\\), \\(L(t)= G(\\exp(-t)), t \\in \\mathbb{R}\\)\n\\(G(u) = \\frac{up}{1-u(1-p)}, |u|&lt;\\frac{1}{1-p}\\), et \\(L(t) = G(\\exp(-t)),  t &gt; \\ln(1-p)\\).\n\\(G(u) = \\exp(\\lambda(u-1)), u \\in \\mathbb{R}\\), et \\(L(t) = G(\\exp(-t)), t \\in \\mathbb{R}\\)\n\\(G(u) = \\exp(\\Lambda(u-1)), u \\in \\mathbb{R}\\) où \\(\\Lambda = \\sum_{i=1}^n \\lambda_i\\), \\(L(t) = G(\\exp(-t)), t \\in \\mathbb{R}\\)\n\n\n\n\n\nExercice 11\nSoit \\(X\\) une v.a.r. de densité \\(f\\). Quelle est la densité de \\(X^2\\)? Qu’obtient-on dans le cas où \\(X \\sim \\mathcal{N}(0,1)\\)?\n\n\n\n\n\n\nNoteSolution\n\n\n\nSoit \\(\\phi : \\mathbb{R} \\to \\mathbb{R}\\) borélienne positive. et \\(Y=X^2\\). On a, en effectuant le changement de variables \\(u=x^2\\) à la quatrième ligne ci-dessous,\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(Y^2)] & =  \\mathbb{E}[\\phi(X^2)] \\\\ & =  \\int_{\\mathbb{R}} \\phi(x^2) f(x) dx \\\\ & =  \\int_{\\mathbb{R}_-}\\phi(x^2) f(x) dx + \\int_{\\mathbb{R}_+} \\phi(x^2) f(x) dx \\\\ & =  \\int_{0}^{\\infty} \\phi(u) f(-\\sqrt{u}) \\frac{du}{2\\sqrt{u}} + \\int_0^{\\infty} \\phi(u) f(\\sqrt{u})\\frac{du}{2\\sqrt{u}} \\\\  & =  \\int_0^{\\infty} \\phi(u) \\frac{f(-\\sqrt{u}) + f(\\sqrt{u})}{2\\sqrt{u}} du\n\\end{align*}\\]\nComme l’égalité ci-dessus est valable pour toute \\(\\phi : \\mathbb{R} \\to \\mathbb{R}\\) borélienne positive, on conclut que \\(Y\\) possède la densité\n\\[u\\to f_Y(u)  = \\frac{f(-\\sqrt{u}) + f(\\sqrt{u})}{2\\sqrt{u}} \\mathbf{1}_{\\{u&gt;0\\}}.\\]\nDans le cas où \\(X \\sim \\mathcal{N}(0,1)\\), on obtient\n\\[f_Y(u) = \\frac{1}{\\sqrt{2\\pi}} \\frac{\\exp\\left(-\\frac{u}{2}\\right)}{\\sqrt{u}} \\mathbf{1}_{\\{u &gt; 0\\}},\\]\nde sorte que \\(Y \\sim \\Gamma(1/2,1/2)\\).\n\n\n\n\nExercice 12\nSoit \\(X \\sim \\exp(1)\\). Calculer la densité des variables suivantes :\n\n\\(Y = aX+b\\), où \\(a&gt;0\\) et \\(b \\in \\mathbb{R}\\). Qu’observe-t-on dans le cas où \\(b=0\\)?\n\n\\(Z = X^2\\).\n\\(U = \\exp(-X)\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nOn a d’après l’exercice 2\n\\[f_Y(u) = \\frac{1}{a} \\exp\\left(- \\frac{u-b}{a}\\right)\\mathbf{1}_{\\{u \\ge 0\\}}, u \\in \\mathbb{R} \\]\nLorsque \\(b =0\\), on constate que \\(Y \\sim \\exp\\left(\\frac{1}{a}\\right)\\)\nD’après l’exercice précédent\n\\[f_Z(u) = \\frac{\\exp(- \\sqrt{u})}{\\sqrt{u}} \\mathbf{1}_{\\{u&gt;0\\}}, u \\in \\mathbb{R}.\\]\nSoit \\(\\phi : \\mathbb{R} \\to \\mathbb{R}_+\\) borélienne, on a, en effectuant le changement de variables \\(u=\\exp(-x)\\) à la troisième ligne ci-dessous\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(U)]\n& = \\mathbb{E}[\\phi(\\exp(-X)] \\\\\n& =  \\int_0^{\\infty} \\phi(\\exp(-x)) \\exp(-x) dx \\\\\n& =  \\int_{0}^1 \\phi(u) du,\n\\end{align*}\\]\net, comme cette égalité est valable pour tout \\(\\phi : \\mathbb{R} \\to \\mathbb{R}_+\\) borélienne, on conclut que \\(U \\sim \\mathrm{Unif}[0,1]\\).\n\n\n\n\n\nExercice 13\nTrouver la loi de \\(\\arcsin(X)\\) lorsque\n\n\\(X \\sim \\mathrm{Unif}[0,1]\\),\n\\(X \\sim \\mathrm{Unif}[-1,1]\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nSoit \\(\\phi : \\mathbb{R} \\to \\mathbb{R}_+\\) borélienne, on a, en effectuant le changement de variables \\(u=\\arcsin(x)\\) à la deuxième ligne ci-dessous\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(\\arcsin(X))]\n& =  \\int_0^1 \\phi(\\arcsin(x)) dx  \\\\\n& =  \\int_0^{\\pi/2} \\phi(u) \\cos(u) du.\n\\end{align*}\\]\n\nOn conclut que \\(Y=\\arcsin(X)\\) possède la densité \\(u \\to f_Y(u) = \\cos(u) \\mathbf{1}_{[0,\\pi/2]}(u)\\).\n\nSoit \\(\\phi : \\mathbb{R} \\to \\mathbb{R}_+\\) borélienne, on a, en effectuant le changement de variables \\(u=\\arcsin(x)\\) à la deuxième ligne ci-dessous\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(\\arcsin(X))]\n& =  \\frac{1}{2} \\int_{-1}^1 \\phi(\\arcsin(x)) dx  \\\\\n& =  \\frac{1}{2} \\int_{-\\pi/2}^{\\pi/2} \\phi(u) \\cos(u) du.    \n\\end{align*}\\]\nOn conclut que \\(Y=\\arcsin(X)\\) possède la densité \\(u \\to f_Y(u) = \\frac{\\cos(u)}{2} \\mathbf{1}_{[-\\pi/2,\\pi/2]}(u)\\).\n\n\n\n\n\nExercice 14\nOn souhaite peindre un mur (infini!) en utilisant un arroseur automatique qui effectue des demi-révolutions successives. Pour simplifier le modèle, on représentera le mur par une droite verticale \\(\\Delta\\), et l’arroseur par une source ponctuelle \\(O\\) située à \\(1\\) mètre du mur, et émettant en tout instant \\(t\\) de façon parfaitement rectiligne dans la direction \\(\\overset{\\rightarrow}{u}(t)\\). On note \\(M\\) la projection orthogonale de \\(O\\) sur \\(\\Delta\\) et \\(\\theta(t)\\) l’angle entre \\(O \\overset{\\rightarrow}{u}(t)\\) et \\(\\overset{\\rightarrow}{OM}\\). L’intersection de \\(O\\overset{\\rightarrow}{u}\\) avec \\(\\Delta\\) est notée \\(H(\\theta)\\).\nOn suppose en outre que lors d’une demi-révolution, \\(\\theta(t)\\) parcourt exactement l’intervalle \\((-\\pi/2,\\pi/2)\\).\nOn fait l’hypothèse que la demi-révolution s’effectue à vitesse angulaire constante, et on se demande quelle sera la répartition de l’épaisseur de la couche de peinture le long de \\(\\Delta\\) après un nombre entier de demi-révolutions.\n\nJustifier qu’une particule de peinture choisie uniformément au hasard parmi toutes les particules est envoyée suivant un angle \\(\\theta \\sim \\mathrm{Unif}(-\\pi/2,\\pi/2)\\). Une telle particule se pose alors en \\(H(\\theta)\\), On note \\(h(\\theta)\\) l’ordonnée de \\(H(\\theta)\\) (c’est également la distance algébrique entre \\(O\\) et \\(H\\)).\nExprimer \\(h(\\theta)\\) en fonction de \\(\\theta\\). Quelle est la loi de \\(h(\\theta)\\)? Que pouvez-vous en déduire sur la distribution de l’épaisseur de la couche de peinture le long du mur?\nA posteriori, quelle critique peut-on formuler sur le modèle?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nLa vitesse angulaire étant constante, et le nombre de révolutions étant entier, aucune direction dans \\(]-\\pi/2,\\pi/2[\\) n’est privilégiée. Plus précisément, si \\(-\\pi/2 &lt; \\theta_0 &lt; \\theta_0 + \\theta_1 &lt; \\pi/2\\), le nombre de particules envoyées lors des \\(n\\) révolutions dans un secteur angulaire \\((\\theta_0, \\theta_0+\\theta_1)\\) ne dépend pas de \\(\\theta_0\\) et est proportionnel à \\(\\theta_1\\).\nAinsi, lorsqu’on choisit une des particules de peinture envoyées uniformément au hasard, la probabilité qu’elle ait été envoyée dans ce secteur angulaire est proportionnelle à \\(\\theta_1\\),, elle vaut donc \\(\\frac{2}{\\pi} \\theta_1\\) (en effet la probabilité est \\(1\\) pour \\(\\theta_0=-\\pi/2, \\theta_1=\\pi\\). Toujours pour \\(-\\pi/2&lt; \\theta_0&lt; \\theta_0 +\\theta_1&lt;\\pi/2\\), \\(\\frac{1}{\\pi} \\theta_1=\\int_{\\theta_0}^{\\theta_0+\\theta_1} \\frac{1}{\\pi} dx\\), et comme ceci est valable pour tous \\(-\\pi/2&lt; \\theta_0&lt; \\theta_0 +\\theta_1&lt;\\pi/2\\), cela caractérise la loi de l’angle, et on déduit que celui-ci suit bien une loi \\(\\mathrm{Unif}]-\\pi/2,\\pi/2[\\).\nOn a \\(h(\\theta) = \\tan(\\theta)\\), et donc si on pose \\(X = h(\\theta)\\) et si \\(\\phi : \\mathbb{R} \\to \\mathbb{R}_+\\) est borélienne on a (on effectue le changement de variables \\(x=\\tan(\\theta)\\) à la troisième ligne ci-dessous)\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(X)]\n& = \\mathbb{E}[\\phi(\\tan(\\theta))] \\\\\n& = \\int_{-\\pi/2}^{\\pi/2} \\frac{2}{\\pi} \\phi(\\tan(\\theta)) d\\theta \\\\\n& = \\int_{-\\infty}^{\\infty} \\frac{1}{\\pi} \\frac{\\phi(x)}{1+x^2} dx  \n\\end{align*}\\]\nde sorte que \\(X\\) a pour densité \\(f_X: x \\to \\frac{1}{\\pi(1+x^2)}\\).\nOn a \\(\\mathbb{E}[|X|]=+\\infty\\), autrement dit, la distance moyenne à l’axe des abcisses à laquelle une particule choisie uniformément atterit est … infinie.\n\n\n\n\n\nExercice 15\nSoit \\(X \\sim \\mathrm{Cauchy}\\) (de paramètre \\(1\\)).\nQuelle est la loi de\n\n\\(Y:=\\frac{1}{X}\\)?\n\\(Z:= \\frac{1}{1+X^2}\\)?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nComme \\(\\mathbb{P}(X=0)=0\\), la variable \\(Y\\) est bien définie.\nSi \\(\\phi : \\mathbb{R} \\to \\mathbb{R}_+\\) est borélienne on a (on effectue le changement de variables \\(y=\\frac{1}{x}\\) à la toisième ligne ci-dessous)\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(Y)]\n& =  \\mathbb{E}[\\phi(\\frac{1}{X})] \\\\\n& =  \\int_{-\\infty}^{0} \\frac{\\phi(1/x)}{\\pi(1+x^2)} dx  + \\int_{0}^{\\infty} \\frac{\\phi(1/x)}{\\pi(1+x^2)} dx \\\\\n& = \\int_{-\\infty}^0 \\frac{\\phi(y)}{\\pi y^2(1+\\frac{1}{y^2})} dy + \\int_0^{\\infty} \\frac{\\phi(y)}{\\pi y^2(1+\\frac{1}{y^2})} dy \\\\  \n& =  \\int_{-\\infty}^{\\infty}  \\frac{\\phi(y)}{\\pi (1+y^2)} dy\n\\end{align*}\\]\nde sorte que \\(Y \\sim \\mathrm{Cauchy}(1)\\).\nSi \\(\\phi : \\mathbb{R} \\to \\mathbb{R}_+\\) est borélienne on a (on effectue le changement de variables \\(z=\\frac{1}{1+x^2}\\) à la toisième ligne ci-dessous)\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(Z)]\n& =  \\mathbb{E}[\\phi(\\frac{1}{1+X^2})] \\\\\n& =  \\int_{-\\infty}^{0} \\frac{\\phi(\\frac{1}{1+x^2})}{\\pi(1+x^2)} dx  + \\int_{0}^{\\infty} \\frac{\\phi(\\frac{1}{1+x^2})}{\\pi(1+x^2)} dx \\\\\n& =  2 \\int_{0}^1 \\frac{z \\phi(z)}{\\pi} \\frac{1}{2 z^2\\sqrt{\\frac{1}{z}-1}}  \\\\\n& =  \\int_{0}^{1} \\frac{\\phi(z)}{\\pi\\sqrt{z(1-z)}}\n\\end{align*}\\]\nde sorte que \\(Z \\sim \\mathrm{Beta}(1/2,1/2)\\).\n\n\n\n\n\nExercice 16\nSoit \\(Z \\sim \\mathcal{N}(0,1)\\). Montrer que pour tout \\(x&gt;0\\),\n\\[\\left(x^{-1}-x^{-3}\\right) \\exp(-x^2/2) \\le \\sqrt{2\\pi} \\mathbb{P}(Z&gt;x) \\le x^{-1} \\exp(-x^2/2).\\]\nIndication : on pourra penser à utiliser le changement de variable \\(y=x+z\\) pour obtenir l’inégalité de droite, et on commencera par calculer la dérivée de \\(\\left(x^{-1}-x^{-3}\\right) \\exp(-x^2/2)\\) pour obtenir celle de gauche.\n\n\n\n\n\n\nNoteSolution\n\n\n\nOn a pour \\(x &gt;0\\), en effectuant le changement de variables \\(y=x+z\\) suggéré dans l’énoncé\n\\[\\begin{align*}\n\\sqrt{2\\pi} \\mathbb{P}(Z&gt;x) & =  \\int_{x}^{\\infty} \\exp(-y^2/2) dy\n\\\\ & =  \\int_0^{\\infty} \\exp(-x^2/2-xz-z^2/2) \\mathrm{d}z\n\\\\ & =  \\exp(-x^2/2) \\int_{\\mathbb{R}_+} \\exp(-zx - z^2/2) \\mathrm{d}z\n\\\\ & \\le  \\exp(-x^2/2) \\int_{\\mathbb{R}_+} \\exp(-zx) \\mathrm{d}z = x^{-1} \\exp(-x^2/2)\n\\end{align*}\\]\nPar ailleurs, si \\(g : x \\to (x^{-1}-x^{-3})(\\exp(-x^2/2)\\), on a pour \\(x&gt;0\\)\n\\[\\begin{align*}\ng'(x) & =  \\left(-\\frac{1}{x^2}-\\frac{3}{x^4} -x (x^{-1}-x^{-3}) \\right)\\exp(-x^2/2)   \\\\\n& = \\left( -1 -\\frac{3}{x^4} \\right) \\exp(-x^2/2)\n\\end{align*}\\]\nPar ailleurs, si \\(h : x \\to \\sqrt{2\\pi} \\mathbb{P}(Z&gt;x)\\), on a \\(h'(x) = - \\exp(-x^2/2)\\) de sorte que \\(g'(x) &lt;h'(x),\\) pour tout \\(x &gt;0\\).\nComme \\(g(x) \\to -\\infty\\) lorsque \\(x \\searrow 0\\), alors que \\(h(0) = \\sqrt{\\pi/2}\\), on déduit, comme souhaité, que\n\\[g(x)  \\le h(x), \\ \\forall x &gt;0 \\]\n\n\n\n\nExercice 17 (calcul d’une loi conditionnelle discrète)\nSoient \\(X_1,...,X_n\\) des variables de Poisson, indépendantes, de paramètres respectifs \\(\\lambda_1,...,\\lambda_n\\).\n\nDéterminer la loi de \\(Y:=\\sum_{k=1}^n X_k\\).\nPour \\(r \\in \\mathbb{N}\\), que vaut la loi conditionnelle de \\((X_1,...,X_n)\\) sachant \\(Y=r\\)?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nOn a pour tout \\(t \\in \\mathbb{R}\\), en utilisant l’indépendance des \\((X_k, 1 \\le k \\le n)\\) à la première ligne ci-dessous,\n\n\\[\\begin{align*}\n\\Phi_Y(t) & =  \\prod_{k=1}^n \\Phi_{X_k}(t) \\\\ & =  \\prod_{k=1}^n \\exp(\\lambda_k (\\exp(it)-1)) \\\\ & =  \n\\exp(\\Lambda (\\exp(it)-1)\n\\end{align*}\\]\noù \\(\\Lambda = \\sum_{k=1}^n \\lambda_k\\). La fonction caractéristique caractérisant la loi, on déduit que \\(Y \\sim \\mathrm{Poisson}(\\Lambda)\\). 1. Soient \\((\\ell_1,\\ell_2, \\dots,\\ell_n) \\in \\mathbb{N}^n\\). Si \\(\\sum_{k=1}^n \\ell_k \\ne r\\) on a bien sûr \\(\\{(X_1,...,X_n)= (\\ell_1,... \\ell_n)\\} \\cap \\{Y=r\\} = \\emptyset\\) donc \\[ \\mathbb{P}((X_1,X_2, \\dots ,X_n) = (\\ell_1, \\ell_2, \\dots, \\ell_n)  \\mid Y=r) = 0\\] Si \\((\\ell_1,\\ell_2, \\dots, \\ell_n) \\in \\mathbb{N}^n\\) sont tels que \\(\\sum_{k=1}^n \\ell_k = n\\), on a \\(\\{Y=r\\} \\supset \\{(X_1,X_2, \\dots,X_n)= (\\ell_1, \\ell_2, \\dots, \\ell_n)\\}\\) et donc en utilisant l’indépendance des $(X_k)_{1 k n} $ à la deuxième ligne ci-dessous\n\\[\\begin{align*}\n\\mathbb{P}((X_1, X_2, \\dots,X_n) = (\\ell_1,\\ell_2, \\dots, \\ell_n) \\mid Y =r) & =  \\frac{\\mathbb{P}(X_1=\\ell_1, X_2= \\ell_2, \\dots, X_n = \\ell_r)}{\\mathbb{P}(Y=r)} \\\\ & =  \n\\frac{\\prod_{k=1}^n \\frac{\\lambda_k^{\\ell_k}\\exp(-\\lambda_k)}{\\ell_k!}}{\\frac{\\exp(-\\Lambda) \\Lambda^r}{r!} }\n\\\\ & =  {r \\choose \\ell_1, \\ell_2, \\dots,\\ell_n} \\prod_{k=1}^n \\left(\\frac{\\lambda_k}{\\Lambda}\\right)^{\\ell_k},\n\\end{align*}\\]\navec \\({r \\choose \\ell_1, \\ell_2, \\dots, \\ell_n} =\\frac{r!}{\\ell_1! \\ell_2! \\dots \\ell_n!}\\) le coefficient multinômial de \\((\\ell_1,...,\\ell_n)\\) parmi \\(r\\).\nOn conclut que la loi conditionnelle de \\((X_1,...,X_n)\\) sachant \\(\\{Y=r\\}\\) est une loi mutinômiale de paramètres \\(\\left(r,\\frac{\\lambda_1}{\\Lambda}, \\frac{\\lambda_2}{\\Lambda}, \\dots, \\frac{\\lambda_n}{\\Lambda}\\right)\\)."
  },
  {
    "objectID": "corriges/td1.html#exemples-divers",
    "href": "corriges/td1.html#exemples-divers",
    "title": "Variables aléatoires réelles",
    "section": "Exemples divers",
    "text": "Exemples divers\n\nExercice 18\nOn suppose que le nombre \\(X\\) d’oeufs pondus par un insecte suit une loi de Poisson de paramètre \\(\\lambda&gt;0\\) et que la probabilité qu’un oeuf meurt sans éclore est, indépendamment des autres oeufs, égale à \\(1-p\\), où \\(p \\in ]0,1[\\).\n\nDémontrer que le nombre \\(Y\\) d’oeufs qui arrivent à éclosion suit une loi de Poisson de paramètre \\(\\lambda p\\).\nQuelle est la loi jointe de \\((Y,Z)\\), où \\(Z= X-Y\\) est le nombre d’oeufs morts avant éclosion?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\nQuitte à introduire des variables \\((\\xi_i, i \\ge 1)\\) indépendantes de \\(X\\), et i.i.d suivant la loi de Bernoulli de paramètre \\(p\\) (\\(\\xi_i=1\\) si le \\(i\\)-ième oeuf arrive à éclosion),\n\\[Y = \\sum_{k=1}^X \\xi_i, \\quad Z = \\sum_{k=1}^X (1-\\xi_i),\\]\noù par convention \\(\\sum_{k=1}^0 \\dots = 0\\).\nOn a donc pour \\((s,t) \\in \\mathbb{R}^2\\), en utilisant l’indépendance de \\(X\\) et des \\((\\xi_i, i \\ge 1)\\) à la quatrième ligne ci-dessous, puis le fait que les \\((\\xi_i, i \\ge 1)\\) sont i.i.d suivant une loi Ber\\((p)\\) à la suivante :\n\\[\\begin{align*}\n\\Phi_{(Y,Z)}(s,t) & =  \\mathbb{E}[\\exp(isY + itZ)] \\\\\n& =  \\mathbb{E}\\left[\\exp\\left(is \\sum_{k=1}^X \\xi_k + it \\sum_{k=1}^X (1-\\xi_k)\\right)\\right]\n\\\\ & =  \\sum_{j \\in \\mathbb{N}} \\mathbb{E}\\left[\\mathbf{1}_{\\{X= j\\}} \\exp\\left(is \\sum_{k=1}^j \\xi_k + it \\sum_{k=1}^j (1-\\xi_k)\\right)\\right]\n\\\\ & =  \\sum_{j \\in \\mathbb{N}} \\frac{\\lambda^j \\exp(-\\lambda)}{j!} \\mathbb{E}\\left[\\prod_{k=1}^j  \\exp(is \\xi_k + it(1-\\xi_k))\\right]\n\\\\ & =  \\sum_{j \\in \\mathbb{N}} \\frac{\\lambda^j \\exp(-\\lambda)}{j!}  \\bigg(p\\exp(is) + (1-p)\\exp(it)\\bigg)^j\n\\\\ & =  \\exp(-\\lambda + \\lambda p \\exp(is) + \\lambda(1-p)\\exp(it))\n\\\\ & =  \\exp(\\lambda p (\\exp(is)-1)) \\exp(\\lambda(1-p) (\\exp(it)-1))\n\\end{align*}\\]\nOn déduit que \\((Y,Z)\\) est un couple de variables de Poisson indépendantes, de paramètres respectifs \\(\\lambda p, \\lambda(1-p)\\).\n\n\n\n\nExercice 19\n(Somme d’exponentielles et \\(\\chi^2\\))\nSoit \\(\\lambda&gt;0\\) et \\((Y_i)_{1 \\le i \\le n}\\) des variables i.i.d, \\(\\sim \\exp(\\lambda)\\).\n\nCalculer \\(\\mathbb{E}[\\exp(-tY_1)]\\) pour \\(t \\ge 0\\).\n\nCalculer \\(\\mathbb{E}\\left[\\exp(-t\\sum_{i=1}^n) Y_i \\right]\\) pour \\(t \\ge 0\\).\nMontrer que la densité de la variable \\(X= \\sum_{i=1}^n Y_i\\) est proportionnelle à \\(x^{n-1} \\exp(-\\lambda x) \\mathbf{1}_{x \\ge 0}\\). En déduire la valeur de cette densité.\nSoient \\(X_n, n \\ge 1\\) des variables i.i.d, \\(\\sim \\mathcal{N}(0,1)\\). On pose \\(\\chi^2(n):=\\sum_{i=1}^n X_i^2\\), \\(Z:=X_1X_2 +X_3X_4\\). \\ Calculer \\(\\Phi_{\\chi^2(n)}, \\Phi_Z\\). Pouvez-vous deviner la distribution de \\(Z\\) (on pourra utiliser un résultat d’un exercice précédent)?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\\(\\mathbb{E}[\\exp(-tY_1)] = \\frac{\\lambda}{\\lambda+t}, t \\ge 0\\)\n\\(\\mathbb{E}\\left[\\exp(-t\\sum_{i=1}^n) Y_i \\right] = \\left(\\frac{\\lambda}{\\lambda+t}\\right)^n, \\ t \\ge 0\\).\nOn montre cette assertion par récurrence sur \\(n \\in \\mathbb{N}^*\\). L’assertion est évidente pour \\(n=1\\) (avec facteur de proportionalité \\(c_1:=\\lambda\\)). Supposons la vraie au rang \\(n\\) et posons \\(Z_{n+1} = \\sum_{i=1}^{n+1} Y_i\\). Comme \\(Y_{n+1}\\) est indépendante de \\(Z_n\\) le vecteur \\((Z_n, Y_{n+1})\\) possède la densité jointe sur \\(\\mathbb{R}^2\\) : \\[ f_{(Z_n, Y_{n+1})}(z,y) = f_{Z_n}(z) f_{Y_{n+1}}(y) = c_n  \\lambda z^{n-1} \\exp(-\\lambda z - \\lambda y) \\mathbf{1}_{\\{z \\ge 0, y \\ge 0\\}}, (z,y) \\in \\mathbb{R}^2\\] Soit \\(\\phi : \\mathbb{R} \\to \\mathbb{R}\\) borélienne positive\n\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(Z_{n+1})] & =  \\mathbb{E}[\\phi(Z_n + Y_{n+1})]\n\\\\ & =  \\int_{\\mathbb{R}_+^2} \\phi(z+y) f_{Z_n}(z) f_{Y_{n+1}}(y) \\mathrm{d}z dy\n\\\\ & =  \\int_{\\mathbb{R}_+^2} \\phi(z+y)     c_n  \\lambda z^{n-1} \\exp(-\\lambda z - \\lambda y)  \\mathrm{d}z dy\n\\\\ & =  \\int_{u \\in \\mathbb{R}_+, u \\ge v}  \\phi(u) c_n \\lambda v^{n-1} \\exp(-\\lambda u) du dv\n\\end{align*}\\]\noù à la dernière ligne on a utilisé le changement de variable \\((u,v) = (z+y,z)\\) de \\(\\mathbb{R}_+^2\\) dans \\(\\{(u,v) \\in \\mathbb{R}_+^2 : v \\le u\\}\\), de jacobien \\(1\\). On a donc\n\\[\\begin{align*}\n  \\mathbb{E}[\\phi(Z_{n+1})] & =   \\int_{u \\in \\mathbb{R}_+} \\lambda c_n \\phi(u)  \\exp(-\\lambda u) \\int_{v =0}^u  v^{n-1} dv  du \\\\\n& =  \\int_{\\mathbb{R}_+} \\phi(u) \\lambda \\frac{c_n}{n} u^n \\exp(-\\lambda u) du\n\\end{align*}\\]\net on obtient la conclusion souhaitée, avec \\(c_{n+1} = \\frac{\\lambda c_n}{n}\\).\nOn déduit par une récurrence immédiate que \\(c_n = \\frac{\\lambda^n}{(n-1)!}, n \\in \\mathbb{N}^*\\).\n\nOn a vu (exercice 12) que \\(X_i^2 \\sim \\mathrm{Gamma}(1/2,1/2)\\), et par un raisonnement similaire à celui de la question précédente, on obtient que \\(\\chi^2(n) \\sim \\mathrm{Gamma}(n/2,1/2)\\). On peut aussi raisonner directement avec les fonctions caractéristiques, pour obtenir que \\[ \\Phi_{\\chi^2(n)}(t) = \\Phi_{X_1^2}(t)^n = \\left(\\frac{1/2}{1/2-it}\\right)^{n/2}.\\] Par ailleurs pour \\(t \\in \\mathbb{R}\\),\n\n\\[\\begin{align*}\n\\Phi_{X_1X_2}(t) & =  \\mathbb{E}[\\exp(i t X_1 X_2)] = \\int_{\\mathbb{R}^2} \\frac{1}{2\\pi} \\exp(it x_1 x_2 - x_1^2/2 - x_2^2/2) dx_1 dx_2 \\\\\n& =  \\int_{\\mathbb{R}} dx_1 \\frac{1}{\\sqrt{2\\pi}} \\exp(-x_1^2 /2) \\int_{\\mathbb{R}} dx_2 \\frac{1}{\\sqrt{2\\pi}} \\exp(it x_1 x_2 - x_2^2/2)\n\\\\ & =  \\int_{\\mathbb{R}} \\frac{1}{\\sqrt{2\\pi}} \\exp(-x_1^2/2) \\Phi_{X_2}(tx_1) dx_1\n\\\\ & =  \\int_{\\mathbb{R}}  \\frac{1}{\\sqrt{2\\pi}} \\exp(-x_1^2/2) \\exp(-t^2 x_1^2 /2) dx_1\n\\\\ & =  \\int_{\\mathbb{R}} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(- \\frac{x_1^2 (1+t^2)}{2} \\right) dx_1\n\\\\ & =  \\frac{1}{\\sqrt{1+t^2}}\n\\end{align*}\\]\net donc les \\((X_i, i \\ge 1)\\) étant i.i.d, pour \\(t \\in \\mathbb{R}\\),\n\\[\\Phi_Z(t) = \\Phi_{X_1X_2}(t) \\Phi_{X_3X_4}(t) = \\frac{1}{1+t^2}.\\]\nIl s’agit de la fonction caractéristique d’une variable “exponentielle symétrique”, de densité \\(x \\to \\frac{1}{2}\\exp(-|x|)\\) (c’est d’ailleurs une manière d’effectuer le calcul de la fonction caractéristique d’une Cauchy\\((1)\\)). On peut le vérifier directement. Soit \\(\\xi \\sim \\mathrm{Ber}(1/2)\\), et \\((X,Y)\\) i.i.d suivant une loi exp\\((1)\\). La variable \\(T=\\xi X -(1-\\xi)Y\\) a une loi exponentielle symétrique. De plus pour \\(t \\in \\mathbb{R}\\),\n\\[\\begin{align*}\n\\Phi_T(t) & =  \\mathbb{E}[\\exp(it \\xi X -it(1-\\xi)Y)]  \\\\\n& =  \\frac{1}{2} \\mathbb{E}[\\exp(itX)] + \\frac{1}{2}\\mathbb{E}[\\exp(-itY)] \\\\\n& =  \\frac{1}{2} \\frac{1}{1-it} + \\frac{1}{2} \\frac{1}{1+it}\n\\\\ & =  \\frac{\\frac{1}{2}(1+it) + \\frac{1}{2}(1-it)}{1+t^2} = \\frac{1}{1+t^2}\n\\end{align*}\\]\ncomme souhaité.\n\n\n\n\nExercice 20\nSoit \\(Z = (X, Y )\\), une variable aléatoire à valeurs dans \\(\\mathbb{R}^2\\). On suppose que \\(Z\\) admet une densité \\(f\\) définie par\n\\[f(x, y) = \\frac{4}{\\pi \\sigma^2} \\exp\\left(-\\frac{x^2+y^2}{\\sigma^2}\\right) \\mathbf{1}_{\\{x \\ge |y|\\}},\\]\noù \\(\\sigma&gt;0\\).\n\nVérifier que \\(f\\) est bien une densité de probabilité.\nCalculer les lois de \\(X\\) et de \\(Y\\) . Les variables aléatoires \\(X\\) et \\(Y\\) sont-elles indépendantes ?\nCalculer la loi de \\((X - Y, X + Y )\\) et montrer que \\(X - Y\\) et \\(X + Y\\) sont indépendantes.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\\(f : \\mathbb{R}^2 \\to \\mathbb{R}_+\\) est continue donc borélienne Par symétrie des rôles de \\(x\\) et \\(y\\),\n\n\\[\\int_{\\mathbb{R}^2} f(x,y) dx dy = \\int_{\\mathbb{R}^2}\\frac{4}{\\pi \\sigma^2} \\exp\\left(-\\frac{x^2+y^2}{\\sigma^2}\\right) \\mathbf{1}_{\\{y \\ge |x|\\}}.\\]\nPar parité de \\(x \\to \\exp(-\\frac{x^2}{2})\\),\n\\[\\int_{\\mathbb{R}^2} f(x,y) dx dy = \\int_{\\mathbb{R}^2}\\frac{4}{\\pi \\sigma^2} \\exp\\left(-\\frac{x^2+y^2}{\\sigma^2}\\right) \\mathbf{1}_{\\{x \\le -|y|\\}}.\\]\nA nouveau par symétrie des rôles de \\(x\\) et \\(y\\)\n\\[\\int_{\\mathbb{R}^2} f(x,y) dx dy = \\int_{\\mathbb{R}^2}\\frac{4}{\\pi \\sigma^2} \\exp\\left(-\\frac{x^2+y^2}{\\sigma^2}\\right) \\mathbf{1}_{\\{y  \\le -|x|\\}}.\\]\nEnfin, la réunion des \\(4\\) ensembles \\(\\{(x,y) \\in \\mathbb{R}^2: x \\ge |y|\\}, \\{(x,y) \\in \\mathbb{R}^2 : y \\ge |x|\\},  \\{(x,y) \\in \\mathbb{R}^2 : x \\le -|y|\\}, \\{(x,y) \\in \\mathbb{R}^2 : y \\le -|x|\\}\\) est \\(\\mathbb{R}^2\\) tout entier. L’intersection des \\(2\\) premiers est \\(\\{(x,y) : x = y \\ge 0\\}\\), de mesure de Lebesgue nulle, et des considérations similaires permettent d’assurer que c’est également le cas pour l’intersection de n’importe quelle paire parmi ces \\(4\\) ensembles. On conclut que\n\\[\\begin{align*}\n\\int_{\\mathbb{R}^2} f(x,y) dx dy  \n& =  \\frac{1}{4} \\int_{\\mathbb{R}^2}   \\frac{4}{\\pi \\sigma^2} \\exp\\left(-\\frac{x^2+y^2}{\\sigma^2}\\right) \\\\\n& =  \\int_{0}^{\\infty} \\int_0^{2\\pi} \\frac{1}{\\pi \\sigma^2} \\exp\\left(-\\frac{r^2}{\\sigma^2}\\right) r dr d\\theta\\\\\n& = \\left[ \\exp\\left(-\\frac{r^2}{\\sigma^2}\\right) \\right]_0^{\\infty} = 1,  \n\\end{align*}\\]\ncomme souhaité. 1. Si \\(\\phi : \\mathbb{R} \\to \\mathbb{R}_+\\) est borélienne\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(X)] & =  \\int_{\\mathbb{R}^2} \\phi(x) f(x,y) dx dy \\\\\n& =  \\int_{\\mathbb{R}_+} \\phi(x) \\frac{4}{\\pi \\sigma^2} \\exp\\left(-\\frac{x^2}{\\sigma^2}\\right) \\int_{-x}^x \\exp\\left(-\\frac{y^2}{\\sigma^2}\\right) dy dx\n\\end{align*}\\]\nOn déduit que \\(X\\) possède la densité \\(f_X\\) telle que \\[f_X(x) =  \\frac{4}{\\pi \\sigma^2} \\exp\\left(-\\frac{x^2}{\\sigma^2}\\right) \\int_{-x}^x \\exp\\left(-\\frac{y^2}{\\sigma^2}\\right) dy \\mathbf{1}_{\\{x \\ge 0\\}}\\]\nRemarque : Avec \\(\\mathrm{erf}(x) = \\int_{0}^x \\frac{2}{\\sqrt{\\pi}} \\exp\\left(-u^2\\right) du\\), on peut réexprimer\n\\[\\frac{1}{\\sigma \\sqrt{\\pi}} \\int_{-x}^x \\exp\\left(-\\frac{y^2}{\\sigma^2}\\right) dy =  \\mathrm{erf}\\left(\\frac{x}{\\sigma}\\right),\\]\nde sorte que\n\\[f_X(x) =  \\frac{4}{\\sigma \\sqrt{\\pi} } \\exp\\left(-\\frac{x^2}{\\sigma^2}\\right) \\mathrm{erf}\\left(\\frac{x}{\\sigma}\\right) \\mathbf{1}_{\\{x \\ge 0\\}}\\]\nToujours pour \\(\\phi: \\mathbb{R} \\to \\mathbb{R}_+\\) borélienne, on a\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(Y)] & =  \\int_{\\mathbb{R}^2} \\phi(y) f(x,y) dx dy \\\\\n& =  \\int_{\\mathbb{R}} \\phi(y) \\frac{4}{\\pi \\sigma^2} \\exp\\left(-\\frac{y^2}{\\sigma^2}\\right) \\int_{|y|}^{\\infty} \\exp\\left(-\\frac{x^2}{\\sigma^2}\\right) dx dy\n\\end{align*}\\]\nOn déduit que \\(Y\\) possède la densité \\(f_Y\\) telle que \\[f_Y(y) =  \\frac{4}{\\pi \\sigma^2} \\exp\\left(-\\frac{y^2}{\\sigma^2}\\right) \\int_{|y|}^{\\infty} \\exp\\left(-\\frac{x^2}{\\sigma^2}\\right) dx \\]\nRemarque : On a \\[\\int_{|y|}^{\\infty} \\frac{2}{\\sqrt{\\pi}} \\exp\\left(-u^2\\right) du = \\frac{1}{2}(1 - \\mathrm{erf}(|y|))= \\frac{1}{2}\\mathrm{erfc}(|y|)\\] et donc \\[\\int_{|y|}^{\\infty} \\frac{1}{\\sigma \\sqrt{\\pi}} \\exp \\left( -\\frac{x^2}{\\sigma^2} \\right)  dx  = \\frac{1}{2}\\mathrm{erfc}\\left(\\frac{|y|}{\\sigma}\\right),\\]\nde sorte que \\[ f_Y(y) =  \\frac{2}{ \\sigma \\sqrt{\\pi}} \\exp\\left(-\\frac{y^2}{\\sigma^2}\\right)  \\mathrm{erfc}\\left(\\frac{|y|}{\\sigma}\\right).\\] Enfin puisque par exemple \\(\\mathbb{P}(X \\ge Y \\ge 1) =\\mathbb{P}(Y \\ge 1)\\) on a \\(\\mathbb{P}(X \\ge 1, Y \\ge 1) \\ne \\mathbb{P}(X \\ge 1) \\mathbb{P}(Y \\ge 1)\\) et les variables \\(X\\) et \\(Y\\) ne sont pas indépendantes.\n\nSoit \\(\\psi : \\mathbb{R}^2 \\to \\mathbb{R}_+\\) borélienne, on a \\[\\begin{align*}\n\\mathbb{E}[\\psi(X-Y,X+Y)] & =  \\int_{\\mathbb{R}^2} \\psi(x-y,x+y) f(x,y) dx dy \\\\\n& =  \\int_{\\mathbb{R}^2} \\psi(x-y,y+y)  \\frac{4}{\\pi \\sigma^2} \\exp\\left(-\\frac{x^2+y^2}{\\sigma^2}\\right)  \\mathbf{1}_{\\{x \\ge |y|\\}}\n\\end{align*}\\]\n\nNotons que \\(G: \\begin{cases} & \\{(x,y) \\in \\mathbb{R}^2 : x \\ge |y|\\} \\to \\{(u,v) \\in \\mathbb{R}_+^2\\} \\\\ & (x,y) \\to (u,v) =(x+y,x-y) \\end{cases}\\) est un \\(\\mathcal{C}^1\\)-difféomorphisme (d’inverse \\(G^{-1} : (u,v) \\to (x,y)\\) avec \\(x = \\frac{u+v}{2}, y = \\frac{u-v}{2}\\)), de jacobien \\(2\\). Par la formule de changement de variables, et en remarquant que \\(u^2 + v^2 = 2(x^2+y^2)\\), on obtient\n\\[\\begin{align*}\n\\mathbb{E}[\\psi(X-Y, X+Y)]\n& = \\int_{\\mathbb{R}_+^2}  \\psi(u,v) \\frac{2}{\\pi \\sigma^2} \\exp\\left(-\\frac{u^2 + v^2}{2\\sigma^2}\\right)  du dv \\\\\n& =  \\int_{\\mathbb{R}^2} \\psi(u,v) f_U(u) f_V(v) du dv\n\\end{align*}\\]\navec \\[f_U(u) = f_V(u) = \\frac{2}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{u^2}{2\\sigma^2} \\right) \\mathbf{1}_{\\{u \\ge 0\\}},\\] qui est la densité de \\(|Z|\\) où \\(Z \\sim \\mathcal{N}(0,1)\\).\nOn conclut que \\((X+Y, X-Y)\\) sont i.i.d. suivant la loi de \\(|Z|\\).\n\n\n\n\nExercice 21\n\nSoit \\(X_1,...,X_n\\) des variables i.i.d, \\(\\sim \\mathcal{N}(0,1)\\), et \\(Z=\\sum_{i=1}^n \\alpha_i X_i\\) (où les \\(\\alpha_i, i=1,...,n\\) sont de rééls fixés). Quelle est la loi de \\(Z\\)?\nSoit \\((X_1,...,X_n) \\sim \\mathcal{N}(0,A)\\). Quelle est la loi de \\(Z=\\sum_{i=1}^n \\alpha_i X_i\\)?\nSoit \\((X_1,...,X_n) \\sim \\mathcal{N}(0,A)\\). Quelle est la loi de \\((Z_1,Z_2)\\), où, pour des rééls \\(\\alpha_{i,1}, \\alpha_{i,2}, i=1,...,n\\) fixés, \\[  Z_1=\\sum_{i=1}^n \\alpha_{i,1} X_i,  Z_2=\\sum_{i=1}^n \\alpha_{i,2} X_i.\\]\nGénéraliser la question précédente en exprimant la loi de \\[ (Z_1,...,Z_n) =  (X_1,...,X_n) P,\\] où \\(P\\) est une matrice \\(n \\times n\\).\nSoient \\(X,Y\\) deux variables indépendantes, \\(\\sim \\mathrm{Unif}[0,1]\\). Quelle est la loi de \\(S=X+Y\\)?\nSoient \\(X, Y\\) des variables indépendantes de loi respectives \\(\\Gamma(a,c), \\Gamma(b,c)\\), où \\(a,b,c&gt;0\\). On pose \\(S=X+Y,\nT= \\frac{X}{X+Y}\\) Quelle est la loi du couple \\((S,T)\\)?\nSoient \\(X,Y\\) deux variables indépendantes, \\(\\sim \\mathrm{Unif}[0,1]\\). On pose \\(U = \\sqrt{-2\\log(X)} \\cdot \\cos(2\\pi Y), V = \\sqrt{-2\\log(X)} \\cdot \\sin(2\\pi Y)\\). Quelle est la loi du couple \\((X,Y)\\)?\nSoient \\(X,Y\\) deux variables indépendantes, \\(\\sim \\mathcal{N}(0,1)\\). On pose \\(T = \\frac{Y}{X}\\). Quelle est la loi de \\(T\\)?\nSoient \\((X,Y)\\) un couple de variables indépendantes, \\(\\sim \\mathcal{N}(0,1)\\). On pose \\(U=X, V= X^2 + Y^2\\). Quelle est la loi de \\((U,V)\\)?\nSoit \\(X\\) de densité \\(\\exp(-x) \\mathbf{1}_{\\mathbb{R}_+}(x)\\). On pose \\(U = [X]\\) et \\(V= X-[X]\\), la partie entière, resp. la partie décimale de \\(X\\). Quelle est la loi de \\((U,V)\\)?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nEn utilisant d’abord l’indépendance des \\((X_k, 1 \\le k \\le n)\\) à la deuxième ligne ci-dessous, puis le fait que \\(\\Phi_{X_k}(u) =\\exp(-u^2/2)\\) à la suivante, on obtient que pour \\(t \\in \\mathbb{R}\\),\n\n\\[\\begin{align*}\n\\Phi_{Z}(t) & =  \\mathbb{E}\\left[\\exp\\left(it \\sum_{k=1}^n \\alpha_k X_k \\right) \\right] \\\\\n& =  \\prod_{k=1}^n \\Phi_{X_k}(t \\alpha_k) \\\\\n& =  \\exp\\left( - \\frac{t^2}{2} \\sum_{k=1}^n \\alpha_k^2 \\right)\n\\end{align*}\\]\net on déduit que \\(Z \\sim \\mathcal{N}\\left(0, \\sum_{k=1}^n \\alpha_k^2\\right)\\). 1. Lorsque \\(X\\) est un vecteur gaussien, toute combinaison linéaire des coordonnées de \\(X\\) suit une loi gaussienne. Comme les coordonnées de \\(X\\) sont ici centrées, il en va de même pour \\(Z\\). Par ailleurs\n\\[\\begin{align*}\n\\mathrm{Var}(Z)\n& = \\sum_{k=1}^n \\sum_{\\ell=1}^n \\alpha_k \\alpha_{\\ell} \\mathrm{Cov}(X_k, X_{\\ell}) \\\\\n& = \\alpha^T A \\alpha.\n\\end{align*}\\]\nFinalement \\(Z \\sim \\mathcal{N}(0,\\alpha^T A \\alpha)\\), autrement dit \\[\\Phi_Z(t) = \\exp\\left(-t^2\\frac{\\alpha^T A \\alpha}{2} \\right).\\]\n1. Soit \\((u,v) \\in \\mathbb{R}^2\\), on a \\[uZ_1+vZ_2 = \\sum_{i=1}^n (\\alpha_{i,1} + v \\alpha_{i,2}) X_i,\\] et d’après la question précédente, ceci est distribué suivant une loi \\(\\mathcal{N}(0, (u \\alpha_1 + v \\alpha_2)^T A (u\\alpha_1+v \\alpha_2))\\), et donc\n\\[\\Phi_{uZ_1+vZ_2}(1) = \\mathbb{E}[\\exp (iu Z_1+ivZ_2)] = \\Phi_{(Z_1,Z_2)}(u,v) = \\exp\\left( - \\frac{(u\\alpha_1+ v \\alpha_2)^T A (u \\alpha_1+v\\alpha_2)}{2} \\right).\\] Reste à observer que \\(u\\alpha_1+v\\alpha_2\\) est le produit d’une matrice, disons \\(P\\) à \\(n\\) lignes et deux colonnes, la première ayant les coordonnées de \\(\\alpha_1\\) la deuxième celle de \\(\\alpha_2\\), par le vecteur \\(\\begin{pmatrix} u \\\\ v \\end{pmatrix}\\). On conclut que \\(Z \\sim \\mathcal{N}(0, P^T A P)\\)\n\nLa matrice \\(P\\) joue le même rôle que la matrice de la question précédent (à ceci près qu’elle possède désormais \\(n\\) lignes et \\(n\\) colonnes). Par le même raisonnement qu’à la question précédente, on trouve que \\(Z \\sim \\mathcal{N}(0,P^T A P)\\).\n\nPuisque \\(X\\) et \\(Y\\) sont indépendantes et à densité, \\((X,Y)\\) possède la densité jointe \\(f_{(X,Y)}\\) telle que \\(f_{(X,Y)}(x,y) = f_X(x) f_Y(y)\\). On a donc pour \\(\\phi: \\mathbb{R}^2 \\to \\mathbb{R}_+\\) borélienne,\n\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(X+Y)] & =  \\int_{[0,1]^2} \\phi(x+y) \\mathrm{d}x \\mathrm{d}y\n\\end{align*}\\]\nOn fait le changement de variables \\(\\begin{cases} & [0,1]^2 \\to \\{(s,t) \\in [0,2] \\times [0,1] :  t+1 \\ge s \\ge t \\} \\\\ & (x,y) \\to ( s=x+y, t=y) \\end{cases}\\), de jacobien \\(1\\), et on trouve\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(X+Y)]\n& =  \\int_{0}^2 ds \\phi(s) \\int_{\\max(s-1,0)}^{\\min(s,1)} \\mathrm{d}t\\\\\n& =   \\int_0^2 \\phi(s) \\min(s, 1-s) \\mathrm{d}s\n\\end{align*}\\]\net on déduit que \\(S\\) possède la densité \\(f_S\\), où \\[f_S(s) = \\begin{cases} & s \\mbox{ si } 0 \\le s \\le 1 \\\\ & (1-s) \\mbox{ si } 1 \\le s \\le 2 \\\\ & 0 \\mbox{ sinon.} \\end{cases}\\]\n\nD’après l’énoncé, \\((X,Y)\\) possède la densité\n\\[f_{(X,Y)}(x,y) = \\frac{c^{a+b}}{\\Gamma(a)\\Gamma(b)} x^{a-1} y^{b-1} \\exp(-c(x+y)) \\mathbf{1}_{\\{x &gt; 0, y &gt; 0\\}}.\\] Pour \\(\\phi : \\mathbb{R}^2 \\to \\mathbb{R}_+\\) borélienne on a donc \\[\\mathbb{E}[\\phi(S,T)] = \\int_{(\\mathbb{R}_+^*)^2} \\phi\\left(x+y, \\frac{x}{x+y}\\right)   \\frac{c^{a+b}}{\\Gamma(a)\\Gamma(b)} x^{a-1} y^{b-1} \\exp(-c(x+y)) dx dy \\] On effectue alors le changement de variables via le \\(\\mathcal{C}^1\\)-difféomorphisme :\n\\[G : \\begin{cases} & (\\mathbb{R}_+^*)^2 \\to \\mathbb{R}_+^* \\times (0,1) \\\\ & (x,y) \\to \\left(x+y, \\frac{x}{x+y}\\right) \\end{cases}\\] d’inverse \\[G^{-1} : \\begin{cases} &  \\mathbb{R}_+^* \\times (0,1)  \\to (\\mathbb{R}_+^*)^2  \\\\ & (s,t) \\to (st, s(1-t) \\end{cases}\\] dont le jacobien est \\(|J^{-1}| = s\\), pour obtenir \\[\\mathbb{E}[\\phi(S,T)]  =  \\int_{\\mathbb{R}_+^*} ds \\int_0^1 dt  \\phi(s,t)   \\frac{c^{a+b}}{\\Gamma(a)\\Gamma(b)} s^{a+b-1}  \\exp(-c s) t^{a-1}(1-t)^{b-1} \\] et on conclut que \\(S \\sim \\mathrm{Gamma}(a+b,c)\\) est indépendant de \\(T \\sim \\mathrm{Beta}(a,b)\\).\nL’application \\[\\Psi : \\begin{cases}  & [0,1]^2 \\to \\mathbb{R}^2  \\\\ & (x,y) \\to (u,v) = \\left(\\sqrt{-2\\ln(x)} \\cos(2\\pi v),  \\sqrt{-2\\ln(x)} \\sin(2\\pi y)\\right) \\end{cases}\\] est un \\(\\mathcal{C}^1\\)-difféomorphisme, composée de \\((x,y) \\to (r,\\theta) = (\\sqrt{-2\\ln(x)},2\\pi y)\\) et \\((r, \\theta) \\to (u,v) = (r \\cos(\\theta), r \\sin(\\theta))\\). Le Jacobien de \\(\\Psi\\) est \\(2\\pi \\exp((u^2+v^2)/2)\\) et on déduit que pour \\(\\phi\\) continue bornée de \\(\\mathbb{R}^2\\) dans \\(\\mathbb{R}\\), \\[\\begin{align*}\n\\mathbb{E}[\\phi(U,V)] & =  \\int_{0}^1 \\int_0^1 dx dy \\phi \\left(\\sqrt{-2\\ln(x)} \\cos(2\\pi y), \\sqrt{-2\\ln(x)} \\sin(2\\pi y)\\right) \\\\ & =  \\int_{\\mathbb{R}^2} du dv \\phi(u,v) \\frac{1}{2\\pi} \\exp(-u^2/2) \\exp(-v^2/2),\n\\end{align*}\\]\n\net finalement que \\((U,V) \\sim \\mathcal{N}(0,I_2)\\).\n\nNotons d’abord que \\(\\mathbb{P}(X=0)=0\\) de sorte que \\(Z\\) est définie p.s. Par ailleurs \\((X,Y) \\sim \\mathcal{N}(0,Id_2)\\) et a la densité correspondante.\n\nOn a, pour \\(\\phi: \\mathbb{R}^2 \\to \\mathbb{R}\\) bornée mesurable, gr^ace au changement de variables de \\(\\mathbb{R}^* \\times \\mathbb{R}^*\\) dans lui-même, qui à \\((x,y)\\) associe \\((x,z)=(x,\\frac{y}{x})\\) (et dont l’inverse du jacobien vaut \\(|x|\\)) :\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(X,Z)]\n& = \\frac{1}{2\\pi} \\int_{(\\mathbb{R}^*)^2} \\phi(x, \\frac{y}{x}) \\exp(-x^2/2-y^2/2) dx dy \\\\\n& = \\frac{1}{2\\pi} \\int_{(\\mathbb{R}^*)^2} \\phi(x, z) |x| \\exp\\left(-\\frac{x^2}{2} (1+z^2 \\right) dx \\mathrm{d}z   \n\\end{align*}\\]\net donc \\((X,Z)\\) a densité\n\\[f_{(X,Z)}(x,z) = \\frac{1}{2\\pi}  |x| \\exp\\left(-\\frac{x^2}{2} (1+z^2)\\right).\\]\nOn en déduit que\n\\[\\begin{align*}\nf_Z(z) & =  2 \\int_{\\mathbb{R}_+} \\frac{1}{2\\pi}  x z^2 \\exp\\left(-\\frac{x^2}{2} (1+z^2)\\right) dx = \\frac{1}{\\pi} \\frac{1}{1+z^2},\n\\end{align*}\\]\nde sorte que \\(Z \\sim \\mathrm{Cauchy}(1)\\).\n\nQuitte à considérer les \\(\\mathcal{C}^1\\)-difféomorphisme\n\\[G : \\begin{cases} & \\mathbb{R} \\times \\mathbb{R}_-^* \\to \\{(u,v) \\in \\mathbb{R} \\times \\mathbb{R}_+^* : v \\ge u^2\\} \\\\ (x,y) \\to (x, x^2+y^2) \\end{cases}, \\ H: \\begin{cases} & \\mathbb{R} \\times \\mathbb{R}_+ \\to \\{(u,v) \\in \\mathbb{R} \\times \\mathbb{R}_+^* : v \\ge u^2\\} \\\\ (x,y) \\to (x, x^2+y^2) \\end{cases},\\]\ndont les jacobiens sont tous deux égaux à \\(2|y| = 2 \\sqrt{u-v^2}\\), on obtient pour \\(\\phi : \\mathbb{R}^2 \\to \\mathbb{R}_+\\) borélienne (les deux intégrales fournissent des contributions identiques) :\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(U,V)]  \n&  =  2 \\int_{\\mathbb{R} \\times \\mathbb{R}_+} \\phi(u,v) \\frac{\\exp\\left(-\\frac{v}{2}\\right)}{4 \\pi \\sqrt{u-v^2}} \\mathbf{I}_{\\{v \\ge u^2\\}}du dv\n\\end{align*}\\]\net on conclut que \\((U,V)\\) possède la densité \\(f_{(U,V)}\\) telle que pour tout \\((u,v) \\in \\mathbb{R}^2\\),\n\\[f_{(U,V)}(u,v) = \\frac{\\exp\\left(-\\frac{v}{2}\\right)}{2 \\pi \\sqrt{u-v^2}} \\mathbf{I}_{\\{v \\ge u^2\\}}.\\]\nRemarque : On peut recalculer à partir de cette densité les marginales, mais il est évident que \\(X \\sim \\mathcal{N}(0,1)\\), et on avait vu plus haut que \\(X^2+Y^2 \\sim \\exp(1/2)\\).\nOn a pour \\(\\phi : \\mathbb{R}^2 \\to \\mathbb{R}_+\\) borélienne\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(U,V)]\n& =  \\int_{\\mathbb{R}_+} \\phi( [x], x-[x])  \\exp(-x) dx \\\\\n& =  \\sum_{n \\ge 0} \\int_n^{n+1} \\phi(n, x-n) \\exp(-x) dx \\\\\n& =  \\sum_{n \\ge 0} \\int_0^1 \\phi(n,v) \\exp(-n) \\exp(-v) du\n\\end{align*}\\]\nSi \\(g : \\mathbb{R} \\to \\mathbb{R}_+\\), \\(h : \\mathbb{R} \\to \\mathbb{R}_+\\) boréliennes, quitte à considérer \\(\\phi(u,v) = g(u) h(v)\\) on obtient\n\\[\\begin{align*}\n\\mathbb{E}[g(U) h(V)]\n& =  \\sum_{n \\ge 0} g(n) \\exp(-1)^n (1-\\exp(-1)) \\int_0^1 h(v) \\frac{\\exp(-v)}{1-\\exp(-1)} du \\\\  \n& =  \\mathbb{E}[g(U)] \\mathbb{E}[h(V)]  \n\\end{align*}\\]\nOn conclut que \\(U\\) et \\(V\\) sont indépendantes, avec \\(U + 1 \\sim  \\mathrm{Geom}(1-\\exp(-1))\\) et \\(V\\) de densité \\(f_V\\) avec, pour tout \\(v\\in \\mathbb{R}\\),\n\\[f_V(v) = \\frac{1}{1-\\exp(-1)} \\exp(-v) \\mathbf{1}_{(0,1)}(v).\\]\nRemarque : La loi de \\(V\\) est celle d’une variable exponentielle de paramètre \\(1\\) conditionnée à être plus petite que \\(1\\).\n\n\n\n\n\nExercice 22\nSoient \\(X_1, X_2\\) deux variables indépendantes et identiquement distribuées suivant la loi \\(\\mathrm{Unif}\\{1,2,3\\}\\).\nOn note \\(U = \\min\\{X_1,X_2\\}\\), \\(V= \\max\\{X_1,X_2\\}\\) et enfin \\(S= U+V\\).\n\nDéterminer la loi jointe de \\((U, V)\\) et \\((V, S)\\).\nEn déduire les lois de \\(U, V\\), et \\(S\\). Calculer les lois de \\(UV\\) et \\(VS\\).\nCalculer les covariances et les coefficients de corrélation de \\((U, V)\\) et \\((V, S)\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nPar hypothèse, la loi de \\((X_1,X_2)\\) est uniforme sur \\(\\{1,2,3\\}^2\\), et on déduit, quitte à disjoindre les cas, que\n\\[\\begin{align*}\n& \\mathbb{P}((U,V) =(1,1)) = \\mathbb{P}(X_1=1, X_2=1) = 1/9 \\\\  \n& \\mathbb{P}((U,V)=(1,2)) = \\mathbb{P}(X_1=1,X_2=2) + \\mathbb{P}(X_1=2,X_2=1) = 2/9 \\\\\n& \\mathbb{P}((U,V) = (1,3)) =  \\mathbb{P}(X_1=1,X_2=3) + \\mathbb{P}(X_1=3,X_2=1) = 2/9 \\\\\n& \\mathbb{P}((U,V) = (2,2)) = \\mathbb{P}(X_1=2, X_2=2) = 1/9 \\\\  \n& \\mathbb{P}((U,V)=(2,3)) = \\mathbb{P}(X_1=2,X_2=3) + \\mathbb{P}(X_1=3,X_2=2) = 2/9 \\\\\n& \\mathbb{P}((U,V)=(3,3)) = \\mathbb{P}(X_1=3,X_2=3) = 1/9\n\\end{align*}\\]\nLa même disjonction de cas conduit à la loi de \\((V,S)\\) :\n\\[\\begin{align*}\n& \\mathbb{P}(V=1,S=2) = 1/9 \\\\  \n& \\mathbb{P}(V=2,S=3) = 2/9 \\\\  \n& \\mathbb{P}(V=3,S=4) = 2/9 \\\\\n& \\mathbb{P}(V=2,S=4) = 1/9 \\\\\n& \\mathbb{P}(V=3, S=5) = 2/9 \\\\  \n& \\mathbb{P}(V=3, S=6) = 1/9  \n\\end{align*}\\]\nOn en déduit (on peut également faire un raisonnement direct)\n\\[\\begin{align*}\n& \\mathbb{P}(U=1) = 5/9, \\ \\mathbb{P}(U=2)=1/3, \\ \\mathbb{P}(U=3)=1/9 \\\\  \n& \\mathbb{P}(V=1) = 1/9, \\ \\mathbb{P}(V=2) = 1/3,\\ \\mathbb{P}(V=3)=5/9 \\\\\n& \\mathbb{P}(S=2) = \\mathbb{P}(S=6) = 1/9, \\ \\mathbb{P}(S=3)=\\mathbb{P}(S=5) = 2/9. \\ \\mathbb{P}(S=4)=1/3\n\\end{align*}\\]\nLa loi de \\(UV\\) se déduit facilement de la loi jointe de \\((U,V)\\) calculée à la question précédente\n\\[\\mathbb{P}(UV=1) = \\mathbb{P}(UV=4) = pp(UV=9) = 1/9, \\ \\mathbb{P}(UV=2) = \\mathbb{P}(UV=3) = \\mathbb{P}(UV=6) = 2/9.\\]\n\nDe même pour la loi de \\(VS\\)\n$$\\mathbb{P}(VS=2) = \\mathbb{P}(VS=8) = \\mathbb{P}(VS=18) = 1/9, \\ \\mathbb{P}(VS=6) = \\mathbb{P}(VS=12) = \\mathbb{P}(VS=15) = 2/9$$ \n\nOn déduit de la question précédente\n\\[\\mathbb{E}[U]=\\frac{14}{9}, \\ \\mathbb{E}[V]=\\frac{22}{9}, \\ \\mathbb{E}[UV]=4,\\ \\mathrm{Cov}(U,V) = \\frac{16}{81},\\]\net\n\\[\\mathrm{Var}(U) = \\frac{38}{81} = \\mathrm{Var}(V), \\ \\ \\rho(U,V)= \\frac{8}{19}.\\]\nPar ailleurs\n\\[\\mathbb{E}[V]= \\frac{22}{9}, \\ \\mathbb{E}[S]=4, \\ \\mathbb{E}[VS]=\\frac{94}{9}, \\ \\mathrm{Cov}(V,S) = \\frac{2}{3}.\\]\net\n\\[\\mathrm{Var}(V) = \\frac{38}{9}, \\mathrm{Var}(S) = \\frac{2}{3}, \\rho(V,S) \\approx 0.397\\]"
  },
  {
    "objectID": "corriges/td1.html#le-cadre-gaussien",
    "href": "corriges/td1.html#le-cadre-gaussien",
    "title": "Variables aléatoires réelles",
    "section": "Le cadre gaussien",
    "text": "Le cadre gaussien\n\nExercice 23\nOn considère deux variables indépendantes \\(Y \\sim \\mathcal{N}(0,1)\\) et\n\\[\\varepsilon= \\begin{cases} & 1 \\mbox{ avec probabilité } p \\\\ & -1 \\mbox{ avec probabilité } 1-p, \\end{cases}\\]\noù \\(p \\in (0,1)\\).\n\nQuelle est la loi de \\(Z= \\varepsilon Y\\)\nQuelle est la loi de \\(Y+Z\\)?\nLe vecteur \\((Y,Z)\\) est-il un vecteur gaussien?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nSoit \\(\\phi : \\mathbb{R} \\to \\mathbb{R}_+\\) borélienne\n\\[\\begin{align*}\n\\mathbb{E}[\\phi(\\varepsilon)]\n& =  \\mathbb{E}[\\phi(Y) \\mathbf{1}_{\\varepsilon=1}] + \\mathbb{E}[\\phi(-Y) \\mathbf{1}_{\\varepsilon=-1}]  \\\\\n& =  p \\mathbb{E}[\\phi(Y)] + (1-p) \\mathbb{E}[\\phi(-Y)] \\\\\n& =  p \\int_{\\mathbb{R}} \\phi(z) \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2) \\mathrm{d}z + (1-p) \\int_{\\mathbb{R}} \\phi(-z) \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2) \\mathrm{d}z  \\\\\n& =  \\int_{\\mathbb{R}} \\phi(z) \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2) \\mathrm{d}z\n\\end{align*}\\]\noù on a effectué le changement de variables \\(u=-z\\) pour voir que \\(\\int_{\\mathbb{R}} \\phi(-z) \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2) \\mathrm{d}z = \\int_{\\mathbb{R}} \\phi(u) \\frac{1}{\\sqrt{2\\pi}} \\exp(-u^2/2) du\\). On conclut que \\(Z \\sim \\mathcal{N}(0,1)\\).\nOn a $Y+Z = (1+et donc pour \\(\\phi\\) comme ci-dessus\n\\[\\mathbb{E}[\\phi(Y+Z)] = (1-p)\\phi(0) + p\\int_{\\mathbb{R}} \\phi(u) \\frac{1}{2 \\sqrt{2\\pi}} \\exp(-u^2/8) du.\\]\nEn particulier la loi de \\(Y+Z\\) n’est ni discrète ni à densité, donc pas gaussienne.\nNon, puisque \\(Y+Z\\) n’est pas gaussienne.\n\n\n\n\n\nExercice 24\nSoit \\((X,Y,Z)\\) le vecteur aléatoire gaussien d’espérance \\((1,1,0)\\) et de matrice de covariance\n\\[K = \\begin{pmatrix} 2 & 1 & 1 \\\\ 1 & 2 & 2 \\\\ 1 & 2 & 2  \\end{pmatrix}.\\]\n\nEcrire la fonction caractéristique de \\((X,Y,Z)\\).\n\nTrouver la loi de \\(2X+Y+Z\\), de \\(4X-2Y+Z\\), enfin de \\(Y-Z\\).\nLe vecteur \\((X,Y)\\) admet-il une densité dans \\(\\mathbb{R}^2\\). Si oui, laquelle?\nPour \\(a \\in \\mathbb{R}\\) on définit \\(u_a : \\mathbb{R}^3 \\to \\mathbb{R}^3\\) de matrice\n\\[A = \\begin{pmatrix} 1/\\sqrt{2} & 0 & 0 \\\\ a & -2a & 0 \\\\ 0 & 1 & -1\\end{pmatrix}.\\]\nDéterminer la loi de \\(u_a(X-1,Y-1,Z)\\) en fonction de \\(a\\).\nPour quelle valeur de \\(a\\) les deux premières coordonnées de \\(u_a(X-1,Y-1,Z)\\) suivent-ils une loi normale centrée réduite sur \\(\\mathbb{R}^2\\)?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\nDans les questions 2,3,4 ci-dessous, on fait usage de la Prop 3 du cours : si \\(V \\sim \\mathcal{N}(m,K)\\) est un vecteur gaussien de dimension \\(d\\) et \\(P\\) est une matrice à \\(p\\) lignes et \\(d\\) colonnes, on a \\(X \\sim \\mathcal{N}(Pm, P K P^T)\\). Ici \\(d=3\\), \\(m\\) est le vecteur de coordonnées \\((1,1,0)\\), et \\(K\\) la matrice précisée dans l’énoncé.\n\nD’après le cours (Prop 2), pour tout \\(t \\in \\mathbb{R}^3\\) de coordonnées \\((t_1,t_2,t_3)\\),\n\\[\\begin{align*}\n\\Phi_X(t)\n& = \\exp(it^Tm + PKP^T) \\\\  \n&  =  \\exp\\bigg(i(t_1+t_2) - (t_1^2+t_2^2+t_3^2+t_1 t_2+t_1 t_3+2t_2 t_3)\\bigg)\n\\end{align*}\\]\nOn applique le résultat général avec \\(P = (2 \\ \\ 1 \\ \\ 1)\\) puis avec \\(P=(4 \\ -2 \\ \\ 1)\\), et enfn avec \\(P=(0 \\ \\ 1 \\ -1)\\) pour obtenir\n\\[2X+Y+Z \\sim \\mathcal{N}(3,24), \\quad 4X-2Y+Z \\sim \\mathcal{N}(2, 26), \\quad Y-Z \\sim \\mathcal{N}(1,0)\\]\nDans le troisième cas notons que \\(\\mathbb{P}(Y-Z = 1)=1\\), de sorte que \\(Y-Z\\) n’a pas de densité.\nOn a \\(\\begin{pmatrix} X \\\\ Y \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mu=\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\Sigma = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\\right)\\). On a \\(\\det(\\Sigma)=3\\), et est \\(\\Sigma^{-1} = \\frac{1}{3} \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix}\\), et donc ce vecteur possède la densité \\(f\\) sur \\(\\mathbb{R}^2\\), où pour \\(x \\in \\mathbb{R}^2\\) de coordonnées \\((x_1,x_2)\\),\n\\[\\begin{align*}\nf(x)\n& =  \\frac{1}{2\\pi\\sqrt{3}} \\exp(-\\frac{1}{2} (x-\\mu)^T \\Sigma^{-1} (x-\\mu) \\\\\n& =  \\frac{1}{2 \\pi \\sqrt{3}} \\exp\\left(-\\frac{1}{3} ((x_1-1)^2 + (x_2-1)^2 - (x_1-1)(x_2-1)) \\right)\n\\end{align*}\\]\nOn obtient ici que\n\\[A \\begin{pmatrix} X \\\\ Y \\\\ Z \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 6a^2 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} \\right),\\]\nde sorte qu’on a le résultat souhaité pour \\(a \\in \\left\\{-\\frac{1}{\\sqrt{6}}, \\frac{1}{\\sqrt{6}} \\right\\}\\)\n\n\n\n\n\nExercice 25\nSoit \\(\\rho\\in ]-1,1[\\) et \\((X,Y)\\) un vecteur gaussien centré de matrice de covariances \\(M = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}\\). On notera \\(\\sigma = \\sqrt{1-\\rho^2}\\).\n\nCalculer det(\\(M\\)), \\(M^{-1}\\), puis exprimer la densité \\(f_{(X,Y)}\\) du vecteur \\((X,Y)\\).\nMontrer que\n\\[g_x(y) := \\frac{f_{(X,Y)}(x,y)}{f_X(x)} = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp \\left( - \\frac{1}{2(1-\\rho^2)} \\left(y-\\rho x\\right)^2\\right).\\]\nMontrer que pour tout \\(x \\in \\mathbb{R}\\), \\(y \\to g_x(y)\\) définit une densité.\nSi on note \\(Y_x\\) une variable de densité \\(g_x\\), que pouvez-vous dire sur la loi de \\(Y_x\\)?\n\nTrouver \\(\\alpha\\), \\(\\beta\\) deux réels tels que \\((X, \\alpha X + \\beta Y)\\) suit la loi normale centrée réduite.\n\nRemarquer que l’on peut écrire \\(Y= -\\frac{\\alpha}{\\beta} X +\\frac{1}{\\beta}(\\alpha X + \\beta Y)\\). Sauriez-vous dire pourquoi cette écriture est intéressante?\n\n\n\n\n\n\nNoteSolution\n\n\n\n\nOn a \\(\\det(M) = 1-\\rho^2 \\ne 0\\), \\(M^{-1} =  \\frac{1}{1-\\rho^2}  \\begin{pmatrix} 1 & - \\rho \\\\ -\\rho & 1 \\end{pmatrix}\\), et donc \\(\\begin{pmatrix} X \\\\ Y \\end{pmatrix}\\) possède la densité \\(f_{(X,Y)}\\) où, pour \\(\\begin{pmatrix} x \\\\ y \\end{pmatrix} \\in \\mathbb{R}^2\\),\n\\[\\begin{align*}\nf_{(X,Y)}(x)\n& = \\frac{1}{2\\pi \\sqrt{1-\\rho^2}} \\exp\\left(-\\frac{1}{2} \\begin{pmatrix} x \\\\ y \\end{pmatrix}^T M^{-1} \\begin{pmatrix} x \\\\ y \\end{pmatrix}  \\right)\\\\  \n& =  \\frac{1}{2\\pi \\sigma} \\exp\\left(- \\frac{1}{2 \\sigma^2} \\left(x^2 + y^2 - 2 \\rho x y \\right) \\right)\n\\end{align*}\\]\n\\(X \\sim \\mathcal{N}(0,1)\\) donc \\(f_X(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-x^2/2), x \\in \\mathbb{R}\\). On a donc, pour \\(y \\in \\mathbb{R}\\)\n\\[\\begin{align*}\ng_x(y)\n& =  \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp\\left( \\frac{1}{2(1-\\rho^2)} (x^2+y^2 - 2\\rho x y) + \\frac{1}{2} x^2 \\right) \\\\  \n& = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left( \\frac{1}{2(1-\\rho^2)} \\left( - \\frac{1}{2} (\\rho^2 x^2 - 2 \\rho x y + y ^2) \\right)\\right) \\\\\n& =   \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left( \\frac{1}{2(1-\\rho^2)} \\left( - \\frac{1}{2} (y-\\rho x)^2 \\right) \\right)\n\\end{align*}\\]\ncomme souhaité, et donc \\(Y_x \\sim \\mathcal{N}(\\rho x, 1-\\rho^2)\\).\nComme \\(\\begin{pmatrix} X \\\\ \\alpha X + \\beta Y \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ \\alpha & \\beta \\end{pmatrix}\\) d’après Prop 3 du cours\n\\[\\begin{pmatrix} X \\\\ \\alpha X + \\beta Y \\end{pmatrix}  \\sim \\mathcal{N}\\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 1 & 0 \\\\ \\alpha & \\beta \\end{pmatrix} \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ \\alpha & \\beta \\end{pmatrix}^T\\right)\\]\net donc\n\\[\\begin{pmatrix} X \\\\ \\alpha X + \\beta Y \\end{pmatrix}  \\sim \\mathcal{N}\\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 1 & 0 \\\\ \\alpha & \\beta \\end{pmatrix}\\begin{pmatrix} 1 & \\alpha+\\rho \\beta \\\\ \\rho & \\rho \\alpha + \\beta \\end{pmatrix} \\right) \\sim \\mathcal{N}\\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 1 & \\alpha+ \\rho \\beta \\\\ \\alpha + \\rho \\beta  & \\alpha^2 + 2 \\rho \\alpha \\beta + \\beta^2  \\end{pmatrix} \\right)\\]\nPour obtenir le résultat souhaité il faut et il suffit que \\(\\begin{cases} & \\alpha + \\rho \\beta = 0 \\\\ & \\alpha^2 + 2\\rho \\alpha \\beta + \\beta ^2 = 1 \\end{cases}\\),\nEn remplaçant \\(\\alpha= -\\rho \\beta\\) dans la deuxième équation il vient que nécessairement\n\\[\\rho^2 \\beta^2 - 2 \\rho \\alpha \\beta + \\beta ^2  = (1-\\rho)^2 \\beta ^2 = 1,\\]\net on obtient que \\((\\alpha,\\beta) \\in \\left\\{ \\left( \\frac{\\rho}{1-\\rho}, -\\frac{1}{1-\\rho}\\right), \\left(\\frac{-\\rho}{1-\\rho},\\frac{1}{1-\\rho}\\right) \\right\\}\\)\nOn a alors, comme indiqué dans l’énoncé\n\\[Y =  \\rho X + (1-\\rho) \\left(-\\frac{\\rho}{1-\\rho} X + \\frac{1}{1-\\rho} Y \\right),\\]\net quitte à poser \\(Z =  -\\frac{\\rho}{1-\\rho} X + \\frac{1}{1-\\rho} Y\\) qui est une normale centrée réduite indépendante de \\(X\\), on a\n\\[Y = \\rho X + (1-\\rho) Z,\\]\nqui permet de comprendre comment \\(Y\\) dépend de \\(X\\).\n\nLa décomposition sera en particulier très utile pour caculer espérance et loi conditionnelle de \\(Y\\) sachant \\(X\\).\n\n\n\n\nExercice 26\nMontrer que le vecteur aléatoire de dimension \\(3\\) de moyenne \\(m = (7,0,1)\\) et de matrice de covariances\n\\[K = \\begin{pmatrix} 10 & -1 & 4 \\\\ -1 & 1 & -1 \\\\ 4 & -1 & 2 \\end{pmatrix}\\]\nappartient presque sûrement à un hyperplan affine de \\(\\mathbb{R}^3\\) que l’on déterminera.\n\n\n\n\n\n\nNoteSolution\n\n\n\nEn résolvant \\(K x = 0\\) on trouve \\(\\ker(K) = \\mathrm{Vect} \\left\\{ \\begin{pmatrix} -1\\\\ 2\\\\  3 \\end{pmatrix}  \\right\\}\\). On a donc\n\\[\\ker(K)^{\\perp} = \\left\\{ (x,y,z) \\in \\mathbb{R}^3 : -x + 2y + z =0 \\right\\}\\]\nD’après le cours, \\(\\mathbb{P}(X - m \\in \\ker(K)^{\\perp}) =1\\), de sorte que p.s.\n\\[ X \\in \\left\\{  (x,y,z) \\in \\mathbb{R}^3 : -x + 2y + z = -6 \\right\\}.\\]"
  }
]