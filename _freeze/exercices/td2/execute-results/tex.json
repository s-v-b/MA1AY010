{
  "hash": "a6cdf842448cf99093a05fded7d27a9f",
  "result": {
    "engine": "knitr",
    "markdown": "---\n#title: \"Exercices : semaine III\"\n# subtitle: \"M1 ISIFAR MA1AY010\"\n\n# date: \"2025-09-19\"\n\nformat:\n  pdf:\n    output-file: td2.pdf\n    class: exam\n    include-in-header:\n      - text: \"\\\\lhead{{\\\\sf  Probabilités \\\\\\\\ TD 2}}\"\n  html:\n    output-file: td2.html\n\nengine: knitr\n---\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n::: {.callout-note}\n\n### TD II : Espérances et lois conditionnelles \n\n  22 Septembre 2025-26 Septembre 2025\n\n- Master I Isifar\n\n- **Probabilités** \n\n:::\n\n\n\n\n\n\n\n### Exercice 1\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.callout-note}\n\n### Cours\n\nEspérance conditionnelle par rapport à une tribu engendrée par une partition dénombrable.\n\n:::\n\n\n1.  Soit $(A_n, n \\in \\mathbb{N}^*)$ une partition de $\\Omega$ et $\\mathcal{F}= \\sigma(A_n, n \\ge 1)$ la tribu engendrée par les $A_n, n \\ge 1$. Rappelons qu'une v.a.r. $Y$ est $\\mathcal{F}$-mesurable si et seulement si il existe une suite de réls $(a_n)$ \ntelle que $Y= \\sum_{n \\ge 1} a_n \\mathbf{1}_{A_n}$.  Exprimer  $\\mathbb{E}[X \\mid \\mathcal{F}]$.\n\n2. Soient $X,Y$ deux variables i.i.d. $\\sim$ Ber$(p)$. \nOn considère $\\mathcal{G} = \\sigma(\\{X+Y=0\\})$. \nCalculer $\\mathbb{E}[X \\mid \\mathcal{G}], \\mathbb{E}[Y\\mid \\mathcal{G}]$. Les variables obtenues sont-elles toujours indépendantes?\n\n\n::: {.content-visible when-profile=\"solution\"}\n \n\n\n1. Nécessairement $Y:=\\mathbb{E}[X \\mid \\mathcal{F}]$ est $\\mathcal{F}$-mesurable et donc on peut le chercher sous la forme $\\sum_{n \\ge 1} a_n \\mathbb{I}_{A_n}$. \n\nComme $A_n \\in \\mathcal{F}$ on doit nécessairement avoir de plus \n$$ \\mathbb{E}[X \\mathbb{I}_{A_n}] = \\mathbb{E}[Y \\mathbb{I}_{A_n}] = a_n  \\mathbb{P}(A_n),$$\ncar $(A_n, n \\ge 1)$ est une partition de $\\Omega$. \nOn déduit que \n$$ a_n = \\frac{\\mathbb{E}[X \\mathbb{I}_{A_n}]}{ \\mathbb{P}(A_n)}, \\ n \\ge 1.$$\net donc \n$$ \\mathbb{E}[X \\mid \\mathcal{F}] = \\sum_{n \\ge 1}   \\frac{\\mathbb{E}[X \\mathbb{I}_{A_n}]}{ \\mathbb{P}(A_n)} \\mathbb{I}_{A_n}.$$\n1. On a $\\cG = \\{\\emptyset, \\{X = Y = 0\\}, \\{X=1\\} \\cup \\{Y=1\\}, \\Omega\\}$, et on est dans la situation précédente avec une partition à deux éléments non dégénérés \n$A_1 = \\{X=Y = 0\\}, A_2 = A_1^c = \\{X=1\\} \\cup \\{Y=1\\}$.  \n\nOn a donc \n$$mathbb{E}[X \\mid \\cG] & = & \\frac{\\mathbb{E}[X \\mathbb{I}_{A_1}]}{ \\mathbb{P}(A_1)} \\mathbb{I}_{A_1} +  \\frac{\\mathbb{E}[X \\mathbb{I}_{A_2}]}{ \\mathbb{P}(A_2)} \\mathbb{I}_{A_2} \\\\ \n& = & \\frac{2}{3}  \\mathbb{I}_{A_2} $$ \nen utilisant que $\\mathbb{E}[X \\mathbb{I}_{A_1}] =0, \\mathbb{E}[X \\mathbb{I}_{A_2}] = \\frac{1}{2},  \\mathbb{P}(A_2) = \\frac{3}{4}$.\n\n\nPar le même raisonnement ($X$ et $Y$ jouent des r\\^oles symétriques) \n$$ \\mathbb{E}[Y \\mid \\cG] = \\frac{2}{3}  \\mathbb{I}_{A_2}$$\n\nOn obtient que $\\mathbb{E}[X \\mid \\cG] = \\mathbb{E}[Y \\mid \\cG] = \\frac{2}{3} \\mathbb{I}_{A_2}$, ces variables ne sont clairement pas indépendantes. \n\n:::\n \n\n\n### Exercice 2\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note}\n\n### Cours \n\nConditionnement continu\n\n:::\n\n\nSoient $(X,Y)$ un couple de v.a. réelles intégrables de densité jointe $f$, $g : \\mathbb{R}^2 \\to \\mathbb{R}$ borélienne telle que $g(X,Y) \\in \\mathbb{L}^1$. \n \nRappeler l'expression de $\\phi, \\psi$ telles que \n$$\\mathbb{E}[g(X,Y)\\mid Y] = \\phi(Y), \\quad \\mathbb{E}[g(X,Y)|X] = \\psi(X).$$\n\n\n1. On considère $(X,Y)$ de densité jointe $f(x,y)= \\frac{1}{x} \\mathbf{1}_{\\{0 \\le y \\le x \\le 1\\}}.$ Quelle est la loi de $X$? Calculer la distribution conditionnelle $f_{Y \\mid X}$ de $Y$ sachant $X$. \nCalculer $\\mathbb{P}(X^2 +Y^2 \\le 1 |X)$, puis en déduire $\\mathbb{P}(X^2+Y^2 \\le 1)$. \n\n    Pour simplifier l'expression obtenue on pourra utiliser que $x \\to \\sqrt{1-x^2} - \\tanh^{-1}(\\sqrt{1-x^2}) = \\sqrt{1-x^2}-\\frac{1}{2} \\ln(1+\\sqrt{1-x^2}) + \\frac{1}{2} \\ln(1-\\sqrt{1-x^2})$ est une primitive de $x \\to \\frac{\\sqrt{1-x^2}}{x}$. \n\n1. Dans le cas général, montrer que $\\mathbb{E}[\\mathbb{E}[Y|X]] = \\mathbb{E}[Y]$. Que vaut $\\mathbb{E}[Y]$ dans l'exemple de la question précédente?\n\n1. Montrer, dans le cas général, que \n$$ \\mathbb{E}[\\mathbb{E}[Y|X] g(X)] = \\mathbb{E}[Yg(X)],$$\npour toute fonction $g$ telle que les deux espérances sont définies. \nQue vaut $\\mathbb{E}[Y g(X) \\mid X]$? \n\n\n\n::: {.content-visible when-profile=\"solution\"}\n \nLorsque $(X,Y)$ a densité jointe $f$, rappelons que si on pose\n$$f_{Y \\mid X}(y \\mid x) = \\begin{cases} \\frac{f(x,y)}{f_X(x)}  & \\mbox{ si } f_X(x)>0 \\\\ \n0 & \\mbox{ sinon} \\end{cases}, \\quad f_{X \\mid Y}(x \\mid y) = \\begin{cases} \\frac{f(x,y)}{f_Y(y)}  & \\mbox{ si } f_Y(y)>0 \\\\ \n0 & \\mbox{ sinon} \\end{cases}, $$\n$$ \\phi(y) = \\int_{\\mathbb{R}} g(x,y) f_{X \\mid Y}(x \\mid y) dx \\quad \\ \\psi(x) = \\int_{\\mathbb{R}} g(x,y) f_{Y \\mid X}(y \\mid x) dy, $$ alors \n$$\\mathbb{E}[g(X,Y) \\mid Y]= \\phi(Y), \\qquad \\mathbb{E}[g(X,Y) \\mid X] = \\psi(X).$$  \nMontrons par exemple la deuxième assertion : si $A \\in \\sigma(X)$, i.e. il existe $B \\in \\cB(\\mathbb{R})$ tel que $A = X^{-1}(B),$ et $\\mathbb{I}_A(\\omega) = \\mathbb{I}_B(X(\\omega))$, de sorte que (l'usage de Fubini à la troisième ligne ci-dessous est justifié car $(x,y) \\to |g(x,y)|\\mathbb{I}_B(x)$ est $ \\mathbb{P}_{(X,Y)}$-intégrable puisque $(x,y) \\to |g(x,y)|$ l'est ) :   \n$$mathbb{E}[g(X,Y) \\mathbb{I}_A] & = & \\int_{\\mathbb{R}^2} g(x,y) \\mathbb{I}_B(x) f(x,y) dx dy  \\\\ \n& = & \\int_{\\mathbb{R}^2} g(x,y) f_{Y \\mid X}(y \\mid x) f_X(x)  \\mathbb{I}_B(x) dx dy \\\\ \n& = & \\int_{\\mathbb{R}}  \\left(\\int_{\\mathbb{R}}  g(x,y) f_{Y \\mid X}(y \\mid x)\\right) \\mathbb{I}_B(x) f_X(x) dx \\\\ \n& = & \\mathbb{E}[\\psi(X) \\mathbb{I}_B(X)] = \\mathbb{E}[\\psi(X) \\mathbb{I}_A] $$ \ncomme souhaité. \n\n\n\n\n\n1. $X$ a densité $f_X$ avec $f_X$ nulle en dehors de $[0,1]$ et \n$$ f_X(x) = \\int_{\\mathbb{R}} f(x,y) dy = \\frac{1}{x} \\int_{0}^x dy = 1, 0 \\le x \\le 1,$$\non déduit que $X \\sim \\mathrm{Unif}[0,1]$. \n \nPar ailleurs  \n$$ f_{Y \\mid X} (y \\mid x) = \\frac{1}{x} \\mathbb{I}_{\\{0 \\le y \\le x\\}}.$$\n{\\em Remarque } : Cela signifie que sachant $X$, $Y \\sim \\mathrm{Unif}[0,X]$. \n\nOn en déduit \n$$  \\mathbb{P}(X^2 + Y^2 \\le 1 \\mid X) =  \\mathbb{P}(Y^2 \\le 1-X^2 \\mid X) = \\begin{cases} 1 & \\mbox{ si } X \\le \\frac{1}{\\sqrt{2}}  \\\\ \\frac{\\sqrt{1-X^2}}{X} & \\mbox{ sinon. } \\end{cases}.$$ \n\nOn a alors, puisque $X \\sim \\mathrm{Unif}[0,1]$, et en utilisant l'indication   \n$$&  \\mathbb{P}(X^2+Y^2 \\le 1)  =  \\mathbb{E}[ \\mathbb{P}(X^2+Y^2\\le 1 \\mid X)] \\\\ & = & \\frac{1}{\\sqrt{2}} + \\int_{\\frac{1}{\\sqrt{2}}}^{1} \\frac{\\sqrt{1-x^2}}{x} dx \\\\ \n& = & \\frac{1}{\\sqrt{2}} + \\left[  \\sqrt{1-x^2}-\\frac{1}{2} \\ln(1+\\sqrt{1-x^2}) + \\frac{1}{2} \\ln(1-\\sqrt{1-x^2})\\right]_{\\frac{1}{\\sqrt{2}}}^{1}\\\\ \n& = & \\frac{1}{\\sqrt{2}} - \\frac{1}{\\sqrt{2}} + \\frac{1}{2} \\ln\\left(1+ \\frac{1}{\\sqrt{2}}\\right) - \\frac{1}{2} \\ln\\left(1-\\frac{1}{\\sqrt{2}}\\right) = \\ln(\\sqrt{2}+1). $$   \n1. Comme $Y$ est intégrable on peut appliquer Fubini à la troisième ligne ci-dessous et  se servir du fait que  \n$\\ \\forall (x,y) \\in \\mathbb{R}^2, \\ f_X(x) f_{Y \\mid X}(y \\mid x) = f(x,y)$ pour voir que \n$$mathbb{E}[\\mathbb{E}[Y \\mid X]] & = & \\mathbb{E}[\\psi(X)] = \\int_{\\mathbb{R}} \\psi(x) f_X(x) dx \\\\ \n& = & \\int_{\\mathbb{R}} \\int_{\\mathbb{R}} y  f_{Y\\mid X}(y \\mid x) dy f_X(x) dx \n\\\\ & = &  \\int_{\\mathbb{R}^2} y f(x,y) dx dy = \\mathbb{E}[Y] $$  \n\nDans l'exemple précédent on a $\\mathbb{E}[Y \\mid X] = \\frac{X}{2}$ et donc \n$$\\mathbb{E}[Y]= \\mathbb{E}[\\mathbb{E}[Y \\mid X]] = \\frac{\\mathbb{E}[X]}{2} = \\frac{1}{4}.$$\n\n1. On peut appliquer Fubini à la troisième ligne ci-dessous car $\\mathbb{E}[|Y g(X)|]<\\infty$, et se servir du fait que  \n$\\ \\forall (x,y) \\in \\mathbb{R}^2, \\ f_X(x) f_{Y \\mid X}(y \\mid x) = f(x,y)$\n$$mathbb{E}[\\mathbb{E}[Y \\mid X] g(X)] & = & \\mathbb{E}[\\psi(X) g(X)] = \\int_{\\mathbb{R}} \\psi(x) g(x) f_X(x) dx \\\\ \n& = & \\int_{\\mathbb{R}} \\int_{\\mathbb{R}} y  f_{Y\\mid X}(y \\mid x) dy g(x) f_X(x) dx \n\\\\ & = &  \\int_{\\mathbb{R}^2} y g(x) f(x,y) dx dy = \\mathbb{E}[Y g(X)] $$  \nPour $B \\in \\cB(\\mathbb{R})$, quitte à considérer la fonction $\\hat{g} = g \\mathbb{I}_B$, \non déduit \n$$ \\mathbb{E}[Y g(X) \\mathbb{I}_B(X)] = \\mathbb{E}[\\psi(X) g(X) \\mathbb{I}_B]$$ de sorte que  \n$$ \\mathbb{E}[Y g(X) \\mid X] = g(X) \\mathbb{E}[Y \\mid X] = g(X) \\psi(X) $$\n\n:::\n\n\n\n### Exercice 3\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note}\n\n### Partiel passé\n\n:::\n\n\nSoient $0 \\le r \\le p \\le 1$ tels que $1-2p+r \\ge 0$.\n\nSoient $X_1, X_2$ tels que\n\n\\begin{eqnarray*} \n&&  \\mathbb{P}(X_1=1, X_2=1)=r, \\quad   \\mathbb{P}(X_1=0, X_2=1)=p-r, \\\\ \n&& \\mathbb{P}(X_1=1, X_2=0)=p-r, \\quad  \\mathbb{P}(X_1=0, X_2=0)=1-2p+r.\n\\end{eqnarray*}\n\n\n\n1. Quelle est la loi de $X_1$? celle de $X_2$?\n2. Calculer $Y = \\mathbb{E}[X_1\\mid X_2]$ et vérifier que\n$$Y= \\begin{cases} & \\frac{p-r}{1-p} \\mbox{ avec probabilité } 1-p\\\\ \n& \\frac{r}{p} \\mbox{ avec probabilité } p.\\end{cases}$$\n1. Rappelons que par définition $\\mathrm{Var}[X_1 \\mid X_2] = \\mathbb{E}[X_1^2\\mid X_2] - \\mathbb{E}[X_1\\mid X_2]^2$. Montrer que \n$$\\mathrm{Var}[X_1 \\mid X_2] = \\left( \\frac{p-r}{1-p} - \\left(\\frac{p-r}{1-p}\\right)^2\n\\right) \\mathbf{1}_{\\{X_2=0\\}} + \\left( \\frac{r}{p} - \\left(\\frac{r}{p}\\right)^2 \\right)\n\\mathbf{1}_{\\{X_2=1\\}}.$$\n1. Que vaut $\\mathrm{Var}(\\mathbb{E}[X_1\\mid X_2])$? $\\mathbb{E}[\\mathrm{Var}[X_1\\mid X_2]]$?\nVérifier qu'on a bien\n$$\\mathrm{Var}(X_1) = \\mathrm{Var}(\\mathbb{E}[X_1\\mid X_2]) + \\mathbb{E}[\\mathrm{Var}[X_1\\mid X_2]].$$\n\n\n\n\n::: {.content-visible when-profile=\"solution\"}\n \n\n\n1. $X_1$, comme $X_2$, prend ses valeurs dans $\\{0,1\\}$. \nOn a $ \\mathbb{P}(X_1=1) = r+ p-r$ de sorte que $X_1 \\sim \\mathrm{Ber}(p)$, \net $ \\mathbb{P}(X_2=1) = r+ p-r$ de sorte qu'également $X_2 \\sim \\mathrm{Ber}(p)$. \n1. On a (cf EF3)  \n$$mathbb{E}[X_1 \\mid X_2] & = & \\frac{\\mathbb{E}[X_1 \\mathbb{I}_{\\{X_2 = 1\\}}]}{ \\mathbb{P}(X_2 =1)} \\mathbb{I}_{\\{X_2=1\\}} +  \\frac{\\mathbb{E}[X_1 \\mathbb{I}_{\\{X_2 = 0\\}}]}{ \\mathbb{P}(X_2 =0)} \\mathbb{I}_{\\{X_2=0\\}}  \\\\ & = & \\frac{r}{p} \\mathbb{I}_{\\{X_2=1\\}} + \\frac{p-r}{1-p} \\mathbb{I}_{\\{X_2=0\\}} $$ \n\nRemarquons que $ \\mathbb{P}(Y = \\frac{r}{p}) =  \\mathbb{P}(X_2=1) = p,  \\mathbb{P}(Y = \\frac{p-r}{1-p}) =  \\mathbb{P}(X_2=0) = 1-p$. \nAutrement dit $Y$ est une variable qui prend deux valeurs, $\\frac{r}{p}$ sur l'événement $\\{X_2=1\\}$ (qui est bien de probabilité $p$) et $\\frac{p-r}{1-p}$ sur l'événement complémentaire (qui est bien de probabilité $1-p$). \n\n\n1. On a p.s. $X_1^2 = X_1$ puisque $X_1$ prend ses valeurs dans $\\{0,1\\}$ et donc $\\mathbb{E}[X_1^2 \\mid X_2 ]= \\mathbb{E}[X_1 \\mid X_2]$. Par ailleurs un rapide calcul assure que \n$$ \\mathbb{E}[X_1 \\mid X_2]^2 =  \\frac{r^2}{p^2} \\mathbb{I}_{\\{X_2=1\\}} + \\frac{(p-r)^2}{(1-p)^2} \\mathbb{I}_{\\{X_2=0\\}},$$\net on obtient donc la formule souhaitée.  \n1.\tD'après la question 2, \n$ Y = c + \\left|\\frac{r}{p}- \\frac{p-r}{1-p}\\right| \\xi,$ où $\\xi \\sim \\mathrm{Ber}(p)$. \nOn obtient donc \n$$mathrm{Var}(Y) = \\left(\\frac{r}{p}- \\frac{p-r}{1-p}\\right)^2 p(1-p) & = &  \\frac{r^2(1-p)}{p} + \\frac{p (p-r)^2}{1-p} - 2r (p-r) \\\\ & = & \n\\frac{r^2}{p} - r^2 + \\frac{(p-r)^2}{1-p} - (p-r)^2 - 2r (p-r) \\\\ \n& = & \\frac{r^2}{p} + \\frac{(p-r)^2}{1-p} -(r+(p-r))^2 $$  \nPar ailleurs d'après la question 3, \n$$ \\mathbb{E}[\\mathrm{Var}[X_1 \\mid X_2]]) = \\left( \\frac{p-r} - \\frac{(p-r)^2}{1-p}\n\\right)  + \\left( r - \\frac{r^2}{p} \\right) = p - \\frac{(p-r)^2}{1-p} - \\frac{r^2}{p}.$$\n\nOn a donc \n$$ \\mathrm{Var}(Y) + \\mathbb{E}[\\mathrm{Var}[X_1 \\mid X_2]] =  p - p^2= \\mathrm{Var}[X_1].$$\n\n\n:::\n\n\n### Exercice 4\n\n\n::: {.cell}\n\n:::\n\n\n\nSoit $(X_n)$ une suite de v.a. .i.i.d intégrables, et $S_n = \\sum_{i=1}^n X_i$. \n\n\n1. Que valent $\\mathbb{E}[X_1\\mid X_2], \\mathbb{E}[S_n \\mid X_1], \\mathbb{E}[S_n \\mid S_{n-1}]?$ \n1. Montrer que si les paires de variables $(X,Z)$, $(Y,Z)$ ont la même loi jointe, alors pour toute fonction réelle positive (ou satisfaisant une condition d'intégrabilité),  $\\mathbb{E}[f(X)\\mid Z] = \\mathbb{E}[f(Y)\\mid Z]$. En déduire $\\mathbb{E}[X_1 \\mid S_n]$.\n   \n::: {.content-visible when-profile=\"solution\"}\n \n\n\n1. Puisque $X_1$ est indépendant de $X_2$ on a (EF2)  \n$$\\mathbb{E}[X_1 \\mid X_2] = \\mathbb{E}[X_1].$$\nDe même pour $i \\ge 2$ $\\mathbb{E}[X_i \\mid X_1] = \\mathbb{E}[X_i] = \\mathbb{E}[X_1],$\ntandis que (EF1) : $\\mathbb{E}[X_1 \\mid X_1] = X_1$. On conclut en faisant usage de la linéarité de $\\mathbb{E}[\\cdot \\mid \\cdot]$ que \n$$\\mathbb{E}[S_n \\mid X_1] = X_1 + (n-1) \\mathbb{E}[X_1].$$\nPar un raisonnement similaire, $\\mathbb{E}[S_{n-1} \\mid S_{n-1}] = S_{n-1}$, tandis que $X_n$ étant indépendant de $S_{n-1}$ on a $\\mathbb{E}[X_n \\mid S_{n-1}] = \\mathbb{E}[X_1]$. En utilisant que $S_n = S_{n-1}+X_n$, la linéarité de $\\mathbb{E}[\\cdot \\mid \\cdot]$ permet de conclure que \n$$\\mathbb{E}[S_n \\mid S_{n-1}] = S_{n-1} + \\mathbb{E}[X_1].$$  \n1. Supposons que $ \\mathbb{P}_{(X,Z)} =  \\mathbb{P}_{(Y,Z)}$, et que $X \\in \\mathbb{L}^1$, notons $T= \\mathbb{E}[X \\mid Z]$ (qui est, par définition, $\\sigma(Z)$-mesurable). Soit $A \\in \\sigma(Z)$, de sorte que $A = Z^{-1}(B)$ pour un $B$ dans la tribu dont on a muni l'espace dans lequel $Z$ prend ses valeurs. \nAlors \n$$ \\mathbb{E}[Y \\mathbb{I}_A] = \\mathbb{E}[Y \\mathbb{I}_B(Z)] = \\mathbb{E}[X \\mathbb{I}_B(Z)] = \\mathbb{E}[X \\mathbb{I}_A] = \\mathbb{E}[T \\mathbb{I}_A],$$\ndonc $T = \\mathbb{E}[Y \\mid Z]$. \n\nOn peut faire le même raisonement avec $f(X), f(Y)$, ou simplement remarquer que \n$\\mathbb{P}_{(X,Z)} =  \\mathbb{P}_{(Y,Z)} \\ \\Rightarrow \\  \\mathbb{P}_{(f(X),Z)} =  \\mathbb{P}_{(f(Y),Z)}$. \n\nComme les $X_i, 1 \\le i \\le n$ jouent des r\\^oles parfaitement symétriques dans $S_n$ puisqu'elles sont i.i.d, on a \n$\\mathbb{P}_{(X_i,S_n)} =  \\mathbb{P}_{(X_1, S_n)}$ pour tout $1 \\le i \\le n$. On déduit de ce qui précède que \n$\\mathbb{E}[X_1 \\mid S_n] = \\mathbb{E}[X_i \\mid S_n], 1 \\le i \\le n$. \nMais alors par linéarité \n$$ S_n= \\mathbb{E}[S_n \\mid S_n]  = \\sum_{i=1}^n  \\mathbb{E}[X_i \\mid S_n] = n \\mathbb{E}[X_1 \\mid S_n],$$\net on conclut que $\\mathbb{E}[X_1 \\mid S_n] = \\frac{S_n}{n}$.    \n\n\n:::\n\n\n### Exercice 5\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note}\n\n\n### (Examen passé)\n\n:::\n\nSoit $(X_n, n \\ge 0)$ une suite de variables i.i.d, avec $X_1 \\sim \\text{Ber}(1/2)$. On pose $S_n = \\sum_{i=1}^n (X_i -1/2)$, $\\mathcal{F}_n= \\sigma(X_1,...,X_n)$. \n\nCalculer $\\mathbb{E}[S_n \\mid \\mathcal{F}_5]$ en fonction de $n$. Quelle est la loi de cette variable aléatoire?  \n\n::: {.content-visible when-profile=\"solution\"}\n Si $n \\le 5$, $S_5$ est $\\mathcal{F}_5$ mesurable et donc (EF1) ;  \n$$ \\mathbb{E}[S_n \\mid \\mathcal{F}_5] = S_n \\quad \\forall n \\le 5$$. \nComme dans l'exercice précédent, puisque $X_i$ est indépendant de $\\mathcal{F}_5$ pour tout $i \\ge 6$, on a \n$$ \\mathbb{E}[(X_i-1/2) \\mid \\mathcal{F}_5] = \\mathbb{E}[X_i-1/2]= 0.$$\nDonc \n$$ \\mathbb{E}[S_n \\mid \\mathcal{F}_5] = S_5 \\ \\ \\forall n \\ge 5.$$\nEnfin $S_k+\\frac{k}{2} \\sim \\mathrm{Bin}(k,1/2)$.  \n\n:::\n\n\n### Exercice 6\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note}\n\n###  Partiel passé \n\n:::\n\nSoient $\\{\\mathbf{e}_i, i \\in \\mathbb{N} \\}$ des variables i.i.d exponentielles de paramètre $1$.\nPour $n \\in \\mathbb{N}^*$ on note $S_n := \\sum_{i=1}^n \\mathbf{e}_i$. \n\n\n\n\n1. On note $f_n$ la fonction de densité de la variable $S_n$. \nMontrer que pour tout $t \\ge 0$ \n$$ f_n(t) = \\frac{t^{n-1}}{(n-1)!} \\exp(-t).$$ \n1. Pour $t >0, n \\in \\mathbb{N}^*$, que vaut $\\mathbb{P}(S_n \\le t)$? \n1. On fixe $t>0$ et on suppose $X_t \\sim \\mathrm{Poisson}(t)$. Que vaut \n$\\mathbb{P}(X_t \\ge n)$, pour $n \\in \\mathbb{N}^*$? \n1. Sur la demi-droite $\\mathbb{R}_+$ on place les points $S_1, S_2, S_3,...$. \nOn note $N_t$ le nombre de ces points qui tombent dans l'intervalle $[0,t]$. \nExprimer l'événement $\\{N_t \\ge n\\} = \\{S_n \\le t\\}$.\nDéterminer la loi de $N_t$ à l'aide des questions préc\\'dentes.  \n1.  Montrer que, conditionnellement à $\\{N_t=1\\}$, la loi de $\\mathbf{e}_1$ est uniforme sur $[0,t]$. \n1.  Conditionnellement à $\\{N_t=2\\}$, quelle est la loi du vecteur $(\\mathbf{e}_1; \\mathbf{e}_2)$?  \n\n\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n\n\n1. On montre l'assertion souhaitée par récurrence sur $n \\in \\mathbb{N}^*$. L'assertion est trivialement vérifiée pour $n=1$ puisqu'on reconna\\^\\i t en $f_1$ la densité d'une $\\exp(1)$ et donc de $S_1 = \\mathbf{e}_1$.\n\nSoit $n \\in \\mathbb{N}^*$, supposons que $S_n$ a densité $f_n$, comme $(S_n, \\mathbf{e}_{n+1})$ sont indépendantes, le couple a densité \n$$ g(s,t) = f_n(s) \\exp(-t) \\mathbb{I}_{s \\ge 0, t \\ge 0}$$ et donc  \n$$mathbb{E}[\\phi(S_{n+1})] & = & \\mathbb{E}[\\phi(S_n + \\mathbf{e}_{n+1})] \\\\ \n& = & \\int_{\\mathbb{R}_+^2} \\phi(s+t)  \\frac{s^{n-1}}{(n-1)!} \\exp(-s-t) ds dt $$ \nAvec $(u,v)=(s+t,t)$ on a un $\\mathcal{C}^1$-difféomorphisme de $\\mathbb{R}_+^2$ dans $\\{(u,v) \\in \\mathbb{R}_+^2 : v \\le u\\}$, de jacobien $1$, et donc par changement de variables, on obtient comme souhaité :  \n$$ \\mathbb{E}[\\phi(S_{n+1}] = \\int_{\\mathbb{R}_+} du \\phi(u) \\exp(-u) \\left(\\int_0^u \\frac{(u-v)^{n-1}}{(n-1)!} dv\\right) = \\int_{\\mathbb{R}_+} \\phi(u) f_{n+1}(u) du$$    \n \n\n{\\em Remarque } Avec des exponentielles indépendantes de paramètre commun $\\lambda$, on obtient la densité d'une $\\Gamma(n, \\lambda)$ pour la somme, ici on est dans le cas $\\lambda=1$. \n\n1. On a \n$$\\mathbb{P}(S_n \\ge t) & = & \\int_{t}^{\\infty} f_n(u) du $$ \nCette intégrale se calcule, en fonction de $n, t$, au moyen d'intégrations par parties successives : \n$$ \\int_t^{\\infty} f_n(u) du = \\left[ \\frac{u^{n-1}}{(n-1)!} \\right]_t^{\\infty} + \\int_t^{\\infty} f_{n-1}(u) du.$$ \nComme $\\int_t^{\\infty} f_1(u) du = \\exp(-t)$, une récurrence immédiate fournit donc que \n$$ \\int_t^{\\infty} f_n(u) du = \\exp(-t) \\sum_{k=0}^{n-1} \\frac{t^{n-1}}{(n-1)!}.$$ \n\n1. Soit $n \\in \\mathbb{N}^*$, on a \n$$ \\mathbb{P}(X_t  \\ge n) = \\exp(-t) \\sum_{k \\ge n} \\frac{t^k}{k!}$$\net on remarque d'après la question précédente que ceci vaut précisément $1- \\mathbb{P}(S_n \\ge t) =  \\mathbb{P}(S_n \\le t)$ (pour la dernière égalité on a utilisé que $S_n$ possède une densité pour assurer que $ \\mathbb{P}(S_n=t) =0$). \n\n1. Par définition $N_t \\ge n$ ssi au moins $n$ points parmi $\\{S_1,S_2,\\dots,S_n, \\dots\\}$ tombent dans l'intervalle $[0,t]$. \nComme $(S_k, k \\ge 0)$ est p.s. croissante ceci se produit (p.s.) lorsque $S_n \\le t$ et on on déduit que \n$$\\{N_t \\ge n\\} = \\{S_n \\le t\\}$$\n\nLa variable $N_t$ est à valeurs dans $\\mathbb{N}$, et on a pour tout $n \\in \\mathbb{N}$ (cf la question précédente pour $n\\in \\mathbb{N}^*$, on a ajouté la cas trivial $n=0$), \n$$  \\mathbb{P}(N_t \\ge n) =  \\mathbb{P}(X_t \\ge n). $$\nMais ces valeurs caractérisent la fonction de répartition de $N_t$, et donc la loi de $N_t$, et on conclut que $N_t \\sim \\mathrm{Poisson}(t)$. \n\n\n1. On a $\\{N_t=1\\} = \\{\\mathbf{e}_1 \\le t, \\mathbf{e}_2 > t- \\mathbf{e}_1\\}$.\n\nPar ailleurs, $(\\mathbf{e}_1, \\mathbf{e}_2)$ sont indépendantes et possèdent donc la densité jointe \n$$ f_{(\\mathbf{e}_1,\\mathbf{e}_2}(u,v) = \\exp(-u)\\exp(-v) \\mathbb{I}_{\\{u \\ge 0\\}} \\mathbb{I}_{\\{v \\ge 0\\}}$$\nPour $\\phi : \\mathbb{R} \\to \\mathbb{R}_+$ borélienne, on en déduit que \n$$& \\mathbb{E}[\\phi(\\mathbf{e}_1) \\mid N_t=1]  =  \\frac{\\mathbb{E}[\\phi(\\mathbf{e}_1) \\mathbb{I}_{\\{N_t = 1\\}}]}{ \\mathbb{P}(N_t = 1)} \\\\ & =&\n \\frac{\\mathbb{E}[\\phi(\\mathbf{e}_1) \\mathbb{I}_{\\{\\mathbf{e}_1 \\le t, \\mathbf{e}_2 > t-\\mathbf{e}_1\\}}]}{t \\exp(-t)} \n\\\\ & = & \\frac{\\exp(t)}{t} \\int_{\\mathbb{R}} \\int_{\\mathbb{R}} \\phi(u) \\exp(-u)\\exp(-v) \\mathbb{I}_{\\{0 \\le u \\le t\\}} \\mathbb{I}_{\\{0\\le t-u < v\\}} du dv \n\\\\ & = & \\frac{\\exp(t)}{t} \\int_{\\mathbb{R}} du \\phi(u) \\exp(-u) \\mathbb{I}_{[0,t]}(u) \\int_{t-u}^{\\infty} \\exp(-v) dv \n\\\\ & = & \\frac{\\exp(t)}{t} \\int_{\\mathbb{R}} du \\phi(u) \\exp(-u) \\mathbb{I}_{[0,t]}(u) \\exp(u-t) \\\\\n& = & \\int_{\\mathbb{R}}\\phi(u) \\frac{\\mathbb{I}_{[0,t]}(u)}{t} du, \n $$ \noù on a utilisé Fubini-Tonelli à la troisième ligne ci-dessus. \n\nOn conclut, gr\\^ace au théorème de caractérisation habituel, que la loi conditionnelle de $\\mathbf{e}_1$ sachant $\\{N_t=1\\}$ est $\\mathrm{Unif}[0,t]$, \n1. On effectue un raisonnement similaire à celui de la question qui précède. \nOn a \n$$\\{N_t=2\\} = \\{S_1 \\le t, \\mathbf{e}_3 > t- S_1\\}= \\left\\{\\mathbf{e}_1 \\le t, \\mathbf{e}_2 \\le t-\\mathbf{e}_1, \\mathbf{e}_3 > t - (\\mathbf{e}_1+\\mathbf{e}_2)\\right\\}.$$\n\nPar ailleurs, $(\\mathbf{e}_1, \\mathbf{e}_2,\\mathbf{e}_3)$ sont indépendantes et possèdent donc la densité jointe \n$$ f_{(\\mathbf{e}_1,\\mathbf{e}_2,\\mathbf{e}_3}(u,v,w) = \\exp(-u)\\exp(-v)\\exp(-w) \\mathbb{I}_{\\{u \\ge 0\\}} \\mathbb{I}_{\\{v \\ge 0\\}}\\mathbb{I}_{\\{w \\ge 0\\}}.$$\nPour $\\phi : \\mathbb{R}^2 \\to \\mathbb{R}_+$ borélienne, on en déduit que \n$$&  \\mathbb{E}[\\phi(\\mathbf{e}_1,\\mathbf{e}_2) \\mid N_t=2]  =  \\frac{\\mathbb{E}[\\phi(\\mathbf{e}_1, \\mathbf{e}_2) \\mathbb{I}_{\\{N_t = 2\\}}]}{ \\mathbb{P}(N_t = 2)} \\\\ & =&\n \\frac{\\mathbb{E}[\\phi(\\mathbf{e}_1, \\mathbf{e}_2) \\mathbb{I}_{\\{\\mathbf{e}_1 \\le t, \\mathbf{e}_2 \\le t-\\mathbf{e}_1, \\mathbf{e}_3 > t - (\\mathbf{e}_1+\\mathbf{e}_2)\\}}]}{\\frac{t^2}{2} \\exp(-t)} \n\\\\ & = & \\frac{2\\exp(t)}{t^2} \\int_{\\mathbb{R}^3}  \\phi(u,v) \\exp(-u-v)\\exp(-w) \\mathbb{I}_{\\{0\\le u \\le u+v \\le t\\}} \\mathbb{I}_{\\{w>t-(u+v)\\}} du dv dw \n\\\\ & = & \\frac{2\\exp(t)}{t^2} \\int_{\\mathbb{R}^2} du dv \\phi(u,v) \\exp(-u-v) \\mathbb{I}_{\\{0\\le u \\le u+v \\le t\\}} \\int_{t-(u+v)}^{\\infty} \\exp(-w) dw \n\\\\ & = & \\frac{2\\exp(t)}{t^2} \\int_{\\mathbb{R}} du \\phi(u) \\exp(-u-v) \\mathbb{I}_{\\{0\\le u \\le u+v \\le t\\}} \\exp(u+v-t) = \\int_{\\mathbb{R}}\\phi(u,v) \\frac{2\\mathbb{I}_{\\{0\\le u \\le u+v \\le t\\}}}{t^2} du, \n $$ \net on conclut que la loi conditionnelle de $(\\mathbf{e}_1, \\mathbf{e}_2)$ sachant $\\{N_t=2\\}$ a pour densité \n$\\frac{2\\mathbb{I}_{\\{u \\ge 0\\}} \\mathbb{I}_{\\{v \\ge 0\\}} \\mathbb{I}_{\\{u+v \\le t\\}}}{t^2}$. \n\nAutrement dit, la loi conditionnelle de $(\\mathbf{e}_1, \\mathbf{e}_2)$ sachant $\\{N_t=2\\}$ est uniforme sur le triangle $\\{(u,v) \\in [0,t]^2 : u+v \\le t\\}$. \n\n:::\n\n### Exercice 7\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note}\n\n###  CC2 2023\n\n:::\n\nOn considère \n$$ X \\sim \\mathcal{N} \\left( \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 2 & 1 & -1 & -1 \\\\ 1 & 2 & -1 & 0 \\\\ -1 & -1 & 3 & -1 \\\\ -1 & 0 & -1 & 5 \\end{pmatrix}\\right).$$ \n\n\n\n\n1. Calculer $\\mathbb{E}[X_3 \\mid X_4]$, et déterminer la loi conditionnelle de $X_3$ sachant $X_4$.  \n2. On pose $A = \\begin{pmatrix} 2  & 1 \\\\ 1 & 2 \\end{pmatrix}$, $B= \\begin{pmatrix} -1 & -1 \\\\ -1 & 0 \\end{pmatrix}$. Calculer $BA^{-1}$, puis vérifier que \n$$B A^{-1} B^T = \\begin{pmatrix} \\frac{2}{3} & \\frac{1}{3} \\vspace{0.1cm} \\\\ \\frac{1}{3} & \\frac{2}{3}\\end{pmatrix}.$$\n1. Déterminer $\\mathbb{E}\\left[\\begin{pmatrix} X_3 \\\\ X_4 \\end{pmatrix} \\ \\bigg| \\ \\begin{pmatrix} X_1 \\\\ X_2 \\end{pmatrix}\\right]$. \net la loi conditionnelle de $\\begin{pmatrix} X_3 \\\\ X_4 \\end{pmatrix}$ sachant $\\begin{pmatrix} X_1 \\\\ X_2 \\end{pmatrix}$.\n \n \n\n::: {.content-visible when-profile=\"solution\"}\n\n\n\n1. D'après l'énoncé $\\displaystyle{\\begin{pmatrix} X_3 \\\\ X_4 \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix}   3 & -1 \\\\  -1 & 5 \\end{pmatrix}\\right)}$, et donc d'après la formule du cours, sachant $X_4$, $X_3 \\sim \\mathcal{N}\\left(1 -\\frac{X_4}{5},\\frac{14}{5}\\right)$.\nEn particulier $\\mathbb{E}[X_3 \\mid X_4] = 1 - \\frac{X_4}{5}$. \n1. On a $A^{-1} = \\begin{pmatrix} \\frac{2}{3} & -\\frac{1}{3} \\\\ -\\frac{1}{3} & \\frac{2}{3} \\end{pmatrix}$, et donc \n$$ B A^{-1} = \\begin{pmatrix} -\\frac{1}{3} & -\\frac{1}{3} \\\\ -\\frac{2}{3} & \\frac{1}{3} \\end{pmatrix},$$ \net \n$$ BA^{-1}B^T = \\begin{pmatrix} -\\frac{1}{3} & -\\frac{1}{3} \\\\ -\\frac{2}{3} & \\frac{1}{3} \\end{pmatrix}  \\begin{pmatrix} -1 & -1 \\\\ -1 & 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3} & \\frac{1}{3} \\\\ \\frac{1}{3} & \\frac{2}{3}\\end{pmatrix}, $$\ncomme souhaité. \n\n1. D'après le cours, sachant $\\begin{pmatrix} X_1 \\\\ X_2 \\end{pmatrix}$, la loi conditionnelle de $\\begin{pmatrix} X_3 \\\\ X_4 \\end{pmatrix}$ est gaussienne, centrée en \n$$\\mathbb{E}\\left[\\begin{pmatrix} X_3 \\\\ X_4 \\end{pmatrix} \\ \\bigg| \\ \\begin{pmatrix} X_1 \\\\ X_2 \\end{pmatrix}\\right] = BA^{-1} \\begin{pmatrix} X_1 +1 \\\\ X_2 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}  = \\begin{pmatrix} \\frac{2}{3} - \\frac{1}{3}X_1 - \\frac{1}{3} X_2 \\\\ - \\frac{2}{3} -\\frac{2}{3}X_1 + \\frac{1}{3}X_2 \\end{pmatrix},$$\net de matrice de covariances \n$$ \\begin{pmatrix} 3 & -1 \\\\ -1 & 5 \\end{pmatrix} - BA^{-1}B^T =  \\begin{pmatrix} \\frac{7}{3} & \\frac{-4}{3} \\vspace{0.1cm}\\\\ \\frac{-4}{3} & \\frac{13}{3}\\end{pmatrix}.$$ \n \n \n:::\n\n\n### Exercice 8\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.callout-note}\n\n(Partiel passé)\n\n:::\n\nSoit $(X_1,X_2,X_3) \\sim \\mathcal{N}(\\mu, M)$ où\n$$\\mu = \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}, \\quad M = \\begin{pmatrix} 2 & 1 & 0 \\\\ 1 & 2 & 1 \\\\ 0 & 1 & 2 \\end{pmatrix}.$$\n\n\n\n1. Quelle est la loi du couple $(X_1,X_2)$?\n1. Déterminer $\\alpha$ un réel tel que\n$Y = \\alpha X_1 + X_2 $ est indépendante de $X_1$. Que vaut $\\mathbb{E}[Y]$? $\\text{Var}(Y)$? \n1. En déduire $\\mathbb{E}[X_2 \\mid X_1]$. Quelle est la loi conditionnelle de $X_2$ sachant $X_1$?\n1. Déterminer un réel $\\beta$ tels que $Z=\\beta X_1 + X_3$ est\nindépendante de $X_1$. En déduire\n$$ \\mathbb{E}[X_3 \\mid X_1], \\quad \\mathbb{E}[X_3^2 \\mid X_1].$$\n1. Calculer $\\mathbb{E}\\left[X_1^2X_2 + X_3^2 X_1 \\mid X_1\\right]$.\n\n\n\n::: {.content-visible when-profile=\"solution\"}\n \n\n\n1. $(X_1,X_2)$ est un vecteur gaussien (comme image d'un vecteur gaussien par une application linéaire, en l'occurrence une projection), et on lit directement sur $\\mu, M$ moyennes et covariances. On a donc $(X_1,X_2) \\sim \\mathcal{N}(m,A)$, avec $m = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ et $A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2\\end{pmatrix}$. \n1. Quel que soit $\\alpha \\in \\mathbb{R}$, $(X_1,Y)$ est un vecteur gaussien comme image du vecteur gaussien $(X_1,X_2)$ par l'application linéaire de matrice $\\begin{pmatrix} 1 & 0 \\\\ \\alpha & 1\\end{pmatrix}$. \n\nPar théorème caractérisant l'indépendance des coordonnées d'un vecteur gaussien, $X_1$ est indépendant de $Y$ ssi $\\mathrm{Cov}(X_1,Y)=0$. \nOr \n$$ \\mathrm{Cov}(X_1,Y)= \\alpha \\mathrm{Var}[X_1] + 1 = 2\\alpha +1,$$\net donc on a l'indépendance souhaitée lorsque $\\alpha=-\\frac{1}{2}$.\n1. Puisque $-\\frac{1}{2}X_1+X_2$ est indépendant de $X_1$ on a donc \n$$mathbb{E}[X_2 \\mid X_1 ] & = & \\mathbb{E}\\left[\\frac{1}{2}X_1 +\\left(-\\frac{1}{2}X_1+X_2\\right) \\mid X_1\\right] \\\\ & = & \\frac{1}{2}X_1 + \\mathbb{E}\\left[-\\frac{1}{2}X_1 + X_2\\right] = \\frac{1}{2}X_1 -\\frac{1}{2}.$$  \nPar ailleurs, $\\mathrm{Var}\\left(-\\frac{1}{2}X_1+X_2\\right) = \\frac{1}{4} \\mathrm{Var}(X_1) - \\mathrm{Cov}(X_1,X_2) + \\mathrm{Var}(X_2) = \\frac{3}{2}$ et donc $-\\frac{1}{2}X_1+X_2 \\sim \\mathcal{N}\\left(-\\frac{1}{2}, \\frac{3}{2}\\right)$. \nL'écriture $X_2 = \\frac{1}{2}X_1 + \\left(-\\frac{1}{2}X_1+X_2\\right)$ permet donc d'affirmer que sachant $X_1$, la loi conditionnelle de $X_2$ est \n$\\mathcal{N}\\left(\\frac{1}{2}X_1-\\frac{1}{2}, \\frac{3}{2} \\right)$. \n1. Ici $\\mathrm{Cov}(X_1,X_3) = 0$ et donc $X_1$ et $X_3$ sont indépendants, il suffit donc de prendre $\\beta=0$.\nOn trouve donc ici que \n$$ \\mathbb{E}[X_3 \\mid X_1] = \\mathbb{E}[X_3] = -1$$\net que sachant $X_1$, la loi conditionnelle de $X_3$ reste la loi de $X_3$, i.e. $\\mathcal{N}\\left(-1, 2 \\right)$. \nPar ailleurs\n$$mathbb{E}[X_3^2 \\mid X_1] & = &  \\mathbb{E}[X_3^2] = \\mathbb{E}[X_3]^2 + \\mathrm{Var}[X_3] \\\\ \n& = &  1 + 2 = 3.$$ \n1. On a, en utilisant les propriétés de l'espérance conditionnelle et les question précédentes, puis en simplifiant  \n$$mathbb{E}[X_1^2X_2 + X_3^2X_1 \\mid X_1] & = & X_1^2 \\mathbb{E}[X_2 \\mid X_1] + X_1 \\mathbb{E}[X_3^2\\mid X_1 ] \\\\ \n & = & X_1^2 \\left(\\frac{1}{2}X_1 -\\frac{1}{2}\\right) + 3X_1  \\\\ \n& = & \\frac{1}{2}X_1^3 -\\frac{1}{2} X_1^2 + 3X_1 $$ \n \n:::\n\n### Exercice 9\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note}\n\n### Examen passé\n\n:::\n\n\nSoit $(X_1,X_2,X_3) \\sim \\mathcal{N}(\\mu,M)$, où \n$$ \\mu = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\qquad M=  \\begin{pmatrix} 1 & 1/2 & 2 \\\\ 1/2 & 1 & 1 \\\\ 2 & 1 & 3 \\end{pmatrix}.$$ \nCalculer $\\mathbb{E}[X_1+2X_2 \\mid X_3].$ Quelle est la loi conditionnelle de $X_1+2X_2$ sachant $X_3$? \n\n::: {.content-visible when-profile=\"solution\"}\n \nComme dans l'exercice précédent on peut commencer par chercher $\\alpha$ tel que $Y=\\alpha X_3 + X_1 + 2X_2$ est indépendant de $X_3$. \nBien s\\^ur, $(Y,X_3)$ est un vecteur gaussien puisque c'est l'image de $(X_1,X_2,X_3)$ par une application linéaire. \nDonc on a l'indépendance voulue lorsque $ \\mathrm{Cov}(Y,X_3) =0$, i.e. lorsque \n$$ 0 = \\alpha \\mathrm{Var}(X_3) + \\mathrm{Cov}(X_1,X_3)+2 \\mathrm{Cov}(X_2,X_3) = 3\\alpha + 2 + 2,$$\net donc il faut prendre $\\alpha = -\\frac{4}{3}$. \n\nOn a alors \n$$ \\mathbb{E}[X_1+2X_2 \\mid X_3] = \\frac{4}{3}X_3 + \\mathbb{E}[-\\frac{4}{3}X_3 + X_1 + 2X_2] = \\frac{4}{3}X_3+2.$$\n\nPar ailleurs, \n$$mathrm{Var}(Y) & = & \\frac{16}{9} \\mathrm{Var}(X_3) + \\mathrm{Var}(X_1) + 4 \\mathrm{Var}(X_2) - \\frac{8}{3} \\mathrm{Cov}(X_3,X_1) - \\frac{16}{3} \\mathrm{Cov}(X_2,X_3) + 4 \\mathrm{Cov}(X_1,X_2)  \\\\ \n& = & \\frac{16}{3} + 1 + 4 - \\frac{16}{3} -\\frac{16}{3} + 2 = \\frac{5}{3} $$ \net on déduit que sachant $X_3$, la loi conditionnelle de $X_1+2X_2$ est $\\mathcal{N}\\left( \\frac{4}{3}X_3+2, \\frac{5}{3} \\right)$.   \n\n\n{\\em Alternativement}, on peut utiliser les formules du cours. D'abord, avec $K=  \\begin{pmatrix} 1 & 2 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$\n$$\\begin{pmatrix} X_1+2X_2 \\\\ X_3 \\end{pmatrix} = K \\begin{pmatrix} X_1 \\\\ X_2 \\\\X_3 \\end{pmatrix} \\sim \\mathcal{N}\\left( K \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, K M K^T \\right) \\sim \\mathcal{N} \\left( \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 7 & 4 \\\\ 4 & 3 \\end{pmatrix}\\right). $$\nOn peut alors appliquer la méthode précédente à ce vecteur, ou la formule du cours pour le conditionnement avec $\\theta = X_1+2X_2, \\xi=X_3$, \n$\\mu_{\\theta} = 2, \\mu_{\\xi} = 0, \\ M_{\\theta \\xi} = M_{\\xi \\theta} = 4, M_{\\xi \\xi} = 3, M_{\\theta \\theta}=7$, pour obtenir \n$$ \\mathbb{E}[X_1+2X_2 \\mid X_3] = \\mu_{\\theta} + M_{\\theta \\xi} M_{\\xi \\xi}^{-1} (\\xi-\\mu_{\\xi}) = 2 + \\frac{4}{3} X_3,$$ \net \n$$ \\mathrm{Var}[X_1+2X_2 \\mid X_3] = M_{\\theta \\theta} - M_{\\theta \\xi} M_{\\xi \\xi}^{-1} M_{\\xi \\theta} = 7 - \\frac{16}{3} = \\frac{5}{3}.$$ \n\n:::\n\n### Exercice 10\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note}\n\n### (CC2 2023) \n\n:::\n\nOn suppose dans cet exercice que $(X,Y)$ est un couple de variables aléatoires tel que pour toute $\\phi : \\mathbb{R}^2\\to \\mathbb{R}_+$ borélienne, \n$$ \\mathbb{E}[\\phi(X,Y)] = \\sum_{n \\ge 1} \\frac{2}{3^{n}\\sqrt{2\\pi n}}  \\int_{\\mathbb{R}} \\phi(n,y) \\exp\\left(-\\frac{y^2}{2n}\\right) dy.$$ \n\n\n\n1. Montrer que $X \\sim \\mathrm{Geom}(2/3)$. \n1. Vérifier que pour une fonction $f : \\mathbb{R} \\to \\mathbb{C}$ telle que $f(Y) \\in \\mathbb{L}^1$, on a  \n$$ \\mathbb{E}[f(Y) \\mid X] = \\sum_{n \\ge 1} \\left(\\int_{\\mathbb{R}} \\frac{1}{\\sqrt{ 2\\pi n}} f(y) \\exp\\left(-\\frac{y^2}{2n} \\right) dy \\right) \\mathbb{I}_{\\{X=n\\}}$$ \n%1. En déduire que pour tout $k \\in \\mathbb{N}$, \n%$$\\mathbb{E}[Y^k \\mid X] = \\frac{k!}{X^{2k}}$$ \n1. Calculer $\\mathbb{E}[\\exp(itY) \\mid X]$, $t \\in \\mathbb{R}$, quelle est la loi conditionnelle de $Y$ sachant $X$ ?\n1. Déduire que si $t\\in \\mathbb{R}$ \n$$ \\mathbb{E}[\\exp(itY)] = \\frac{2 \\exp\\left(-\\frac{t^2}{2}\\right)}{3-\\exp\\left(-\\frac{t^2}{2}\\right)}.$$ \n \n\n::: {.content-visible when-profile=\"solution\"}\n \n\n\n1. Notons que pour tout $n \\ge 1$, $\\int_{\\mathbb{R}} \\frac{1}{\\sqrt{2\\pi n}} \\exp\\left(-\\frac{y^2}{2n}\\right) dy =1$ (on intègre sur $\\mathbb{R}$ la densité d'une variable de loi $\\mathcal{N}(0,n)$). On en déduit (quitte à considérer $\\phi(X,Y) = \\mathbb{I}_{\\{X= n\\}}$)  \n$$  \\mathbb{P}(X=n) = \\mathbb{E}[\\mathbb{I}_{\\{X=n\\}}] =  \\frac{2}{3^{n}}  \\int_{\\mathbb{R}} \\frac{1}{\\sqrt{2\\pi n}} \\exp\\left(-\\frac{y^2}{2n}\\right) dy = \\frac{2}{3^n},$$\net il découle que $X \\sim \\mathrm{Geom}(2/3)$.  \n1. Les événements $\\{\\{X=n\\}, n \\ge 1\\}$ forment une partition de $\\Omega$,  on est dans le cadre de EF3 pour $\\mathrm{Re}(f), \\mathrm{Im}(f)$ et quitte à utiliser la linéarité de l'espérance, on obtient  \n$$ \\mathbb{E}[f(Y) \\mid X] = \\sum_{n \\ge 1} \\frac{\\mathbb{E}[f(Y) \\mathbb{I}_{\\{X=n\\}}]}{ \\mathbb{P}(X=n)} \\mathbb{I}_{\\{X=n\\}}.$$ \nQuitte à considérer $\\phi(X,Y) = \\mathrm{Re(f(Y))} \\mathbb{I}_{\\{X=n\\}}$ puis $\\phi_2(X,Y) = \\mathrm{Im(f(Y))} \\mathbb{I}_{\\{X=n\\}}$ et utiliser la linéarité de l'espérance, on obtient   \n$$ \\mathbb{E}[f(Y) \\mathbb{I}_{\\{X=n\\}}] = \\frac{2}{3^{n} \\sqrt{2 \\pi n}}  \\int_{\\mathbb{R}}  f(y) \\exp\\left(-\\frac{y^2}{2n}\\right)  dy,$$\net donc \n$$ \\frac{\\mathbb{E}[f(Y) \\mathbb{I}_{\\{X=n\\}}]}{ \\mathbb{P}(X=n)} = \\int_{\\mathbb{R}} \\frac{1}{\\sqrt{2 \\pi n}} f(y) \\exp\\left(-\\frac{y^2}{2n}\\right) dy,$$\nce qui conduit à la formule souhaitée. \n\n1. Puisque la fonction caractéristique d'une variable suivant la loi $\\mathcal{N}(0,n)$ est $t \\to \\exp\\left( -\\frac{t^2 n}{2} \\right)$ on a \n$$  \\int_{\\mathbb{R}} \\exp(ity) \\frac{1}{\\sqrt{2 \\pi n}}  \\exp\\left(-\\frac{y^2}{2n}\\right) dy = \\exp\\left(-\\frac{t^2 n}{2} \\right)$$\nde sorte que \n$$ \\mathbb{E}[\\exp(itY) \\mid X] = \\exp\\left(-\\frac{t^2 X}{2} \\right).$$ \nLa loi conditionnelle de $Y$ sachant  $X$ est donc $\\mathcal{N}(0,X)$. \n\n1. On a gr\\^ace à la propriété de tour et la question précédente  \n$$mathbb{E}[\\exp(itY)] & = & \\mathbb{E}[\\mathbb{E}[\\exp(itY) \\mid X]] = \\mathbb{E}\\left[\\exp\\left(-\\frac{t^2 X}{2}\\right)\\right] \\\\ \n& = & \\sum_{n \\ge 1} \\frac{2}{3^n} \\exp\\left(-\\frac{t^2 n}{2}\\right) \\\\ \n& = & \\frac{2 \\exp\\left(-\\frac{t^2}{2}\\right)}{3} \\sum_{n \\ge 1}  \\left(\\frac{\\exp\\left(-\\frac{t^2}{2}\\right)}{3}\\right)^{n-1} \n\\\\ & = & \\frac{2 \\exp\\left(-\\frac{t^2}{2}\\right)}{3} \\sum_{n' \\ge 0}  \\left(\\frac{\\exp\\left(-\\frac{t^2}{2}\\right)}{3}\\right)^{n'}\n\\\\ & = &  \\frac{2 \\exp\\left(-\\frac{t^2}{2}\\right)}{3} \\frac{1}{1-\\frac{\\exp\\left(-\\frac{t^2}{2}\\right)}{3}} \n\\\\ & = & \\frac{2 \\exp\\left(-\\frac{t^2}{2}\\right)}{3-\\exp\\left(-\\frac{t^2}{2}\\right)} $$ \ncomme souhaité. \n \n:::  \n\n### Exercice 11\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note}\n\n### Partiel passé\n\n:::\n \n\n\n#### Partie I \n\nOn considère le couple $(X,Z)$ de densité jointe\n$$ f(x,z) := (z-x)\\exp(-z) \\mathbf{1}_{\\{z \\ge x \\ge 0\\}}.$$\n\n\n\n1. Calculer la loi de $X$, puis celle de $Z$.\n1. En déduire que\n$$ f_{X \\mid Z}(x \\mid z) = \\frac{2(z-x)}{z^2} \\mathbf{1}_{\\{0 \\le x \\le z, z >0\\}}.$$\n1. Calculer $\\mathbb{E}[X \\mid Z]$, puis $\\mathrm{Var}[X\\mid Z]$.\n1. Calculer $f_{Z \\mid X}(z \\mid x)$, puis démontrer que\n$\\mathbb{E}[Z \\mid X] = X + 2$.\n1. Quelle est la loi du couple $(X, Z-X)$? En déduire la loi de $Z-X$.\n\n::: {.content-visible when-profile=\"solution\"}\n\n\n\n1. La variable de $X$ possède la densité $f_X$ avec pour $x \\in \\mathbb{R}$, \n$$_X(x) & = & \\int_{\\mathbb{R}} dz f(x,z) = \\mathbb{I}_{\\{x \\ge 0\\}} \\int_{x}^{\\infty} (z-x) \\exp(-z) dz \n\\\\ & = & \\mathbb{I}_{\\{x \\ge 0\\}} \\int_0^{\\infty} y \\exp(-(y+x)) dy \\\\   \n& = & \\mathbb{I}_{\\{ x \\ge 0\\}} \\exp(-x) $$ \nen utilisant le changement de variables $y = z-x$ et le fait que $\\int_{0}^{\\infty} y \\exp(y)$ vaut $1$ (par exemple en reconnaissant l'espérance d'une exponentielle standard, ou alors en effectuant une i.p.p). On conclut que $X \\sim \\exp(1)$. \n\nLa variable $Z$ possède la densité $f_Z$ avec pour $z \\in \\mathbb{R}$, \n$$f_Z(z) & = & \\int_{\\mathbb{R}} dx f(x,z) = \\mathbb{I}_{\\{z \\ge 0\\}} \\exp(-z) \\int_{0}^{z} (z-x)  dz \n\\\\ & = & \\mathbb{I}_{\\{z \\ge 0\\}} \\exp(-z) \\frac{z^2}{2} $$ \net on conclut que $Z \\sim \\Gamma(2,1)$. \n1. \nOn a donc \n$$ f_{X \\mid Z}(x \\mid z) = \\begin{cases} \\frac{f(x,z)}{f_Z(z)} & \\mbox{si } z > 0 \\\\ 0 & \\mbox{sinon}\\end{cases} = \\frac{2(z-x)}{z^2} \\mathbf{1}_{\\{0 \\le x \\le z, z >0\\}}.$$ \n1. On déduit pour $z >0$, \n$$Phi(z) & := & \\int_{\\mathbb{R}} x f_{X \\mid Z}(x \\mid z) dx \\\\ \n& = & \\int_{0}^{x} \\frac{2 x(z-x)}{z^2} dx = \\frac{z^3 - \\frac{2}{3}z^3}{z^2} = \\frac{z}{3} $$ \net on conclut d'après le résultat EF4 que $\\mathbb{E}[X \\mid Z] = \\Phi_1(Z) = \\frac{Z}{3}$. \n\nDe plus pour $z>0$, \n$$Phi_2(z) & : = & \\int_{\\mathbb{R}} x^2 f_{X \\mid Z}(x \\mid z) dx \\\\ \n& = & \\int_0^x \\frac{2x^2(z-x)}{z^2} dx = \\frac{2}{3} z^2 - \\frac{1}{2} z^2 = \\frac{z^2}{6} $$ \nde sorte, toujours par le même résultat, que $\\mathbb{E}[X^2 \\mid Z] = \\Phi_2(Z) = \\frac{Z^2}{6}$. \n\nOn déduit que \n$$ \\mathrm{Var}[X \\mid Z] = \\mathbb{E}[X^2 \\mid Z] - (\\mathbb{E}[X \\mid Z])^2 = \\frac{Z^2}{6} - \\frac{Z^2}{9} = \\frac{Z^2}{18}.$$\n \n1. On a \n$$ f_{Z\\mid X}(z \\mid x) = \\begin{cases} \\frac{f(x,z)}{f_X(x)} & \\mbox{si } x > 0 \\\\ 0 & \\mbox{sinon}\\end{cases} = (z-x) \\exp(-(z-x)) \\mathbf{1}_{\\{0 < x \\le z\\}}.$$ \nOn déduit que pour $x >0$, \n$$Psi(x) & := & \\int_{\\mathbb{R}} z f_{Z \\mid X}(z \\mid x) dz \\\\ \n& = & \\int_{x}^{\\infty} z (z-x) \\exp(-(z-x)) dx \n\\\\& = &  \\int_{0}^{\\infty} (x+u) u \\exp(-u) du \\\\ \n& = & \\left[ -(x+u)u \\exp(-u) \\right]_{0}^{\\infty} + \\int_0^{\\infty} (x+2u) \\exp(-u) du \\\\ \n& = & \\left[ -(x+2u) \\exp(-u)\\right]_0^{\\infty} + \\int_0^{\\infty} 2 \\exp(-u) du = x + 2, $$\net on obtient, toujours par EF4, comme souhaité, que $\\mathbb{E}[Z \\mid X] = \\Psi(X)=X+2$. \n\n1. Soit $\\phi : \\mathbb{R}^2 \\to \\mathbb{R}_+$ borélienne, par le changement de variables $(x,z) \\to (x,z-x)$ de $\\{(z,x) : 0 \\le x \\le z\\}$ dans $\\mathbb{R}_+^2$ on obtient \n$$mathbb{E}[\\phi(X, Z-X)] & = & \\int_{\\mathbb{R}_+^2} \\phi(x, z-x) (z-x) \\exp(-z) \\mathbb{I}_{\\{z \\ge x\\}} dx dz \\\\\n& = & \\int_{\\mathbb{R}_+^2} \\phi(u,v) \\exp(-u) v \\exp(-v) du dv $$ \net on obtient que $X \\sim \\exp(1)$ est indépendante de $Z-X \\sim \\mathrm{Gamma}(2,1)$. \n\n:::\n\n#### Partie II\n\n\n1. Soit $z >0$. On suppose que $U_1^z \\sim \\mathrm{Unif}[0,z]$, $U_2^z \\sim\n\\mathrm{Unif}[0,z]$ et que $U_1^z$ est indépendante de $U_2^z$.\nCalculer la densité de $\\min(U_1^z, U_2^z)$.\n1. On suppose à présent que {\\em conditionnellement à $Z$},\n$U_1^Z \\sim \\mathrm{Unif}[0,Z]$, $U_2^Z \\sim \\mathrm{Unif}[0,Z]$ et que $U_1^Z$ est\n(toujours conditionnellement à $Z$) indépendante de $U_2^Z$.\nMontrer que, conditionnellement à $Z$,  $\\mathrm{min}(U_1^Z, U_2^Z)$ a la même loi\nque X.\n1. Soient $X_1,X_2,X_3$ trois variables indépendantes, toutes trois distribuées\nsuivant la distribution exponentielle de paramètre $1$. On note $S= X_1+X_2+X_3$.\nDéterminer la loi de $(X_1,S)$. \\\\\nQue vaut $\\mathbb{E}[X_1\\mid S]$? $\\mathbb{E}[S \\mid X_1]$?\\\\\nMontrer finalement que conditionnellement à $S$, le couple $(X_1,X_1+X_2)$ a la même\nloi que $\\left(\\mathrm{min}(U_1^S, U_2^S), \\mathrm{max}(U_1^S, U_2^S)\\right)$.\n\n::: {.content-visible when-profile=\"solution\"}\n \n\n\n1. La fonction de répartition $F$ de $U_1^z$ (et donc de $U_2^z$ puisqu'elle a la même loi est donnée entre $0$ et $z$ par $F(x) = \\frac{x}{z}, 0 \\le x \\le z$. On déduit que pour $0 \\le x \\le z$, en utilisant l'indépendance de $U_1^z, U_2^z$ à la deuxième ligne ci-dessous, \n$$\\mathbb{P}(\\min(U_1^z, U_2^z) > x) & = &  \\mathbb{P}(U_1^z > x)  \\mathbb{P}(U_2^z > x) \\\\ \n& = & (1 - F(x))^2 = \\left(1-\\frac{x}{z}\\right)^2$$ \net on déduit que la densité de $\\min(U_1^z,U_2^z)$ est donnée par \n$$ g_z(x) = \\frac{2}{z} \\left(1-\\frac{x}{z}\\right)\\mathbb{I}_{[0,z]}(x), \\ x \\in \\mathbb{R}.$$   \n1. D'après la question précédente, conditionnellement à $Z$, $\\min(U_1^Z,U_2^Z)$ possède la densité conditionnelle $g_Z$.  \nPar ailleurs, la densité conditionnelle de $X$ sachant $Z$ est $f_{X|Z}$ calculée à la question I.2 est p.p. égale à $g_Z$. \nOn conclut que conditionnellement à $Z$, les variables $X$ et $\\min(U_1^Z,U_2^Z)$ ont la même loi. \n1. \nTout d'abord, par indépendance des trois variables exponentielles, $(X_1,X_2,X_3)$ a densité donnée par  \n$$ f(x_1,x_2,x_3) = \\exp(-x_1-x_2-x_3) \\mathbb{I}_{\\{x_1\\ge 0, x_2 \\ge 0, x_3 \\ge 0\\}}, \\quad (x_1,x_2,x_3) \\in \\mathbb{R}^3.$$\nOn en déduit pour $\\phi : \\mathbb{R}^2 \\to \\mathbb{R}_+$ borélienne, en utilisant à la deuxiième ligne le changement de variables $(x_1,x_2,x_3) \\to (u=x_1,v=x_1+x_2,w=x_1+x_2+x_3)$ de $\\mathbb{R}_+^3$ dans $\\{(u,v,w) \\in \\mathbb{R}_+^3 : u \\le v \\le w\\}$ \n$$mathbb{E}[\\phi(X_1,S)] & = & \\int_{\\mathbb{R}_+^3} \\phi(x_1,x_1+x_2+x_3) \\exp(-x_1-x_2-x_3) dx_1 dx_2 dx_3 \\\\ \n& = & \\int_{\\mathbb{R}_+^3 : u \\le v \\le w} \\phi(u,w) \\exp(-w) du dv dw  \n\\\\ & = &  \\int_{\\mathbb{R}_+^2 : u \\le w} \\phi(u,w) (w-u) \\exp(-w) $$ \net on déduit que $(X_1,S)$ a même loi que $(X,Z)$. \n\n\n\n\nPuisque les deux vecteurs ont même loi jointe, on peut utiliser la partie I pour déduire que  \n$$ \\mathbb{E}[X_1 \\mid S] = \\frac{S}{3}, \\quad \\mathbb{E}[S \\mid X_1] = X_1 +2.$$\nOn peut aussi prouver ces résultats directement (cf exercice 5) \n\n\nD'après le calcul en début de question, la densité du triplet $(X_1.X_1+X_2.S)$ est donnée par \n$$ h(u,v,w) = \\mathbb{I}_{\\{0< u \\le v \\le w\\}} \\exp(-w) \\quad (u,v,w) \\in \\mathbb{R}^3. $$\nQuitte à noter $T=X_1.V=X_1+X_2$ on a donc  \n$$ h_{(T,V) \\mid S} ((t,v) \\mid s) = \\frac{2}{s^2} \\mathbb{I}_{0< t < v < s}. $$\n\nPar ailleurs, la densité conditionnelle de $(U_1^S,U_2^S)$ sachant $S$ est donnée par \n$$\\frac{1}{w^2}\\mathbb{I}_{[0,w]^2}(u,v), (u,v) \\in \\mathbb{R}^2.$$ \nBien s\\^ur ceci peut être récrit \n$$ \\frac{1}{w^2} \\mathbb{I}_{\\{0<u<v<w\\}} + \\mathbb{I}_{\\{0<v<u<w\\}}, (u,v) \\in \\mathbb{R}^2,$$\nla première partie correspondant aux cas où la première uniforme réalise le $\\min$ des deux, et la deuxième partie aux cas où elle réalise le $\\max$. \n \nComme $(U_1^S,U_2^S)$ jouent, conditionnellement à $S$, des r\\^oles parfaitement symétriques, on déduit que $h_{(U,V) \\mid S}$ est la densité de la statistique d'ordre de ces deux variables, ce qui est le résultat souhaité.  \n\n\n:::\n\n\n\n### Exercice 12\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note}\n\n###  Partiel passé\n\n:::\n\nPour $(x,y) \\in \\mathbb{R}^2$ on définit \n\n$$f(x,y) := \\frac{4y}{x^3} \\mathbf{1}_{\\{0<x<1, 0<y <x^2\\}}.$$\n\n\n\nVérifier que $f$ est bien une densité de probabilité, puis calculer les densités marginales $f_X$, $f_Y$.  \n\n::: {.content-visible when-profile=\"solution\"}\n\nIl est clair que $f$ est à valeurs dans $\\mathbb{R}_+$. Reste à vérifier que  $\\int_{\\mathbb{R}^{2}} f(x,y)dx dy =1$. \nComme $f$ est positive, on peut appliquer Fubini pour voir qu'on peut choisir un ordre quelconque d'intégration. \nCommen\\c cons par exemple par intégrer en $y$, on obtient : \n$$\\int_{\\mathbb{R}^{2}} f(x,y)dx dy &=& \\int_{0}^1 \\left(\\int_{0}^{x^2} f(x,y) dy \\right) dx \\\\ & = & \\int_{0}^1 \\left(\\int_{0}^{x^2} y dy \\right) \\frac{4}{x^3} dx\n\\\\ & = & \\int_{0}^1 \\frac{x^4}{2} \\frac{4}{x^3} dx \\\\ & =& \\int_0^1 2x dx = \\left[ x^2 \\right]_0^1 =1, \n$$  \net on conclut que $f$ est bien une densité de probabilité sur $\\mathbb{R}^2$ (on remarquera qu'étant donnée la présence de l'indicatrice, un vecteur $(X,Y)$ de densité $f$ est presque s\\^urement à valeurs dans le carré ouvert $(0,1)^2$, et même presque s\\^urement à valeurs dans la partie du carré qui se trouve strictement sous la parabole $y=x^2$. En particulier, les lois marginales sont toutes deux supportées par $(0,1)$.). \n\n\nPour $x \\in (0,1)$,  \n$$f_X(x) = \\int_{0}^{x^2} f(x,y) dy = 2x. \n$$ \nde sorte que $f_X(x) = 2x \\mathbf{1}_{(0,1)}(x)$. \n\nEnfin, pour $y \\in (0,1)$, on a \n$$f_Y(y) &= &\\int_{\\sqrt{y}}^1 f(x,y) dx \\\\&=& 2y \\int_{\\sqrt{y}}^1 \\frac{2}{x^3} dx \\\\&=& 2y \\left[ \\frac{-1}{x^2} \\right]_{\\sqrt{y}}^1 = 2y \\left(-1+\\frac{1}{y}\\right) = 2(1-y)$$  \nde sorte que $f_Y(y) = 2(1-y) \\mathbf{1}_{(0,1)}(y)$. \n}\n\n:::\n\n \nCalculer $f_{Y\\mid X}(y \\mid x)$ et en déduire que \n$$\\mathbb{E}[Y \\mid X] = \\frac{2}{3} X^2.$$ \n\n\n::: {.content-visible when-profile=\"solution\"}\n \nRappelons que \n$$ f_{Y\\mid X}(y \\mid x) = \\begin{cases} & \\frac{f(x,y)}{f_X(x)} \\mbox{ si } f_X(x) \\ne 0 \\\\ & 0 \\mbox{ sinon.}\\end{cases} $$\n\nOn a donc  \n$$_{Y\\mid X}(y \\mid x) = \\begin{cases} & \\frac{2y}{x^4} \\mathbf{1}_{\\{0<y <x^2\\}} \\mbox{ si } x \\in (0,1) \\\\ & 0 \\mbox{ sinon.} \\end{cases} $$ \n\nOn a alors $\\mathbb{E}[Y \\mid X] = \\psi(X)$, où   \n$$psi(x)  =  \\int_{\\mathbb{R}} y f_{Y \\mid X}(y \\mid x) dy. $$ \nEn particulier $\\psi$ a pour support $(0,1)$ et si $x \\in (0,1)$,  \n$$psi(x) & = & \n  \\int_{0}^{x^2} y \\frac{2y}{x^4}  dy \\\\ &  = & \\frac{2}{x^4} \\left[ \\frac{y^3}{3} \\right]_0^{x^2} = \\frac{2x^2}{3}. $$ \nOn conclut que \n$$ \\mathbb{E}[Y \\mid X] = \\frac{2}{3} X^2.$$ \n\n\n:::\n\n\n\n\n\nMontrer que \n$$ f_{X \\mid Y}(x\\mid y) = \\frac{2y}{1-y} \\frac{1}{x^3} \\mathbf{1}_{\\{0<x<1, 0<y <x^2\\}},$$ \npuis calculer $\\mathbb{E}[X \\mid Y]$. \n\n::: {.content-visible when-profile=\"solution\"}\n\nComme dans la question précédente, \n$$_{X\\mid Y}(x \\mid y) & = & \\begin{cases} & \\frac{f(x,y)}{f_Y(y)} \\mbox{ si } f_Y(y) \\ne 0 \\\\ & 0 \\mbox{ sinon,} \\end{cases} \\\\ \n& = & \\begin{cases} & \\frac{4y}{2(1-y)x^3}\\mathbf{1}_{\\{0<x<1, 0<y<x^2\\}} \\mbox{ si } y \\in (0,1) \\\\ & 0 \\mbox{ sinon,} \\end{cases} $$ \nce qui est le résultat recherché puisque si $0<x<1$ et $0<y<x^2$, on a bien $y \\in (0,1).$ \n\nOn a alors $\\mathbb{E}[X \\mid Y] = \\phi(Y)$, où\n$$phi(y)  =  \\int_{\\mathbb{R}} x f_{X \\mid Y}(x \\mid y) dx. $$  \nEn particulier $\\phi$ a pour support $(0,1)$ et si $y \\in (0,1)$, \n$$phi(y) & = & \n  \\frac{2y}{1-y} \\int_{\\sqrt{y}}^{1} \\frac{1}{x^2}  dx \\\\ &  = & \\frac{2y}{1-y} \\left[ -\\frac{1}{x} \\right]_{\\sqrt{y}}^{1} \\\\ &=& \\frac{2y}{1-y} \\left(-1+\\frac{1}{\\sqrt{y}} \\right) = 2 \\sqrt{y} \\frac{1-\\sqrt{y}}{1-y} = 2\\frac{\\sqrt{y}}{1+\\sqrt{y}}. $$\nFinalement \n$$ \\mathbb{E}[X \\mid Y] = 2 \\frac{\\sqrt{Y}}{1+\\sqrt{Y}}.$$ \n\n\n:::\n\n\n### Exercice 13\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.callout-note}\n\n###  CC2 2023\n\n:::\n \n\n\nDans cet exercice on suppose que \n$$ \\begin{pmatrix} X \\\\ Y\\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 2 & 1 \\\\ 1 & 1 \\end{pmatrix} \\right),$$  \net on pose $U= X^2$.\n\n\n1. Vérifier que $U \\sim \\mathrm{Gamma}(1/2,1/4)$. \n1. Montrer que $(X,Y)$ possède une densité jointe $g$ que l'on déterminera.  \n1. Montrer que $(U,Y)$ possède la densité jointe \n$$ f(u,y) = \\frac{1}{{ 4} \\pi \\sqrt{u}} \\left( \\exp\\left(- \\frac{u}{2} - y^2 + y \\sqrt{u} \\right) + \\exp\\left(-\\frac{u}{2}- y^2 - y\\sqrt{u} \\right) \\right) \\mathbb{I}_{\\{u >0\\}}. $$\n1. Calculer $f_{Y \\mid U}(y \\mid u)$. En déduire $\\mathbb{E}[Y \\mid U], \\mathbb{E}[Y^2 \\mid U]$ et $\\mathrm{Var}(Y \\mid U)$. \nVérifier qu'on a bien\n$$\\mathrm{Var}[Y] = \\mathbb{E}[\\mathrm{Var}[Y \\mid U]] + \\mathrm{Var}[\\mathbb{E}[Y \\mid U]]\\, .$$\n\n1. On suppose que conditionnellement à $U$, $\\xi$ et $Z$ sont indépendantes avec $\\xi \\sim \\mathrm{Ber}(1/2)$ et $Z \\sim \\mathcal{N}\\left(\\frac{\\sqrt{U}}{2},\\frac{1}{2}\\right)$. \nMontrer que conditionnellement à $U$, $(2\\xi-1) Z$ a même loi que $Y$. Vérifier alors les calculs de la question précédente.   \n \n\n\n\n::: {.callout-tip}\n\n### Indications \n \n1. rappelle que pour $a>0, \\lambda >0$,  la densité d'une variable $G \\sim \\mathrm{Gamma}(a,\\lambda)$ est donnée par \n$$ f_G(x) = \\frac{\\lambda^a x^{a-1}}{\\Gamma(a)} \\exp(-\\lambda x) \\mathbb{I}_{\\{x >0 \\}}$$   \n1. On fera attention à distinguer les domaines $D_1 = \\mathbb{R}_-^*\\times \\mathbb{R}$ et $D_2 = \\mathbb{R}_+^* \\times \\mathbb{R}$ pour pouvoir considérer les $\\mathcal{C}^1$-difféomorphismes \n$\\displaystyle{\\Psi_1 : \\begin{cases} \\!\\!&\\!\\! D_1 \\to \\mathbb{R}_+^* \\times \\mathbb{R} \\\\ \\!\\!&\\!\\! (x,y) \\to (x^2,y) \\end{cases}, \\ \\ \\Psi_2 :  \\begin{cases} \\!\\!&\\!\\! D_2 \\to \\mathbb{R}_+^* \\times \\mathbb{R} \\\\ \\!\\!&\\!\\! (x,y) \\to (x^2,y)\\end{cases}}$.  \n1. Pour $\\alpha \\in \\mathbb{R}$, les deux premiers moments de la variable $\\zeta \\sim \\mathcal{N}\\left(\\alpha \\frac{\\sqrt{u}}{2}, \\frac{1}{2}\\right)$ sont \n\\begin{align*} \n \\mathbb{E}[\\zeta] \n & = \\int_{\\mathbb{R}} \\frac{1}{\\sqrt{\\pi}} y \\exp\\left(-  \\left(y - \\alpha \\frac{\\sqrt{u}}{2}\\right)^2 \\right) dy \\\\ \n & = \\alpha \\frac{\\sqrt{u}}{2},  \\\\ \n\\mathbb{E}[\\zeta^2] \n& = \\mathbb{E}[\\zeta]^2+ \\mathrm{Var}[\\zeta] = \\int_{\\mathbb{R}} \\frac{1}{\\sqrt{\\pi}} y^2 \\exp\\left(-  \\left(y - \\alpha \\frac{\\sqrt{u}}{2}\\right)^2 \\right) dy \\\\\n& = \\alpha^2 \\frac{u}{4} + \\frac{1}{2} \n\\end{align*}\n\n\n:::\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n\n\n1. On a $U= X^2$ avec $X \\sim \\mathcal{N}(0,2)$. Pour $\\phi : \\mathbb{R} \\to \\mathbb{R}_+$ borélienne, on obtient donc \n$$\\mathbb{E}[\\phi(U)] & = & \\int_{\\mathbb{R}} \\phi(x^2) \\frac{1}{2 \\sqrt{\\pi}} \\exp(-x^2) dx \\\\ \n& = & \\int_{\\mathbb{R}_-^*} \\phi(x^2) \\frac{1}{2 \\sqrt{\\pi}} \\exp\\left(-\\frac{x^2}{4}\\right) dx + \\int_{\\mathbb{R}_+^*} \\phi(x^2) \\frac{1}{2 \\sqrt{\\pi}} \\exp\\left(-\\frac{x^2}{4}\\right) dx $$ \nEn effectuant le changement de variables $u=x^2$ dans chacune des deux intégrales ci-dessus on obtient \n$$ \\mathbb{E}[\\phi(U)] = \\int_{\\mathbb{R}_+^*} \\phi(u) \\frac{1}{2\\sqrt{\\pi}} \\exp\\left(-\\frac{u}{4}\\right) du, $$\net on conclut que \n$$ f_U(u) = \\frac{1}{2\\sqrt{\\pi}} \\exp\\left(-\\frac{u}{4} \\right) \\mathbb{I}_{\\{u>0\\}},$$\nce qui est bien la densité d'une $\\mathrm{Gamma}(1/2,1/4)$. \n1. D'après le cours, un vecteur gaussien bi-dimensionnel suivant la loi $\\mathcal{N}(0,\\Sigma)$ a une densité sur $\\mathbb{R}^2$ ssi $\\mathrm{det}(\\Sigma) \\ne 0$ et cette densité au point $(x,y)$ vaut  \n$$ \\frac{1}{2\\pi \\sqrt{\\mathrm{det}(\\Sigma)}} \\exp\\left(- \\frac{1}{2} \\begin{pmatrix} x & y \\end{pmatrix} \\Sigma^{-1} \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\right). $$\nIci $\\Sigma =  \\begin{pmatrix} 2 & 1 \\\\ 1 & 1 \\end{pmatrix}$, donc $\\mathrm{det}(\\Sigma) = 1$, \n$\\Sigma^{-1}= \\begin{pmatrix} 1 & -1 \\\\ -1 & 2 \\end{pmatrix}$, et on obtient donc \n$$ g(x,y) = \\frac{1}{2 \\pi} \\exp\\left( -\\frac{x^2}{2} - xy - y^2\\right) $$\n1. Soit $\\phi: \\mathbb{R}^2 \\to \\mathbb{R}_+$ borélienne, on a d'après la question précédente \n$$mathbb{E}[\\phi(U,Y)] & = & \\int_{\\mathbb{R}^2} \\phi(x^2,y) \\frac{1}{2 \\pi} \\exp\\left( -\\frac{x^2}{2} - xy - y^2\\right) \n \\\\ & = & \\int_{\\mathbb{R}_-^* \\times \\mathbb{R}} \\phi(x^2,y) \\frac{1}{2\\pi} \\exp\\left( -\\frac{x^2}{2} - xy - y^2\\right) dc dy \\\\ && \\qquad + \\int_{\\mathbb{R}_+^* \\times \\mathbb{R}}  \\phi(x^2,y) \\frac{1}{2\\pi} \\exp\\left( -\\frac{x^2}{2} - xy - y^2\\right) dx dy $$ \nL'application $(x,y) \\to (u=x^2,y)$ est un changement de variables de $\\mathbb{R}_-^* \\times \\mathbb{R} \\to \\mathbb{R}_+^* \\times \\mathbb{R}$, avec $x = -\\sqrt{u}$, et de  $\\mathbb{R}_+^* \\times \\mathbb{R} \\to \\mathbb{R}_+^* \\times \\mathbb{R}$ avec $x = \\sqrt{u}$, dont le jacobien inverse est $\\frac{1}{2\\sqrt{u}}$. On obtient donc : \n$$mathbb{E}[\\phi(U,Y)] & = & \\int_{\\mathbb{R}_+^* \\times \\mathbb{R}} \\frac{1}{4\\pi \\sqrt{u}} \\left(\\exp\\left( -\\frac{u}{2} + \\sqrt{u} y - y^2\\right) +  \\exp\\left( -\\frac{u}{2} - \\sqrt{u} y - y^2\\right) \\right)du dy$$ \nce qui conduit bien au résultat souhaité. \n\n{\\em Remarque} : Comme \n$$\\int_{\\mathbb{R}} \\frac{1}{\\sqrt{\\pi}} \\exp\\left(-\\frac{u}{4} + \\sqrt{u} y -y^2\\right) dy = \\int_{\\mathbb{R}}  \\frac{1}{\\sqrt{\\pi}} \\exp\\left(-\\frac{u}{4} - \\sqrt{u} y -y^2\\right) dy = 1,$$\npuisque la première intégrale est celle de la densité d'une variable $\\sim \\mathcal{N}(-\\sqrt{u}/2,1/2)$, et la deuxième intégrale celle de la densité d'une variable $\\mathcal{N}(\\sqrt{u}/2,1/2)$, on retrouve bien que \n\n$$ f_U(u) = \\int_{\\mathbb{R}} f(u,y)  dy = \\frac{2}{4\\sqrt{\\pi u}} \\exp\\left(-\\frac{u}{4}\\right) \\mathbb{I}_{\\{u >0\\}},$$\ncomme à la question 1. \n\n1.  On a donc \n$$ f_{Y \\mid U}(y \\mid u) =   \\frac{1}{2\\sqrt{\\pi}} \\left( \\exp\\left(- \\frac{u}{4} - y^2 - y \\sqrt{u} \\right) + \\exp\\left(-\\frac{u}{4}- y^2+y\\sqrt{u} \\right) \\right) \\mathbb{I}_{\\{u >0\\}}.$$ \nRemarquons que \n$$ \\int_{\\mathbb{R}} \\frac{1}{\\sqrt{\\pi}} y \\exp\\left(- \\frac{u}{4} - y^2 + y \\sqrt{u} \\right) dy = \\int_{\\mathbb{R}} \\frac{1}{\\sqrt{\\pi}} \\exp\\left(-  \\left(y - \\frac{\\sqrt{u}}{2}\\right)^2 \\right) dy$$ est la moyenne d'une variable $\\sim \\mathcal{N}(-\\sqrt{u},1/2)$, elle vaut donc $-\\frac{\\sqrt{u}}{2}$.\n\n Par le même raisonnement, $ \\int_{\\mathbb{R}} \\frac{1}{\\sqrt{\\pi}} y^2 \\exp\\left(- \\frac{u}{4} - y^2 + y \\sqrt{u} \\right) dy$ est l'espérance du carré de cette même variable, et vaut donc \n$\\frac{1}{2}+\\frac{u}{4}$. \n\n\nDe même, $ \\int_{\\mathbb{R}} \\frac{1}{\\sqrt{\\pi}} y \\exp\\left(- \\frac{u}{4} - y^2 - y \\sqrt{u} \\right) dy$ est la moyenne d'une variable $\\sim \\mathcal{N}(\\frac{\\sqrt{u}}{2},1/2)$, et vaut donc $\\frac{\\sqrt{u}}{2}$, tandis que   $ \\int_{\\mathbb{R}} \\frac{1}{\\sqrt{\\pi}} y^2 \\exp\\left(- \\frac{u}{4} - y^2 + y \\sqrt{u} \\right) dy$ est l'espérance du carré de cette même variable, et vaut donc $\\frac{1}{2}+\\frac{u}{4}$. \n\nOn a donc \n$$ \\int_{\\mathbb{R}} y f_{Y \\mid U}(y \\mid u) dy = -\\frac{1}{4}\\sqrt{u}+\\frac{1}{4}\\sqrt{u}=0, \\quad \\mbox{ donc } \\mathbb{E}[Y \\mid U]=0$$\ntandis que \n$$ \\int_{\\mathbb{R}} y^2 f_{Y \\mid U}(y \\mid u) dy = \\frac{1}{4}(u+2) + \\frac{1}{4}(u+2) = u+2 \\quad \\mbox{ donc } \\mathbb{E}[Y^2 \\mid U] = \\frac{1}{2} +\\frac{U}{4}.$$\nEnfin \n$$ \\mathrm{Var}[Y \\mid U] = \\mathbb{E}[Y^2 \\mid U] - \\mathbb{E}[Y \\mid U]^2= \\frac{1}{2}+\\frac{U}{4}.$$\n\nBien s\\^ur $\\mathrm{Var}[\\mathbb{E}[Y \\mid U]]=0$.\n Comme $\\mathbb{E}[U] = \\frac{1}{2} \\times \\left(\\frac{1}{4}\\right)^{-1} = 2$, on a bien \n$$ \\mathbb{E}[\\mathrm{Var}[Y \\mid U]] = \\frac{1}{2} + \\frac{1}{2} = 1,$$\net on a bien $\\mathrm{Var}(Y) = 1 = \\mathbb{E}[\\mathrm{Var}[Y \\mid U]]+\\mathrm{Var}[\\mathbb{E}[Y \\mid U]]$. \n \n\n1. La densité d'une variable $\\zeta \\sim \\mathcal{N}\\left(\\frac{\\sqrt{u}}{2}, \\frac{1}{2}\\right)$ est donnée par \n$$ f_{\\zeta}(y) = \\frac{1}{\\sqrt{\\pi}} \\exp\\left(-  \\left(y -  \\frac{\\sqrt{u}}{2}\\right)^2 \\right), \\ y \\in \\mathbb{R}.$$ \nCelle de $-\\zeta \\sim \\mathcal{N}\\left(-\\frac{\\sqrt{u}}{2}, \\frac{1}{2}\\right)$ est donc donnée par \n$$ f_{-\\zeta}(y) = \\frac{1}{\\sqrt{\\pi}} \\exp\\left(-  \\left(y +  \\frac{\\sqrt{u}}{2}\\right)^2 \\right), \\ y \\in \\mathbb{R}.$$ \n\nSi $\\xi \\sim \\mathrm{Ber}(1/2)$, la variable $(2\\xi-1) \\zeta$ a donc densité \n$\\frac{1}{2} \\left(f_{\\zeta}(y) + \\frac{1}{2} f_{-\\zeta}(y)\\right), y \\in \\mathbb{R}$, et on déduit que la densité conditionnelle de $(2\\xi -1) Z$ sachant $U$ au point $(y,u)$ est donnée par $f_{Y \\mid U}(y \\mid u)$. \nAinsi $(U,Y)$ et $(U, (2\\xi-1)Z)$ ont même loi. \n\nOn retrouve bien : \n$$ \\mathbb{E}[Y \\mid U] = \\mathbb{E}[(2\\xi-1)Z \\mid U] = \\frac{1}{2} \\left( \\frac{\\sqrt{U}}{2} - \\frac{\\sqrt{U}}{2}\\right) =0, \\quad \\mathbb{E}(Y^2 \\mid U) = \\mathbb{E}[Z^2 \\mid U] = \\frac{U}{4}+\\frac{1}{2}$$\n\n\n \n:::\n \n\n\n### Exercice 14\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note}\n\n###  Rattrapage passé\n\n:::\n\n\nSoit $X=(X_1,X_2,X_3) \\sim \\mathcal{N}(0,M)$, où    \n\n$$M := \\begin{pmatrix} 2& 2 &-2  \\\\ 2& 5 & 1 \\\\ -2 & 1 & 5 \\end{pmatrix},$$ \n\n\n\n1.  Montrer que $\\det(M)=0$. Le vecteur $X$ possède-t-il une densité dans $\\mathbb{R}^3$? \n1.  Trouver $a \\in \\mathbb{R}$ tel que $X_1$ et $Y=X_2-a X_1$ soient indépendantes. Calculer $\\mathrm{Var}(Y)$ et en déduire la loi de $(X_1,Y)$. \n1.  Trouver la loi conditionnelle de $X_2$ sachant $X_1$. \n\n\n\n::: {.content-visible when-profile=\"solution\"}\n \n\n\n1. On trouve $\\ker(M) = \\mathrm{Vect} \\left\\{ \\begin{pmatrix} 2 \\\\ -1 \\\\ 1 \\end{pmatrix} \\right\\}$ de dimension $1$, donc $\\det(M)=0$. \nLe vecteur $X$ est donc p.s. à valeurs dans $\\mathrm{Ker}(M)^{\\perp} = \\{(x,y,z) \\in \\mathbb{R}^3 : 2x - y + z =0\\}$, en particulier il ne possède pas de densité sur $\\mathbb{R}^3$.    \n1. $(X_1,Y)$ est un vecteur gaussien comme image par une application linéaire d'un vecteur gaussien, ses coordonnées sont donc indépendantes ssi \n$\\mathrm{Cov}(X_1,X_2-aX_1)=0$ ssi $a=1$. \n1. On déduit que $X_2 = X_1 + Y$, avec $Y = X_2-X_1$ indépendant de $X_1$, et de loi $\\mathcal{N}(0,3)$ (on a utilisé que $\\mathrm{Var}(Y) = \\mathrm{Var}(X_1)+\\mathrm{Var}(X_2) - 2 \\mathrm{Cov}(X_1,X_2) = 3$).\n \nOn conclut que conditionnellement à $X_1$, $X_2 \\sim \\mathcal{N}(X_1,3)$.   \n\n\n:::\n\n### Exercice 15\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\nOn considère $X_0 =0$, et $(X_n)_{n \\ge 1}$ une suite de variables aléatoires réelles indépendantes, identiquement distribuées suivant la loi normale centrée réduite. \n\nOn introduit les variables \n\n$$Y_i = \\frac{X_i-X_{i-1}}{i}, i \\ge 1.$$\n\n\n\n\n\nPour $n \\ge 1$, montrer que le vecteur $(Y_1,...,Y_n)$ est gaussien, puis\ncalculer le vecteur moyenne et la matrice de covariances de $(Y_1,...,Y_n)$.   \n\n::: {.content-visible when-profile=\"solution\"}\n\nFixons $n \\ge 1$ et notons $\\xx_n = (X_1,...,X_n), \\yy_n=(Y_1,...,Y_n)$. \n\nLes variables $\\{X_i\\}_{i=1}^n$ étant des gaussiennes centrées réduites indépendantes, on a déjà montré (par exemple à la première question du partiel) que $(X_1,...,X_n)$ est un vecteur gaussien (et d'ailleurs $\\xx_n \\sim \\mathcal{N}(0,I_n)$).  \n\nNotons alors \n\n$$A_n := \\begin{pmatrix} 1 & 0 & 0 & 0 & \\dots & 0 & 0 \\\\ \n                         -1/2 & 1/2 & 0 & 0& \\dots &0 & 0 \\\\ \n                          0 & -1/3 & 1/3 & 0 & \\dots & 0 & 0 \\\\ \n                         \\vdots &  & \\ddots & \\ddots & & & \\\\\n                         \\vdots &  &  & \\ddots & \\ddots & & \\\\\n                         \\vdots &  &  &        & \\ddots & \\ddots & \\\\  \n                          0 & 0 & 0& 0 & \\dots & -1/n & 1/n \\end{pmatrix},$$\n\nde sorte que $\\yy_n = A_n \\xx_n$, et le vecteur $\\yy_n$ est donc bien un vecteur gaussien en tant que transformation linéaire du vecteur gaussien $\\xx_n$. \n\nD'après un théorème du cours, on a alors que $\\yy_n \\sim \\mathcal{N}(A_n 0, A_n I_n A_n^T)$, i.e. $\\yy_n \\sim \\mathcal{N}(0,A_n A_n^T)$, \noù   \n$$ \nA_n A_n^T = \\begin{pmatrix} 1 & -1/2 & 0 & 0 & \\dots & 0 & 0 \\\\ \n                               -1/2 & 1/2 & -1/6 & 0 & \\dots & 0 & 0 \\\\ \n                               0 & -1/6 & 2/9 & -1/12 & \\dots &0 & 0 \\\\\n                               \\vdots & & \\ddots & \\ddots & \\ddots & & \\\\ \n                               \\vdots & &        & \\ddots & \\ddots & \\ddots & \\\\ \n                               \\vdots & &        &        & \\ddots & \\ddots & \\\\ \n                               0 & \\dots &  &  & 0 & -\\frac{1}{n(n-1)} & \\frac{2}{n^2}\\end{pmatrix}.\n$$\n\n:::\n\nCalculer, pour $n \\ge 1$, $\\mathbb{E}[Y_{n+1}\\mid Y_n]$.\n\n::: {.content-visible when-profile=\"solution\"}\n\n Pour $a \\in \\mathbb{R}$ on peut toujours écrire $Y_{n+1} = Y_{n+1} + a Y_n - a Y_n$. \n\nComme le vecteur $\\yy_{n+1}$ est gaussien, la variable $Y_{n+1}+aY_n$ est indépendante de $Y_n$ si et seulement si $ \\mathrm{cov}(Y_{n+1}+a Y_n, Y_n)=0.$ Or \n$$\\mathrm{cov}(Y_{n+1}+a Y_n, Y_n)= -\\frac{1}{n(n+1)} + \\frac{2a}{n^2},$$\nqui s'annule pour $a = \\frac{n}{2(n+1)}$. \n\nOn a alors, en utilisant cette indépendance et le fait que les variables $Y_n, Y_{n+1}$ sont centrées :  \n$$mathbb{E}[Y_{n+1} \\mid \\mathcal{F}_n] & = &\\mathbb{E}[( Y_{n+1} + \\frac{n}{2(n+1)} Y_n )\\mid \\mathcal{F}_n] - \\mathbb{E}[\\frac{n}{2(n+1)} Y_n \\mid \\mathcal{F}_n] \\\\ \n& = & \\mathbb{E}[Y_{n+1} + \\frac{n}{2(n+1)} Y_n] - \\frac{n}{2(n+1)} Y_n \\\\ \n& = & - \\frac{n}{2(n+1)} Y_n. $$ \n\n\n\n\n\n:::\n\n\n\n\n\n\n\n\n\n### Exercice 16\n\n\n::: {.cell}\n\n:::\n\n\nSoient $(X,Y)$ dont la loi jointe a pour densité \n$f(x,y) = x(y-x) \\exp(-y), 0 \\le x \\le y <\\infty$. \nOn introduit la notation $f_{X|Y}(x|y) := f(x,y)/f_Y(y)$ lorsque le quotient est $>0$, $0$ sinon.  \n\n\n1. Exprimer $f_{X|Y}(x|y)$, puis $f_{Y|X}(y|x)$. \n1. En déduire les expressions de $\\mathbb{E}[X|Y], \\mathbb{E}[Y|X]$. \n\n\n\n::: {.content-visible when-profile=\"solution\"}\n \n\n\n1. Calculons d'abord les densités marginales.  \n$$_X(x) & = & \\mathbb{I}_{\\{x>0\\}} \\int_x^{\\infty} x(y-x) \\exp(-y) dy \\\\ \n& = & \\mathbb{I}_{\\{x > 0\\}} x \\exp(-x)  \\int_0^{\\infty} u \\exp(-u) du = \\mathbb{I}_{\\{x > 0\\}} x \\exp(-x) $$ \nde sorte que $X \\sim \\mathrm{Gamma}(2,1)$. \nPar ailleurs \n$$_Y(y) & = & \\mathbb{I}_{\\{y >0\\}} \\exp(-y) \\int_0^y x(y-x) dx \\\\ \n& = & \\mathbb{I}_{\\{y >0\\}} \\exp(-y) \\left( \\frac{y^3}{2} - \\frac{y^3}{3} \\right) =  \\mathbb{I}_{\\{y >0\\}} \\exp(-y) \\frac{y^3}{6} $$ \nde sorte que $Y \\sim \\mathrm{Gamma}(4,1)$. \n\nOn déduit \n$$ f_{X \\mid Y} (x \\mid y) = \\frac{6x(y-x)}{y^3} \\mathbb{I}_{\\{0 < x < y\\}},$$\n$$ f_{Y \\mid X} (y \\mid x) = (y-x) \\exp(-(y-x)) \\mathbb{I}_{\\{0 < x < y\\}}.$$  \n\n{\\em Remarque } : On peut assez facilement interpréter cette deuxième densité conditionnelle : sachant $X$, $Y = X + U$ où $U \\sim \\Gamma(2,1)$ indépendante de $X$.\n\n1. On a \n$$ \\int_{\\mathbb{R}} x f_{X \\mid Y}(x \\mid y) dx = \\int_0^y \\frac{6x^2(y-x)}{y^3} dx = 2y - \\frac{3}{2}y= \\frac{y}{2},$$\net on conclut que $\\mathbb{E}[X \\mid Y] = \\frac{Y}{2}$. \n\nD'autre part \n$$int_{\\mathbb{R}} y f_{Y \\mid X}(y \\mid x) dy & = & \\int_{x}^{\\infty}  y (y-x) \\exp(-(y-x)) dy \\\\ \n& = & \\int_0^{\\infty} (u+x) u \\exp(-u) du \\\\ \n& = & \\left[ -(u+x)u \\exp(-u) \\right]_{0}^{\\infty} + \\int_0^{\\infty} (2u+x) \\exp(-u) du \\\\ \n& = & \\left[ -(2u+x) \\exp(-u) \\right]_0^{\\infty} + \\int_0^{\\infty} 2 \\exp(-u) du = x + 2 $$ \net on conclut que $\\mathbb{E}[Y \\mid X] = X+2$.  \n\n\n:::\n\n\n### Exercice 17\n\n\n::: {.cell}\n\n:::\n\n\n\nSoient $Y,Z$ deux v.a.r. indépendantes $\\sim \\mathrm{exp}(\\lambda)$ où $\\lambda>0$. On pose $X= Y+Z$. Quelle est la loi conditionnelle de $Y$ sachant $X$? Que vaut $\\mathbb{E}[Y|X]$? En déduire l'expression de $\\mathbb{E}[Y|X]$\n\n\n::: {.content-visible when-profile=\"solution\"}\n \nRemarquons déjà que par le même raisonnement que dans l'exercice 5 on trouve $\\mathbb{E}[Y \\mid X] = \\frac{X}{2}$. \n\nPour le reste on peut faire un raisonnement similaire aux exercices 11, 15. D'abord, pour $\\phi  : \\mathbb{R}^2 \\to \\mathbb{R}_+$ borélienne, \n$$mathbb{E}[\\phi(Y, Y+Z)] & = & \\int_{\\mathbb{R}_+^2} \\phi(y,y+z) \\lambda^2 \\exp(-\\lambda (y+z)) dy dz \\\\ \n & = & \\int_{\\mathbb{R}_+^2} \\mathbb{I}_{\\{y \\le x\\}} \\phi(y,x) \\lambda^2 \\exp(-\\lambda x) dy dx $$ \nde sorte que $f_{(Y,X)}(y,x) = \\mathbb{I}_{\\{0<y<x\\}} \\lambda^2 \\exp(-\\lambda x), \\ (x,y) \\in \\mathbb{R}^2$. \n\nComme  $X \\sim \\mathrm{Gamma}(2,1)$ on a \n$ f_X(x) = \\lambda^2 \\exp(-\\lambda x) \\mathbb{I}_{\\{x >0\\}}, x \\in \\mathbb{R}$  \n\net donc \n$$ f_{Y \\mid X}(y \\mid x) = \\frac{1}{x}  \\mathbb{I}_{\\{0 < y < x \\}}$$ \nde sorte que la loi conditionnelle de $Y$ sachant $X$ est $\\mathrm{Unif}[0,X]$. \nOn conclut que $\\mathbb{E}[Y \\mid X] = \\frac{X}{2}.$ \n:::\n\n\n\n\n### Exercice 18\n\n\n::: {.cell}\n\n:::\n\n\n\nSoient $X$ et $Y$ deux variables aléatoires indépendantes, toutes deux normales centrées réduites. On définit pour $\\sigma_1 >0, \\sigma_2>0, |\\rho|\\le 1$, \n$$U = \\sigma_1 X, \\quad V= \\sigma_2 \\rho X + \\sigma_2 \\sqrt{1- \\rho^2} Y.$$ \n\n\n1. Quelle est la loi de $(U,V)$? \n1. Que vaut $\\mathbb{E}[UV]$?\n1. Que vaut $\\mathbb{E}[U \\mid V]? \\mathbb{E}[V \\mid U]? \\mathrm{Var}[U \\mid V]? \\mathrm{Var}[V \\mid U]?$\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n\n1. On a $\\begin{pmatrix} U \\\\ V \\end{pmatrix} = \\begin{pmatrix} \\sigma_1 & 0 \\\\ \\sigma_2 \\rho & \\sigma_2 \\sqrt{1-\\rho^2} \\end{pmatrix} \\begin{pmatrix} X \\\\ Y \\end{pmatrix}$, avec $\\begin{pmatrix} X \\\\ Y \\end{pmatrix} \\sim \\mathcal{N}(0,I_2)$. Or $\\begin{pmatrix} \\sigma_1 & 0 \\\\ \\sigma_2 \\rho & \\sigma_2 \\sqrt{1-\\rho^2} \\end{pmatrix} I_2 \\begin{pmatrix} \\sigma_1 & 0 \\\\ \\sigma_2 \\rho & \\sigma_2 \\sqrt{1-\\rho^2} \\end{pmatrix}^T = \\begin{pmatrix} \\sigma_1^2 & \\sigma_1 \\sigma_2 \\rho \\\\ \\sigma_1 \\sigma_2 \\rho & \\sigma_2^2 \\end{pmatrix}$, on déduit donc que $\\begin{pmatrix} U \\\\ V \\end{pmatrix}  \\sim \\mathcal{N}\\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix},  \\begin{pmatrix} \\sigma_1^2 & \\sigma_1 \\sigma_2 \\rho \\\\ \\sigma_1 \\sigma_2 \\rho & \\sigma_2^2 \\end{pmatrix}\\right).$\n\n1. On a $\\mathbb{E}[UV]=\\mathrm{Cov}(UV) = \\sigma_1\\sigma_2 \\rho$ d'après la question précédente. \n1. D'après les formules du cours, avec $\\theta = U, \\xi =V$, $\\mu_{\\theta}=\\mu_{\\xi}=0, M_{\\theta \\xi} = M_{\\xi\\theta} = \\sigma_1\\sigma_2 \\rho, M_{\\theta \\theta} = \\sigma_1^2$ et $MM_{\\xi \\xi} = \\sigma_2^2$, on trouve que \n$$ \\mathbb{E}[U \\mid V] = M_{\\theta \\xi} M_{\\xi\\xi}^{-1} \\xi = \\frac{\\sigma_1 \\rho}{\\sigma_2} V, \\qquad \\mathrm{Var}[U \\mid V ] = M_{\\theta \\theta} - M_{\\theta \\xi} M_{\\xi \\xi } M_{\\xi \\theta} = \\sigma_1^2(1-\\rho^2)$$\n\nDe manière symétrique on trouve que  \n$$ \\mathbb{E}[V \\mid U] = \\frac{\\sigma_2 \\rho}{\\sigma_1} U, \\qquad \\mathrm{Var}[V \\mid U] = \\sigma_2^2 (1-\\rho^2).$$\n\n\n{\\em Remarque } : $\\mathrm{Cov}(\\alpha U + V, U) = \\alpha \\sigma_1^2 + \\rho \\sigma_1 \\sigma_2$, on trouve donc pour $\\alpha = - \\frac{\\sigma_2 \\rho}{\\sigma_1}$ que \n$$ V = \\frac{\\sigma_2 \\rho}{\\sigma_1} U + \\left( - \\frac{\\sigma_2 \\rho}{\\sigma_1}U + V \\right),$$\nla première partie de la somme étant bien s\\^ur $\\sigma(U)$-mesurable, alors que la deuxième en est indépendante, et suit la loi $\\mathcal{N}(0,\\sigma_2^2(1-\\rho^2))$. On retrouve donc bien que  conditionnellement à $U, V \\sim \\mathcal{N}\\left( \\frac{\\sigma_2 \\rho}{\\sigma_1} U, \\sigma_2^2(1-\\rho^2)\\right)$.\n\n\\medskip\n\n{\\em Remarque 2} : $\\sigma_1^2$ est la variance de $U$, $\\sigma_2^2$ celle de $V$, et $\\rho$ est le coefficient de corrélation de $U$ et $V$. L'énoncé de l'exercice fournit donc une mani\\ ere de fabriquer un vecteur gaussien $2$-dimensionnel et non dégénéré quelconque à partir d'un vecteur gaussien centré réduit.  \n\n:::\n\n\n### Exercice 19\n\n\n::: {.cell}\n\n:::\n\n\n\nSoit $Z = (X, Y )$ un vecteur aléatoire gaussien à valeurs dans $\\mathbb{R}^2$. On suppose que\n$E(X) = E(Y ) = 0$, $\\mathrm{Var}(X) = \\mathrm{Var}(Y ) = 1$ et que \n$\\mathrm{Cov}(X; Y ) = \\rho$ avec $|\\rho|^2 \\ne 1$.\nOn pose $U = X -\\rho Y , V = \\sqrt{1-\\rho^2} Y$.\n\n\n1. Quelles sont les lois de $U$ et $V$ ? Les v.a. $U$ et $V$ sont-elles indépendantes ?\n1. Calculer $\\mathbb{E}(U^2V^2), \\mathbb{E}(U V^3), \\mathbb{E}(V^4)$. En déduire $\\mathbb{E}(X^2Y^2)$.\n1. Retrouver ce dernier résultat par conditionnement. \n\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n\n1. On a  $\\begin{pmatrix} U \\\\ V \\end{pmatrix} = \\begin{pmatrix} 1 & -\\rho \\\\ 0 & \\sqrt{1-\\rho^2} \\end{pmatrix} \\begin{pmatrix} X \\\\ Y \\end{pmatrix}$, avec $\\begin{pmatrix} X \\\\ Y \\end{pmatrix} \\sim \\mathcal{N}\\left(\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1\\end{pmatrix} \\right)$. Or \n $\\begin{pmatrix} 1 & -\\rho \\\\ 0 & \\sqrt{1-\\rho^2} \\end{pmatrix} \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1\\end{pmatrix}   \\begin{pmatrix} 1 & 0 \\\\ - \\rho & \\sqrt{1-\\rho^2} \\end{pmatrix} = \\begin{pmatrix} \\sigma_1^2 & \\sigma_1 \\sigma_2 \\rho \\\\ \\sigma_1 \\sigma_2 \\rho & \\sigma_2^2 \\end{pmatrix}$, on déduit donc que $\\begin{pmatrix} U \\\\ V \\end{pmatrix}  \\sim \\mathcal{N}\\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix},  \\begin{pmatrix} 1-\\rho^2 & 0 \\\\ 0 & 1-\\rho^2 \\end{pmatrix}\\right),$\nautrement dit $U$ et $V$ sont i.i.d de loi $\\mathcal{N}(0,1-\\rho^2)$.\n\n\n1. On déduit \n$$\\mathbb{E}[U^2V^2] = (1-\\rho^2)^2, \\quad \\mathbb{E}[UV^3] = 0, \\quad \\mathbb{E}[V^4] = 3(1-\\rho^2).$$\nOn a donc ($V$ et $Y$ ne diffèrent que par une constante multiplicative donc $U$ est indépendant de $Y$\n$$ \\mathbb{E}[X^2 Y^2] = \\mathbb{E}[(U+\\rho Y)^2 Y^2] = \\mathbb{E}[U^2] \\mathbb{E}[Y^2] + 2 \\rho \\mathbb{E}[U] \\mathbb{E}[Y^3] + \\rho^2 \\mathbb{E}[Y^4] = (1-\\rho^2) +3\\rho^2 = 1+2\\rho^2.$$\n\n\n1. On a vu que $U=X-\\rho Y$ est indépendant de $Y$ (et suit une $\\mathcal{N}(0,1-\\rho^2)$, donc sachant $Y$, $X \\sim \\mathcal{N}(\\rho Y, 1-\\rho^2)$. \nEn particulier $\\mathbb{E}[X^2 \\mid Y] =  \\mathbb{E}[X \\mid Y]^2 + \\mathrm{Var}[X \\mid Y] = \\rho^2 Y^2 + 1-\\rho^2.$\nOn a donc \n$$ \\mathbb{E}[X^2 Y^2] = \\mathbb{E}[\\mathbb{E}[X^2 Y^2 \\mid Y]] = \\mathbb{E}[\\rho^2 Y^4 + (1-\\rho^2)Y^2] = 3 \\rho^2 + (1-\\rho^2) = 1+2\\rho^2.$$\n\n\n \n\n:::\n\n\n\n\n\n### Exercice 20\n\n\n::: {.cell}\n\n:::\n\n\n\nSoient $U, V, W$ trois v.a.r. gaussiennes centrées réduites. On pose\n$$Z =\\frac{U + VW}{\\sqrt{1+W^2}}.$$\n\n\n1. Quelle est la loi conditionnelle de $Z$ sachant $W$?\n1. En déduire que $Z$ et $W$ sont indépendantes et donner la loi de $Z$.\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n\n\n1. Pour $\\alpha \\in \\mathbb{R}$, la loi de $\\frac{U+ \\alpha V}{\\sqrt{1+\\alpha^2}}$ est gaussienne, centrée, et de variance \n$$ \\frac{\\mathrm{Var}(U) + 2\\alpha \\mathrm{Cov}(U,V) + \\alpha^2 \\mathrm{Var}(V)}{1+\\alpha^2} = 1.$$ \nDonc, quelque soit $\\alpha \\in \\mathbb{R}$,  $\\frac{U+ \\alpha V}{\\sqrt{1+\\alpha^2}} \\sim \\mathcal{N}(0,1)$. \nOn déduit que sachant $W$, $Z \\sim \\mathcal{N}(0,1)$\n1. La loi conditionnelle de $Z$ sachant $W$ ne dépend pas de $W$ (et c'est sa loi), on déduit que $Z$ et $W$ sont indépendantes. \nLa loi de $Z$ est $\\mathcal{N}(0,1)$. \n\n\\medskip\n\n{\\em Raisonnement alternatif} : \nPar hypothèse, $(U,V,W)$ possède la densité jointe \n$$ f(u,v,w) = \\frac{1}{(2\\pi)^{3/2}} \\exp\\left(-\\frac{1}{2}(u^2+v^2+w^2) \\right), \\quad (u,v,w) \\in \\mathbb{R}^3.$$ \nCalculons la densité de $(W,Z)$. Soit $\\phi : \\mathbb{R}^2 \\to \\mathbb{R}_+$ borélienne. On va utiliser le changement de variables $\\Psi : \\begin{cases} & \\mathbb{R}^3 \\to \\mathbb{R}^3 \\\\  & (u,v,w) \\to (z,s,t) \\end{cases}$ avec $ z=\\frac{u+vw}{\\sqrt{1+w^2}}, s=v, t=w$. Il s'agit bien d'un $\\mathcal{C}^1$-difféomorphisme, d'inverse  \n$u=z\\sqrt{1+t^2}-  st, v=s, w=t$ et de jacobien inverse $\\sqrt{1+t^2}$ \n$$mathbb{E}[\\phi(W,Z)] & = &  \\frac{1}{(2\\pi)^{3/2}}\\int_{\\mathbb{R}^3}\\phi\\left(w,\\frac{u+vw}{\\sqrt{1+w^2}}\\right) \\exp\\left(-\\frac{1}{2}(u^2+v^2+w^2) \\right) du dv dw \\\\ \n& = & \\frac{1}{(2\\pi)^{3/2}}\\int_{\\mathbb{R}^3}  \\phi(t,z)  \\sqrt{1+t^2} \\exp\\left(-\\frac{1}{2} \\left(z^2(1+s^2) + s^2 t^2 -2z t s \\sqrt{1+t^2}+ s^2 + t^2\\right)\\right) ds dt dz \n\\\\ & =& \\int_{\\mathbb{R}^2} \\frac{1}{(2\\pi)} \\exp\\left( -\\frac{1}{2} (z^2+t^2)\\right) dt dz \\int_{\\mathbb{R}}\\frac{\\sqrt{1+t^2}}{\\sqrt{2\\pi}} \\exp\\left( -\\frac{1}{2} (s\\sqrt{1+t^2} -zt)^2\\right) ds $$ \nComme $\\int_{\\mathbb{R}}\\frac{\\sqrt{1+t^2}}{\\sqrt{2\\pi}} \\exp\\left( -\\frac{1}{2} (s\\sqrt{1+t^2} -zt)^2\\right) ds = 1$ (on reconna\\^\\i t l'intégrale sur $\\mathbb{R}$ de la densité d'une $\\mathcal{N}(\\frac{zt}{\\sqrt{1+t^2}}, \\frac{1}{\\sqrt{1+t^2}})$, on obtient \n   $$mathbb{E}[\\phi(W,Z)]  = \\int_{\\mathbb{R}^2} \\frac{1}{(2\\pi)} \\exp\\left( -\\frac{1}{2} (s^2+t^2)\\right) $$ \net on conclut que $(W,Z)$ est un vecteur gaussien bi-dimensionnel, centré réduit, et on retrouve les résultats précédents.\n\n\n:::\n\n### Exercice 21\n\n\n::: {.cell}\n\n:::\n\n\n\nSoient $X_1$ et $X_2$ des v.a. indépendantes, de lois exponentielles de paramètres\nrespectifs $\\lambda_1$ et $\\lambda_2$.\n\n\n1. Calculer\n$\\mathbb{E}[\\max(X_1,X_2) \\mid X_1]$. \n1. Calculer $\\mathbb{E}[\\max(X_1;X_2)]$.\n \n\n::: {.content-visible when-profile=\"solution\"}\n \n\n\n1. $X_2 \\sim \\exp(\\lambda_2)$ et possède donc la propriété d'absence de mémoire. \nPour $a>0$ est fixé on a donc  \n$$& X_2 \\begin{cases} & \\le a \\mbox{ avec probabilité } 1- \\exp(-\\lambda_2 a) \\\\ \n& = a + \\mathbf{e}_2 \\mbox{ avec probabilité } \\exp(-\\lambda_2 a) \\end{cases}, \\quad \\mbox{ et donc } \\\\ \n&& \\max(X_2,a) = \\begin{cases} &   a \\mbox{ avec probabilité } 1- \\exp(-\\lambda_2 a) \\\\ \n&  a + \\mathbf{e}_2 \\mbox{ avec probabilité } \\exp(-\\lambda_2 a) \\end{cases}.$$  \nOn en déduit que \n$$  \\mathbb{E}[X_1 \\mathbb{I}_{\\{X_1 \\ge X_2\\}} \\mid X_1] = X_1 (1-\\exp(-\\lambda_2 X_1)), \\quad \\mathbb{E}[X_2 \\mathbb{I}_{\\{X_2 \\ge X_1\\}} \\mid X_1] = \\left(X_1+\\frac{1}{\\lambda_2}\\right) \\exp(-\\lambda_2 X_1),$$ \net donc \n$$\\mathbb{E}[\\max(X_1,X_2) \\mid X_1] = X_1 + \\frac{1}{\\lambda_2} \\exp(-\\lambda_2 X_1)$$\n \n1. On a pour $t \\ge 0$, \n$ \\mathbb{E}[\\exp(-t X_1)] = \\frac{\\lambda_1}{\\lambda_1+t}$ et donc \n$$ \\mathbb{E}[\\max(X_1,X_2)] = \\mathbb{E}[\\mathbb{E}[\\max(X_1,X_2) \\mid X_1]] = \\frac{1}{\\lambda_1} + \\frac{1}{\\lambda_2} \\frac{\\lambda_1}{\\lambda_1+\\lambda_2} = \\frac{\\lambda_1^2 + \\lambda_1\\lambda_2 + \\lambda_2^2}{\\lambda_1\\lambda_2(\\lambda_1+\\lambda_2)}.$$\n\n{\\em Remarque, vérification} : Soit $Y = \\max(X_1,X_2)$, on a $F_Y(t) = F_{X_1}(t)F_{X_2}(t) = (1-\\exp(-\\lambda_1 t)) (1-\\exp(-\\lambda_1 t))  \\mathbb{I}_{\\{t \\ge 0\\}}$. \nOn déduit \n$$_Y(t) & = & \\left( \\lambda_1 \\exp(-\\lambda_1 t) (1-\\exp(-\\lambda_2 t)) + \\lambda_2 \\exp(-\\lambda_2 t) (1-\\exp(-\\lambda_1 t)) \\right) \\mathbb{I}_{\\{t \\ge 0\\}} \n\\\\ & = & \\left(\\lambda_1 \\exp(-\\lambda_1 t) + \\lambda_2 \\exp(-\\lambda_2 t) - (\\lambda_1+\\lambda_2) \\exp(-(\\lambda_1+\\lambda_2)t) \\right) \\mathbb{I}_{\\{t \\ge 0\\}}.$$ \nOn calcule alors facilement  \n$$\\mathbb{E}[Y] = \\int_{\\mathbb{R}} t f_Y(t) dt = \\frac{1}{\\lambda_1} + \\frac{1}{\\lambda_2}- \\frac{1}{\\lambda_1+\\lambda_2},$$\net on retrouve le résultat précédent. \n:::\n\n### Exercice 22\n\n\n::: {.cell}\n\n:::\n\n\n\nOn pose $h(x) = \\frac{1}{\\Gamma(a+1)} \\exp(-x)x^{a-1}$ ($a > 0$ fixé) et \n$D = \\{0 < y < x\\}$. Soit\n$f(x, y) = h(x)\\mathbf{1}_D(x, y)$:\n\n\n1. Montrer que $f$ est une densité de probabilité sur $\\mathbb{R}^2$.\nOn considère dans la suite un couple $(X, Y)$ de v.a.r. de densité $f$.\n1. Les v.a. $X$ et $Y/X$ sont-elles indépendantes?\n1. Quelle est la loi conditionnelle de $Y$ sachant $X$ ?\n1. Soit $U$ une v.a.r. indépendante du couple $(X, Y)$ telle que \n$\\mathbb{P}(U = 1) = p$\net $\\mathbb{P}(U = 0) = 1 - p$. On pose $Z = UX + (1 - U)Y$. Quelle est l'espérance\nconditionnelle de $Z$ sachant $X$?\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n\n\n1. Comme $h(x)$ ne dépend pas de $y$, on a pour $x \\in \\mathbb{R}$, \n$$ \\int_{\\mathbb{R}} f(x,y) dy = x h(x) \\mathbb{I}_{\\{x >0\\}} = \\frac{x^a}{\\Gamma(a+1)} \\exp(-x) \\mathbb{I}_{\\{x >0\\}},$$\net on reconna\\^\\i t là la densité d'une variable $\\Gamma(a,1)$. On a donc \n$$ \\int_{\\mathbb{R}^2} f(x,y) dx dy = \\int_{\\mathbb{R}_+}  \\frac{x^a}{\\Gamma(a+1)} \\exp(-x) dx = 1.$$\n\n1. Notons $T = \\frac{Y}{X}$, on a pour $\\phi : \\mathbb{R}^2 \\to \\mathbb{R}_+$ borélienne, en utilisant le changement de variables $\\Psi : \\begin{cases} & D \\to \\mathbb{R}_+^* \\times (0,1) \\\\ & (x,y) \\to \\left(x, t=\\frac{y}{x}\\right)\\end{cases}$ de jacobien inverse $x$,   \n$$mathbb{E}[\\phi(X,T)] & = & \\int_{\\mathbb{R}^2} \\phi(x,\\frac{y}{x})  \\frac{x^{a-1}}{\\Gamma(a+1)} \\exp(-x) \\mathbb{I}_{\\{0<y<x\\}}dx dy \\\\ \n& = & \\int_{\\mathbb{R}^2} \\phi(x,t)  \\frac{x^{a-1}}{\\Gamma(a+1)} \\exp(-x) \\mathbb{I}_{\\{x >0\\}} \\mathbb{I}_{(0,1)}(t) dx dz $$ \net on conclut que $X$ et $T$ sont indépendantes, de lois respectives $\\Gamma(a,1)$, $\\mathrm{Unif}[0,1]$. \n1. D'après ce qui précède, $Y = TX$, avec $T$ indépendante de $X$, \n$\\sim \\mathrm{Unif}[0,1]$. \nRemarquons que si $a>0$, $aT \\sim \\mathrm{Unif}[0,a]$. On déduit que sachant $X$, la loi conditionnelle de $Y$ est $\\mathrm{Unif}[0,X]$. \n1. On a en utilisant que $\\mathbb{E}[XS \\mid X] = X \\mathbb{E}[S \\mid X]$, puis l'indépendance de $X,U,Z$,  \n$$\\mathbb{E}[Z \\mid X] & = & \\mathbb{E}[UX \\mid X] + \\mathbb{E}[(1-U)TX \\mid X]\\\\\n& = & X \\mathbb{E}[U] + X \\mathbb{E}[T(1-U)] = X \\mathbb{E}[U] + X \\mathbb{E}[T]\\mathbb{E}[1-U] = \\frac{3}{4} X.$$ \n\n:::\n\n### Exercice 23\n\n\n::: {.cell}\n\n:::\n\n\n\nSoit $(X_n, n \\in \\mathbb{N})$ une suite de v.a.r.i.i.d. de densité $f$ et fonction de répartition $F$. Soient\n$N := \\min\\{ n \\ge 1 : X_n >X_0\\}$ et  \n$M := \\min \\{n \\ge 1 : X_0 \\ge X_1 \\ge ... \\ge X_{n-1} <X_n \\}.$ \n\n\n1. Trouver $\\mathbb{P}(N=n)$, puis montrer que la fonction de répartition de $X_N$ est $F +(1-F) \\log(1-F)$ (on pourra conditionner par les événements $\\{N=n\\}, n \\in \\mathbb{N}$). \n1. Exprimer $\\mathbb{P}(M=m), m \\ge 1$. \n1. On suppose dans cette question que $f = \\mathbf{1}_{[0,1]}$. Pour $x \\in (0,1)$ on introduit $R^x := \\min\\{ n \\ge 1 : X_1+...+X_n >x \\}$. Montrer que $\\mathbb{E}[\\mathbf{1}_{\\{R^x>n\\}} \\mid X_n] = \\Phi(X_n)$ où $\\Phi(u) = \\mathbb{I}_{\\{u<x\\}}  \\mathbb{P}(R^{x-u} > n-1)$. En déduire   $H_n(x):= \\mathbb{P}(R^x>n)$.   \n\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n\n\n1. Puisque les $(X_i, i \\ge 1)$ sont à densité elles sont p.s. toutes distinctes, et puisqu'elles sont i.i.d elles sont échangeables. \nAutrement dit, pour tout $n \\in \\mathbb{N}^*$, pour tout $\\sigma_n$ permutation de $\\{0,\\dots,n\\}$, $(X_0,...,X_n)$ a même loi que $(X_{\\sigma_n(0)},\\dots,X_{\\sigma_n(n)})$. \n\nIl s'ensuit que pour tout $n \\in \\mathbb{N}^*$ l'application $\\tau_{n-1} : \\{0,...,n-1\\} \\to \\{0,...,n-1\\}$ telle que \n$ X_{\\tau_{n-1}(0)} > X_{\\tau_{n-1}(1)} > \\dots > X_{\\tau_{n-1}(n-1)}$ est uniforme dans les permutations de $\\{0, \\dots,n-1\\}$. En particulier, \n$$&  \\mathbb{P}(\\max(X_0,...,X_{n-1}) = X_0)  =   \\mathbb{P}(\\tau_n(0) =0 ) = \\frac{1}{n}, \\\\\n   \\mathbb{P}(N=n) & = &  \\mathbb{P}(\\max(X_0,...,X_{n-1}) = X_0, \\max(X_0,...,X_n)=X_n) \\\\ &= &  \\mathbb{P}(\\tau_{n+1}(0) = n, \\tau_{n+1}(1)=0) =  \\frac{1}{n(n+1)}. $$\n\nPar ailleurs, le maximum de $(n+1)$ telles variables i.i.d a pour fonction de répartition $F^{n+1}$. Or, sachant $\\{N=n\\}$, $X_N$ réalise ce maximum, on a donc \n$$\\mathbb{P}(X_N < a \\mid N=n)  = F(a)^{n+1} $$  \nIl découle que \n$$  \\mathbb{P}(X_N < a) = \\sum_{n \\in \\mathbb{N}^*}  \\mathbb{P}(N=n)  \\mathbb{P}(X_N < a \\mid N=n) = \\sum_{n \\in \\mathbb{N}^*} \\frac{F(a)^{n+1}}{n(n+1)}.$$\nOr si $y < 1$,  \n$$ y + (1-y) \\log(1-y) = y -(1-y)\\sum_{ n \\ge 1} \\frac{y^n}{n} = y -\\sum_{n \\ge 1} \\frac{y^n}{n}  + \\sum_{n \\ge 1} \\frac{y^{n+1}}{n} = \\sum_{n \\ge 1} \\frac{y^{n+1}}{n(n+1)},$$\net on vérifie que l'égalité reste vraie si $y=1$. \nOn conclut, comme souhaité, que \n$$ \\sum_{n \\in \\mathbb{N}^*} \\frac{F(a)^{n+1}}{n(n+1)} = F(a) + (1-F(a)) \\log(1-F(a)).$$\n\n1. On a \n$$ \\mathbb{P}(M=m) =  \\mathbb{P}(\\tau_m = Id, \\tau_{m+1} \\ne Id) =  \\mathbb{P}(\\tau_m = Id)- \\mathbb{P}(\\tau_{m+1}=Id) = \\frac{1}{m!}- \\frac{1}{(m+1)!} = \\frac{m}{(m+1)!}$$\n\n\n1. On a $\\{R^x>n\\} = \\{X_1+\\dots+X_n < x\\} = \\{X_1+\\dots +X_{n-1} < x-X_n\\} = \\{X_n <x, R^{x-X_n}>n-1\\}$.\nOn déduit que sachant $X_n$, \n$$R^x \\begin{cases} & \\le n \\mbox{ si } X_n \\ge x \\mbox{ ou si } X_1+\\dots + X_{n-1} \\ge x-X_n \\\\ \n    &  < n \\mbox{ si } X_n <x, \\mbox{ et } R^{x-X_n} > n-1 \\end{cases}.$$\nce qui conduit à l'égalité souhaitée gr\\^ace à EF5.\n\nOn a donc \n$$ H_n(x) = \\mathbb{E}[\\Phi(X_n)] = \\int_0^1  \\Phi(u) du =\\int_0^x H_{n-1}(x-u) du = \\int_0^x H_{n-1}(v) dv$$\net donc $H_n$ est la primitive de $H_{n-1}$ nulle en $0$. \nComme $H_1(x) =  \\mathbb{P}(X_1 \\le x) = x$, on conclut que $H_n(x) = \\frac{x^n}{n!}$ \n \n\n\n:::\n \n\n\n\n### Exercice 24\n\n\n::: {.cell}\n\n:::\n\n\n\nSoient $X$ et $Y$ deux v.a.r. indépendantes de loi uniforme sur $[0, 1]$.\n\n\n1. Quelle est l'espérance conditionnelle de $(Y - X)_+$ sachant $X$?\n1. Quelle est la loi conditionnelle de $(Y - X)_+$ sachant $X$?\n\n\n::: {.content-visible when-profile=\"solution\"}\n \n\n\n1. Comme $(X,Y)$ est uniforme sur $[0,1]^2$, si $a \\in [0,1]$, on a  \n$$mathbb{E}[(Y-X)^+ \\mathbb{I}_{\\{X \\le a\\}}] & = & \\int_0^a \\int_0^1 (y-x)^+ dx dy \\\\ \n& = & \\int_0^a \\left(\\int_x^1 (y-x) dy\\right) dx = \\frac{1-(1-a)^3}{6} $$ \nOn cherche donc $\\mathbb{E}[(Y-X)^+ \\mid X]$ sous la forme $f(X)$ avec une fonction $f$ telle que \n$$ \\mathbb{E}[f(X) \\mathbb{I}_{\\{X \\le a\\}}] = \\int_0^a f(u) du = \\frac{1-(1-a)^3}{6},$$\net donc $f(u) = \\frac{(1-u)^2}{2}$, ce qui permet de conclure que \n$$ \\mathbb{E}[(Y-X)^+ \\mid X] = \\frac{(1-X)^2}{2}.$$ \n\n1. Pour $a \\in [0,1]$, $(Y-a)^+ = 0 \\mathbb{I}_{\\{Y \\le a\\}} + (Y-a) \\mathbb{I}_{\\{Y > a\\}}$. \nDe plus, sachant $\\{Y>a\\}$, la loi conditionnelle de $Y-a$ est uniforme sur $[0,1-a]$. \nAutrement dit, \n$$(Y-a)^+ =  \\xi Z$$\noù $\\xi \\sim \\mathrm{Ber}(1-a)$ indépendante de $Z \\sim \\mathrm{Unif}[0,1-a]$, et si $\\phi: \\mathbb{R} \\to \\mathbb{R}_+$ est borélienne, on a \n$$ \\mathbb{E}[\\phi(Y-a)^+] = a \\phi(0) + \\int_0^{1-a} \\phi(u) du.$$ \n\n\nConditionnellement \\ a $X$, définissons $\\xi_X \\sim \\mathrm{Ber}(1-X)$, indépendamment de $Z_X \\sim \\mathrm{Unif}[0,1-X]$ . \nAlors d'après ce qui précède, \n$$\\mathbb{E}[\\phi(Y-X)^+ \\mid X] = X \\phi(0) + \\int_0^{1-X} \\phi(u) du.$$ \nAutrement dit, sachant $X$, \n$$ (Y-X)^+ = \\xi_X Z_X.$$  \n  \n:::\n\n### Exercice 25\n\n\n::: {.cell}\n\n:::\n\n\n\nSoient $X_1, X_2, X_3$ trois v. a. r. gaussiennes centrées réduites indépendantes. On\npose $U = 2X_1 - X_2 - X_3, V = X_1 + X_2 + X_3, W = 3X_1 + X_2 - 4X_3$.\n\n\n1. Quelles sont les lois de $U, V$ et $W$? Quels sont les couples de v.a. indépendantes\nparmi les couples $(U, V), (U,W), (V,W)$?\n1. Montrer qu'il existe $a \\in \\mathbb{R}$ tel que $W = aU + Z$ avec $U$ et $Z$ indépendantes.\nEn déduire $\\mathbb{E}(W \\mid U)$.\n \n\n::: {.content-visible when-profile=\"solution\"}\n\n\n\n1. On a \n$$ \\begin{pmatrix} U \\\\ V \\\\ W \\end{pmatrix} = \\begin{pmatrix} 2 & -1 & -1 \\\\ 1 & 1 & 1 \\\\ 3 & 1 & -4 \\end{pmatrix} \\begin{pmatrix} X_1 \\\\ X_2 \\\\ X_3 \\end{pmatrix}, \\mbox{ et }  \\begin{pmatrix} 2 & -1 & -1 \\\\ 1 & 1 & 1 \\\\ 3 & 1 & -4 \\end{pmatrix}  \\begin{pmatrix} 2 & -1 & -1 \\\\ 1 & 1 & 1 \\\\ 3 & 1 & -4 \\end{pmatrix}^T = \\begin{pmatrix} 6 & 0 & 9 \\\\ 0 & 3 & 0 \\\\ 9 & 0 & 26 \\end{pmatrix},  $$\nde sorte que $\\begin{pmatrix} U \\\\ V \\\\ W \\end{pmatrix} \\sim \\mathcal{N}\\left(\\begin{pmatrix} 0 \\\\ 0 \\\\0 \\end{pmatrix}, \\begin{pmatrix} 6 & 0 & 9 \\\\ 0 & 3 & 0 \\\\ 9 & 0 & 26 \\end{pmatrix} \\right)$. \nEn particulier $U$ et $V$ sont indépendants, tout comme $V$ et $W$, en revanche $U$ et $W$ ne le sont pas. \n\n1. Pour que $W-aU$ soit indépendant de $U$ il faut et il suffit que $\\mathrm{Cov}(W-aU, U) = 9-6a = 0$ et il faut donc choisir $a=\\frac{3}{2}$. \nOn peut en déduire que sachant $U$, la loi conditionnelle de $W$ est $\\mathcal{N}(\\frac{3}{2}U, \\frac{25}{2})$, et en particuler que $\\mathbb{E}[W \\mid U]= \\frac{3}{2}U$.  \n\n:::\n\n### Exercice 26\n\n\n::: {.cell}\n\n:::\n\n\n\nSoient $X$ et $Y$ deux v. a. r. gaussiennes centrées réduites indépendantes. On\npose $Z = X + Y$ , $W = X - Y$.\n\n\n1. Montrer que $Z$ et $W$ sont indépendantes. Quelle est la loi de $W$?\n1. En déduire l'espérance conditionnelle et la loi conditionnelle de $X$ sachant $Z$.\n1. Calculer $\\mathbb{E}(XY \\mid Z)$ et $\\mathbb{E}(XYZ \\mid Z)$.\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n\n1. On a ici \n$$  \\begin{pmatrix} Z \\\\ W \\end{pmatrix} = \\begin{pmatrix} 1 & 1  \\\\ 1 & -1  \\end{pmatrix} \\begin{pmatrix} U \\\\ V  \\end{pmatrix}, \\mbox{ et }  \\begin{pmatrix} 1 & 1  \\\\ 1 & -1    \\end{pmatrix}\\begin{pmatrix} 1 & 1  \\\\ 1 & -1    \\end{pmatrix} = \\begin{pmatrix}2 & 0 \\\\ 0 &2 \\end{pmatrix},$$\nde sorte que $\\begin{pmatrix}Z\\\\ W \\end{pmatrix} \\sim \\mathcal{N}\\left(\\begin{pmatrix} 0 \\\\ 0  \\end{pmatrix}, \\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix} \\right)$. \nEn particulier $Z$ et $W$ sont indépendantes et $W \\sim \\mathcal{N}(0,2)$. \n\n1. On a $X = \\frac{Z+W}{2}$ et donc d'apr\\ es ce qui précède, sachant $Z$, la loi condiitonnelle de $X$ est $\\mathcal{N}\\left(\\frac{1}{2}Z, \\frac{1}{2}\\right)$, et \nen particulier $\\mathbb{E}[X \\mid Z] = \\frac{1}{2}Z$. \n1. On a d'après ce qui précède, et les propriétés de l'espérance conditionnelle \n$$mathbb{E}[XY \\mid Z] & = & \\mathbb{E}[\\frac{Z+W}{2} \\frac{Z-W}{2} \\mid Z] \\\\ & = & \\frac{Z^2}{4} - \\frac{Z \\mathbb{E}[W]}{2} + \\frac{\\mathbb{E}[W^2]}{4} \\\\ \n& = & \\frac{Z^2}{4} + \\frac{1}{2}.$$ \n\nOn déduit que \n$$ \\mathbb{E}[XYZ \\mid Z] = Z \\mathbb{E}[XY \\mid Z] = \\frac{Z^3}{4} + \\frac{Z}{2}.$$ \n\n\n\n:::\n\n\n\n\n\n\n\n \n",
    "supporting": [
      "td2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}