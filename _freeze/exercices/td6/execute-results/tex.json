{
  "hash": "c1bfd9bfe720f3a991b858c6944bbe33",
  "result": {
    "engine": "knitr",
    "markdown": "---\n#title: \"Exercices : semaine VI\"\n# subtitle: \"M1 ISIFAR MA1AY010\"\n\n# date: \"2025-10-13\"\n\nformat:\n  pdf:\n    output-file: td6.pdf\n    include-in-header:\n      - file: before_header_td.tex\n      - text: |\n          \\copypagestyle{style-td1}{mystyle}\n          \\makeevenhead{style-td1}{\\sffamily\\small Probabilités et Extrêmes}{}{\\sffamily\\small TD VI}\n          \\makeoddhead{style-td1}{\\sffamily\\small Probabilités et Extrêmes}{}{\\sffamily\\small TD VI}\n          \\pagestyle{style-td1}\n\n  html:\n    output-file: td6.html\n\nengine: knitr\ncategories: [Convergences, Fourier, Gaussiennes, \"Temps d'arrêt\", Conditionnement]\ndraft: false\n---\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\n\n::: {.callout-note}\n\n# TD VI : Gaussiennes   \n\n- 13 Octobre 2025-17 octobre 2025\n\n- Master I ISIFAR\n\n- **Probabilités** \n\n:::\n\n\n\n::: {#exr-gauss-1 name=\"Lemme de Stein\"}\n\n\n:::\n\nVérifier que les deux propriétés suivantes sont équivalentes:\n\n1. $X \\sim \\mathcal{N}(0,1)$ \n2. Pour toute fonction $g$ absolument continue avec une  dérivée $g'$ telle que $\\mathbb{E}[ |X g(X)|]<\\infty$, on a \n    \n    a. $g'(X)$  intégrable  \n    b. $\\mathbb{E}[g'(X)] = \\mathbb{E}[Xg(X)]$\n\n\n::: {#exr-rotation-invariance  name=\"Invariance par rotation\"}\n\nSi $X \\sim \\mathcal{N}(0, \\text{Id}_n)$, et $A$ est une matrice orthogonale  ($A \\times A^\\top = A^\\top \\times A =\\text{Id}_n$), comment est distribué $A X$ ?  \n\n\n:::\n\n::: {#exr-truc-pisier-1  name=\"Maxima de Gaussiennes\"}\n\n\n:::\n\nVérifier que si $X_1, \\ldots, X_n \\sim_{\\text{i.i.d.}} \\mathcal{N}(0,1)$:\n\n$$\\mathbb{E}\\left[\\max(X_1, \\ldots, X_n) \\right] \\leq \\sqrt{2 \\ln n}$$\n\n\n*Suggestion :* Majorer $\\mathbb{E} \\mathrm{e}^{\\lambda \\max(X_1, \\ldots, X_n )}$ en comparant à $\\mathbb{E} \\sum_{i=1}^n \\mathrm{e}^{\\lambda X_i}$. \nComparer $\\mathbb{E}  \\mathrm{e}^{\\lambda \\max(X_1, \\ldots, X_n )}$ et \n$\\mathrm{e}^{ \\lambda \\mathbb{E} \\max(X_1, \\ldots, X_n )}$.\n\n\n\n\n\n\n\n\n::: {#exr-norme-gaussienne-2  name=\"Norme de vecteurs gaussiens centrés\"}\n\n\n:::\n\nMontrer que $X$ est un vecteur gaussien centré, la loi du carré de la norme euclidienne de $X$ ne dépend que des valeurs propres de la matrice de covariance. \n\n::: {#exr-cochran-1  name=\"Norme de vecteurs gaussiens non centrés\"}\n\n\n:::\n\nMontrer que $X$ est un vecteur gaussien standard et $\\mu$ un vecteur, la loi du carré de la norme euclidienne de $X + \\mu$ ne dépend que la norme de $\\mu$. \n\nMontrer que \n\n$$P \\left\\{ \\Vert X + \\mu \\Vert \\leq x \\right\\} \\leq P \\left\\{ \\Vert X \\Vert \\leq x \\right\\}$$\n\n*Suggestion :* vérifier que c'est vrai en dimension 1, utiliser un argument de couplage. \n\n<!-- ::: {#exr-cochran-2  name=\"Théorème de Cochran 2\"}\n\n\n:::\n\n::: {#exr-student-1  name=\"Théorème de Student\"}\n\n\n::: -->\n\n\n::: {#exr-mills-ratio  name=\"Ratios de Mills\"}\n\n\n:::\n\nSoit $Z \\sim \\mathcal{N}(0,1)$. On note $\\Phi$ la  fonction de répartition de $\\mathcal{N}(0,1)$, \net $\\phi$ sa densité.\n\nMontrer que pour tout $x>0$, \n\n$$\\left(1 - \\frac{1}{x^2}\\right)  \\frac{1}{x} \\phi(x) \\leq  1 - \\Phi(x) \\leq \\frac{1}{x} \\phi(x)$$\n\n*Suggestion :* utiliser l'intégration par parties. \n\n\n::: {#exr-prior-Gaussien name=\"\"}\n\nDans cet exercice $T \\sim \\mathcal{N}(\\mu, \\tau^2)$ et la distribution conditionnelle de $X$ sachant $T$ \nest $\\mathcal{N}(T, \\sigma^2)$. \n\na. Caractériser la loi jointe de $(T,X)$.\na. Quelle est la loi de $X$ ?\nb. Quelle est la distribution conditionnelle de $T$ sachant $X$ ? \n\n:::\n\n\n::: {#exr-conditionnement-gaussien-1  name=\"Conditionnement gaussien\"}\n\n:::\n\n\nSoit $\\rho\\in ]-1,1[$ et $(X,Y)$ un vecteur gaussien centré de matrice de covariances \n$$M = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$$\nOn notera $\\sigma = \\sqrt{1-\\rho^2}$. \n \n1. Calculer $\\text{det}(M)$, $M^{-1}$, puis exprimer la densité jointe $f_{(X,Y)}$ du vecteur $(X,Y)$. \n2. Montrer que  \n\n    $$g_x(y) := \\frac{f_{(X,Y)}(x,y)}{f_X(x)} = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp \\left( - \\frac{1}{2(1-\\rho^2)} \\left(y-\\rho x\\right)^2\\right).$$\n\n    Montrer que pour tout $x \\in \\mathbb{R}$, $y \\to g_x(y)$ définit une densité. \n    \n3. Si on note $Y_x$ une variable de densité $g_x$, que pouvez-vous dire sur la loi de $Y_x$?   \n4. Trouver $\\alpha$, $\\beta$ deux réels tels \nque $(X, \\alpha X + \\beta Y)$ suit la loi normale centrée réduite.\n\nRemarquer que l'on peut écrire $Y= -\\frac{\\alpha}{\\beta} X +\\frac{1}{\\beta}(\\alpha X + \\beta Y)$. Sauriez-vous dire pourquoi cette écriture est intéressante? \n\n\n\n::: {#exr-argaussien name=\"\"}\n\nDans cet exercice, $Y_1, \\ldots, Y_n, \\ldots$ sont i.i.d. selon $\\mathcal{N}(0,1)$, $X_0$ est gaussienne $\\mathcal{N}(\\mu, \\tau^2)$, indépendante de  $Y_1, \\ldots, Y_n, \\ldots$.\n\nOn définit $X_1, \\ldots, X_n, \\ldots$ par \n\n$$X_{i+1} = \\theta X_i  + \\sigma Y_{i+1}$$\n\nOn note $\\mathcal{F}_i = \\sigma\\left(X_0, X_1, \\ldots, X_i \\right)$.\n\na. Calculer $\\mathbb{E}[ X_{i+1} \\mid \\mathcal{F}_i]$.\na. Calculer $\\mathbb{E}X_i$, $\\text{Var}(X_i)$.\na. Peut-on choisir $\\mu, \\tau, \\sigma, \\theta$, pour que les $X_i$ soient tous distribués identiquement?\na. À quelle condition sur $\\theta$, les $X_i$ convergent-elles en distribution ? Préciser la limite si possible.\na. Si $\\theta$ satisfait la condition de la question précédente, calculer $\\text{cov}\\left(X_i, X_{i+k}\\right)$, pour $i, k \\in \\mathbb{N}$ \n\n\n\n\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}